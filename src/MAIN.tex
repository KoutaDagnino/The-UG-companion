\documentclass[a4paper,11pt,oneside]{book}

\usepackage{jheppub} % for details on the use of the package, please
                     % see the JHEP-author-manual
\usepackage[T1]{fontenc} % if needed


%%%%% PACKAGES FOR TYPESETTING MATHEMATICS
\usepackage{amsmath}
\usepackage{booktabs}% http://ctan.org/pkg/booktabs
\newcommand{\tabitem}{~~\llap{\textbullet}~~}
\usepackage{cancel}
\usepackage{mhchem}
\usepackage{csquotes}
\usepackage{tgpagella}
\usepackage{amsthm}
\usepackage{bbold}
\usepackage{bbm}
\usepackage{bigints}
\usepackage{braket}
\usepackage{subcaption}
\usepackage{tcolorbox}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{parskip}
\usepackage{xfrac}
\usepackage{thmtools}
\usepackage{wrapfig}
\usepackage{wrapfig}
\usepackage{systeme}
\usepackage{xcolor}
\usepackage{mdframed}
\usepackage{float}
\usepackage{empheq}
\usepackage{bm}
\usepackage{amsfonts}
\usepackage{url}
\usepackage{gensymb}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{fancyhdr}
\usepackage{tocloft,lipsum,pgffor,sectsty}
\usepackage[nottoc]{tocbibind}
\usepackage{multicol}
\usepackage[toc]{multitoc}
\usepackage{empheq}
\usepackage{pgfplots}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{cancel}
\usepackage{esint}
\usepackage{tcolorbox}
\usepackage{braket}
\usepackage{xcolor}
\usepackage{subcaption}
\usepackage{empheq}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{wrapfig}
\usepackage{xcolor}
\usepackage{mdframed}
\usepackage{float}
\usepackage{empheq}
\usepackage{bm}
\usepackage{amsfonts}
\usepackage{url}
\usepackage{gensymb}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{fancyhdr}
\usepackage{tocloft,lipsum,pgffor,sectsty}
\usepackage[nottoc]{tocbibind}
\usepackage{multicol}
\usepackage[toc]{multitoc}
\usepackage{sectsty}
\usepackage{amsmath}
\usepackage{cancel}

\usepackage{tikz}
\usepackage{tikz-cd}
\usepackage{tcolorbox}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{witharrows}
\usepackage{wrapfig}
\usepackage{braket}
\usepackage{wrapfig}
\usepackage{xcolor}
\usepackage{subcaption}
\usepackage{mdframed}
\usepackage{float}
\usepackage{empheq}
\usepackage{bm}
\usepackage{amsfonts}
\usepackage{url}
\usepackage{gensymb}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{fancyhdr}
\usepackage{tocloft,lipsum,pgffor,sectsty}
\usepackage[nottoc]{tocbibind}
\usepackage{multicol}
\usepackage{amssymb}
\usepackage{sectsty}
\usepackage{xparse}

\usetikzlibrary{positioning}
\tikzset{main node/.style={circle, fill=blue!30,draw,inner sep=0.8pt},
            }

\newcommand{\tikzmark}[1]{\tikz[overlay,remember picture] \node (#1) {};}
\pgfplotsset{compat=newest}
\usepackage{sectsty}
\subsectionfont{\fontsize{12}{14}\sffamily}
\sectionfont{\fontsize{15}{14}\sffamily}
\chapterfont{\fontsize{25}{14}\sffamily}

\newcommand*\widefbox[1]{\fbox{\hspace{2em}#1\hspace{2em}}}

\setlength \parindent{1 pt}
\linespread{1}

\usepackage{changepage}

 
\titleformat{\chapter}[hang]{{\thispagestyle{fancy}}\Huge\bfseries}{\marginnote{
\begin{tcolorbox}[width=2cm,height=3cm,boxrule=0.3mm,colback=black!30!white,colframe=black!55!white,center title,arc=0pt,outer arc=0pt,text fill]
  \begin{center}
{\color{white}\thechapter}
  \end{center}
\end{tcolorbox}
}[-1.4in]}{0pt}{\sffamily\Huge\bfseries}


\sectionfont{\fontsize{15}{14}\sffamily}
\chapterfont{\fontsize{25}{14}\sffamily}
%useful commands to shorten work
\newcommand{\boxedeq}[2]{\begin{empheq}[box={\fboxsep=6pt\fbox}]{align}\label{#1}#2\end{empheq}}

\newcommand{\BF}[1]{\boldsymbol{#1}}

\newcommand{\SF}[1]{\mathsf{#1}}

\newcommand{\at}[2][]{#1|_{#2}}

\newcommand{\dbar}{d\hspace*{-0.08em}\bar{}\hspace*{0.1em}}


% griffiths symbols
\def\rcurs{{\mbox{$\resizebox{.16in}{.08in}{\includegraphics{ScriptR}}$}}}
\def\brcurs{{\mbox{$\resizebox{.16in}{.08in}{\includegraphics{BoldR}}$}}}
\def\hrcurs{{\mbox{$\hat \brcurs$}}}


% Colours used
\definecolor{RedBrown}{RGB}{139,1,1}
\definecolor{ForestGreen}{RGB}{11,102,35}
\definecolor{DarkGray}{RGB}{45,45,45}
% Boxes for theorems, examples, corollaries, lemmas. 
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}{Definition}[section]
\newtheorem{remark}{Remark}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{corollary}{Corollary}[section]

\newtcolorbox{mybox}[3][]
{
  colframe = #2!75,
  colback  = #2!10,
  coltitle = white,  
  title    = {\large \sffamily \textbf{#3}},
  #1,
  sharp corners
}

\newtcolorbox{strategy}{
   boxrule=0.3pt, sharp corners, colback = black!10!white}
   
   
% Font changes to ToC content of sectional units
\renewcommand{\cftpartfont}{\normalfont\large\sffamily\bfseries}% \part font in ToC
\renewcommand{\cftchapfont}{\normalfont\normalsize\itshape}    % \chapter font in ToC
\renewcommand{\cftsecfont}{\normalfont}           % \section font in ToC

% Font changes to document content of sectional units
\renewcommand{\partfont}{\Huge\bfseries}
\renewcommand{\chapterfont}{\small\bfseries}
\renewcommand{\sectionfont}{\small\bfseries}
\renewcommand{\headrulewidth}{1pt}


\declaretheoremstyle[
headfont=\normalfont\bfseries,
notefont=\mdseries, notebraces={(}{)},
bodyfont=\normalfont,
postheadspace=1em,
headpunct={},
qed=$\blacktriangleleft$,
numbered=no
]{solstyle}

\declaretheorem[style=solstyle]{solution}

%titlepage
\title{\boldmath \fontsize{35}{45}\selectfont The Undergraduate Companion to Theoretical Physics}

% Author and affiliation
\author[\ddagger]{\Large Andrea Kouta Dagnino}


\affiliation[\ddagger]{Open University, Milton Keynes, UK.}


% e-mail addresses: one for each author, in the same order as the authors
\emailAdd{k.y.dagnino@gmail.com}

%contents table in two columns
\renewcommand*{\multicolumntoc}{2}
\setlength{\columnseprule}{0.5pt}


\begin{document} 
\maketitle

\newpage

\thispagestyle {empty}

\vspace*{4cm}

\begin{center}
	\Large{\parbox{13cm}{
		\begin{raggedright}
		{\fontsize{20}{48} 
			\textit{If you are out to describe the truth, leave elegance to the tailor.
}
		}
	
		\vspace{.5cm}\hfill{--- Ludwig Eduard Boltzmann}
		\end{raggedright}
	}
}
\vspace{2cm}
\begin{figure}[h!]
    \centering
    \includegraphics[width = 8cm]{mathematicians and physicists/boltzmann.jpeg}
    \label{fig:my_label}
\end{figure}
\end{center}


\onecolumn
\pagestyle{fancy}
\fancyhf{}
\chead{\sffamily{\rightmark}}
\cfoot{$- \  \thepage \ -$}
\chapter*{References}
Several textbooks, online courses/resources were referenced heavily (to the extend of making this text completely unoriginal, yet hopefully helpful for revision) throughout the writing of these lecture notes. Using a typical bibliography (research paper style) would be a formidable task.Pinpointing exactly where each reference as been used is quite difficult for such a large and well-referenced subject, and would probably change the writing style to a far too formal one for lecture notes. Therefore we instead list the most relevant below giving a brief comment on which topics they were mostly used for:
\begin{itemize}
 \item {\sffamily S. Blundell, K. Blundell \textit{Concepts in Thermal Physics}}
 
 Fantastic introduction to thermodynamics. All of part I was typeset referencing this textbook heavily. I particularly enjoyed the extra special topics section with discussions of more contemporary advances in physics based on statistical mechanics. 
  \item {\sffamily W. Greiner, L. Neise, H. Stocker \textit{Thermodynamics and Statistical Mechanics}}
  
 This is a more advanced version of Blundell, covering the same topics but more in-depth. I used this mostly while writing parts III and IV.
  \item {\sffamily M. Kardar \textit{Statistical Physics of Particles}}
  
Comprehensive but advanced, good for a second review of the subject. It is supplemented by an equally fantastic lecture series on MIT OCW. It was used mostly in part IV on Quantum statistical physics. 
 \item {\sffamily A. Schekhohichin course notes on \textit{A3 Statistical Physics}}
 
 Discusses kinetic theory and statistical physics from a more mathematical point of view. It was really helpful for part II.
 
 \item {\sffamily Course notes by D. Tong on \textit{Part II Statistical Mechanics}}
 
 Excellent notes as usual, particularly recommended for statistical mechanics. It skips through thermodynamics at the beginning (indeed part II students taking the course will have already studied thermodynamics beforehand in part IB), but does dedicate an entire chapter to it later on. 
 \item {\sffamily E. Terentjev \textit{Part II Thermal and Statistical Physics}}
 
 Similar content to the notes by David Tong, but the lecture slides made the content really succint and easy to digest while revising.
\end{itemize}
\chapter{Preliminaries}
\section{How much stuff?}
The branch of thermal physics often involves the statistical study of macroscopic systems, assemblies of many, many atoms. For example, the number of atoms in an average human is about a staggering $7\times 10^{27}$, that's more than the number of sand particles on Earth ($10^9$ times more in fact). Therefore, we can't use words like dozen, billion, billion trillion, we must instead invent a name for large quantities of matter, the \textbf{mole}.
\begin{mybox}{ForestGreen}{\textbf{\sffamily{Definition 1}: The mole}}
A \textbf{mole} is the quantity of matter equivalent to the number of atoms in 12 g of $\ce{^12 C}$. It is equivalent to an \textbf{Avogadro number} $N_A$ of atoms:
\begin{equation}
    N_A = 6.022\times 10^{23} 
\end{equation}
For example a mole of ping pong balls would be $6.022\times 10^{23}$ ping pong balls. 
\end{mybox}
The \textbf{molar mass} of a substance is the mass of 1 mole of the substance. Carbon-12 therefore has a molar mass of exactly 12g \footnote{this is not the value found in periodic tables, which is around 12.01 g. Indeed, the periodic table provides the \textbf{relative atomic mass}, which is the weighted average of the molar masses for all isotopes.}
\section{The thermodynamic limit}
Let us consider the following problem. We have 1 kg of nitrogen gas in a container, which is approximately $2\times 10^{25}$ molecules, and we want to model the motion of every single one of these. One year contains $3.2\times 10^7$ seconds, so for a 3 $GHz$ computer counting one molecule per cycle, it would count $9.5 \times 10^16$ molecules in one year. So, to count all the molecules in 1kg of $N_2$, it would take 0.2 billion years. That is just for counting the molecules, imagine what it would take to model their positions and velocities! 

It is clear that we need to approach this problem from a more statistical perspective. Instead of making measurements on individual atoms, we can instead consider their average. For an infinitely large sample ($10^25$ molecules is definitely a good approximation), we can then ignore any fluctuations which get smoothed out for a randomized sample. We refer to this limit of an infinitely large sample as the \textbf{thermodynamic limit}. 

Suppose the container of gas had a volume $V$, temperature $T$, pressure $p$ and total kinetic energy $U$. If we then slice the container, these variables change to $V', T', p', U'$. How do these relate to their initial values?
By definition, slicing the container in half implies its volume is halved:
\begin{equation}
    V'=\frac{V}{2}
\end{equation}
Since there are half as many particles the total kinetic energy will also be halved (assuming a randomized distribution of particles, which is clearly the case):
\begin{equation}
    U'=\frac{U}{2}
\end{equation}
The temperature however should not change, nor does pressure as we will see later:
\begin{equation}
    p'=p \ \text{ and } \ T' = T
\end{equation}
\begin{mybox}{ForestGreen}{\textbf{\sffamily{Definition 2:Intensive/Extensive variables}}}
An \textbf{intensive} variable, such as $p,T,$ does not scale with the size of the system. 


An \textbf{extensive variable}, such as $V,U$ does scale with the system size. 
\end{mybox}

For example, consider combining 2 volumes of identical gases with state variables $N,V,p,T$ each. The combined system will have state variables $2N,2V,p,T$. Hence, volume and the number of particles are extensive, whereas pressure and temperature are intensive. 
\section{Approximating large numbers}
We will often come across numbers much larger than $N_A$. Dealing with statistics and probability, we will oten come across factorials, such as $10^{23}!$. In such cases, we may not need an exact value, which would be difficult to calculate, and an approximation would suffice. 

To do so, we can use \textbf{Stirling's formula}:
\begin{equation}
    \ln n! \approx n \ln n - n
\end{equation}

In the case of $10^{95}!$, Stirling's formula gives:
\begin{equation}
\ln(10^{95}!) \approx 10^{95} \ln 10^{95} - 10^{95} \approx 2.2 \times 10^{97}
\end{equation}
Therefore:
\begin{equation}
    10^{95}! \approx \exp (2.2 \times 10^{95})= 10^{9.55 \times 10^{94}}
\end{equation}
which is a one followed by more zeros than the number of particles in the universe. 
\section{The ideal gas law}
Experiments in the 18th century showed the relation between pressure, temperature and volume. 
Boyle's law showed that:
\begin{equation}
    p \propto \frac{1}{V}
\end{equation}
and Charles' law showed that:
\begin{equation}
    V \propto T
\end{equation}
and finally Gay-Lussac's law showed that:
\begin{equation}
    pV \propto T
\end{equation}
These three can be combined into the \textbf{ideal gas law}. 
\begin{mybox}{RedBrown}{\textbf{\sffamily{The Ideal Gas Law}}}
For a system containing $N$ molecules of an \textbf{ideal gas}:
\begin{equation}
pV=Nk_B T    
\end{equation}
where $k_B$ is the \textbf{Boltzmann constant}, with value:
\begin{equation}
    k_B \approx 1.38 \times 10^{-23} J K^{-1}
\end{equation}
\end{mybox}
This equation makes several assumptions on the properties of the gas. Firstly, it assumes that a gas can be modelled as an assembly of tiny particles which can bounce off of each other and the walls of the container. Secondly, it assumes that the molecules are point-like, and that there are no intermolecular forces. It also models the gas molecules as non-relativistic. 

\begin{mybox}{ForestGreen}{\textbf{\sffamily{Definition 3:} Ideal Gas}}
An \textbf{ideal gas} is made of molecules of point size obeying Newtonian principles. They do not interact through intermolecular forces, and have no potential energy, and do not condense when cooled. 
\end{mybox}
\section{What is heat}
\begin{mybox}{ForestGreen}{\textbf{\sffamily{Definition 4: Heat}}}
Heat is defined as thermal energy in transit. 
\end{mybox}

\section{Probability and statistics}
 
\section{What is temperature}
\section{Partial differentiation}
Suppose we have a set of variables $x,y,z$ related through the constraint:
\begin{equation}
    F(x,y,z) = 0 \iff x=x(y,z)
\end{equation}
We may express this in differentials as:
\begin{equation}\label{x partial}
    dx = \bigg(\frac{\partial x}{\partial y} \bigg)_z dy + \bigg(\frac{\partial x}{\partial z}\bigg)_y dz
\end{equation}
Analogously one may also define:
\begin{equation}\label{z partial}
    z=z(x,y) \implies  dz = \bigg(\frac{\partial z}{\partial x} \bigg)_y dx + \bigg(\frac{\partial z}{\partial y}\bigg)_x dy
\end{equation}
and thus substituting \eqref{z partial} into \eqref{x partial} we find that:
\begin{equation}
    dx = \bigg(\frac{\partial x}{\partial z}\bigg)_y \bigg(\frac{\partial z}{\partial x}\bigg)_y dx + \bigg(\bigg(\frac{\partial x}{\partial y}\bigg)_z + \bigg(\frac{\partial x}{\partial z}\bigg)_y \bigg(\frac{\partial z}{\partial y}\bigg)_x\bigg)dy
\end{equation}
If we choose $x,y$ to be independent then $dy = 0$ yields:
\begin{equation}
    \boxed{\bigg(\frac{\partial x}{\partial z}\bigg)_y \frac{\partial z}{\partial x}\bigg|_y=0 \iff \bigg(\frac{\partial x}{\partial z}\bigg)_y=\frac{1}{\big(\frac{\partial z}{\partial x}\big)_y}}
\end{equation}
whereas $dx=0$ yields:
\begin{equation}
 \boxed{\bigg(\frac{\partial x}{\partial z}\bigg)_y \bigg(\frac{\partial z}{\partial y}\bigg)_x \bigg(\frac{\partial y}{\partial x}\bigg)_z=-1}
\end{equation}

\part{Classical Thermodynamics}
\chapter{The First Law}
\section{Definitions}
A \textbf{system} is whatever part of the universe we choose to study. The \textbf{surroundings} are the regions near the system. 
A system is:
\begin{enumerate}
    \item[(i)] \textbf{Isolated}: if it does not interact with its surroundings. Here the energy, volume and particle number are all conserved.
    \item[(ii)] \textbf{Closed}: if it can only exchange energy with the surroundings. Volume and particle number are still conserved. 
    \item[(iii)] \textbf{Open}: if it can interact with the surroundings
    \item[(iv)] \textbf{Homogeneous}: if it has the same properties everywhere throughout it
    \item[(v)] \textbf{Heterogeneous}: if it has discontinuities in its properties
\end{enumerate}
The \textbf{phases} of a system are the individual components of it which are homogeneous, and they are separated by \textbf{phase boundaries}. 



A system is in \textbf{thermal equilibrium} when the macroscopic observables have no time dependence, this set of observables form a particular \textbf{equilibirum state}. These observables, which have a well-defined value for each equilibrium state of the system are then called \textbf{functions of state.}

If we define equilibrium surfaces from equations of state, then every point on these surfaces will represent a state of equilibrium:
\begin{figure}[h!]
    \centering
    \includegraphics[width=6cm]{chapter 2/equilibrium surface.png}
    \caption{pVT surface defines an equilibrium surface for ideal gases which follow the state equation $pV = k_BnT$}
    \label{fig:my_label}
\end{figure}
If we let $\textbf{x}=(x_1,x_2...)$ specify a state of a system, and let $f(\textbf{x})$ be a function of state. Allowing the system to change from $\textbf{x}_i$ to $\textbf{x}_f$, then:
\begin{equation}
    \Delta f = \int_{\textbf{x}_i}^{\textbf{x}_f} df =\int_{\textbf{x}_i}^{\textbf{x}_f} \nabla f(\textbf{x}) \cdot d\textbf{x}= f(\textbf{x}_f)-f(\textbf{x}_i)
\end{equation}
independent of how the change occurs, it only depends on the end points. We call $df$ an \textbf{exact differential}, and a quantity that does not have an exact differential is not a function of state. 


For example, if $\dbar g = y dx$, then for a change from $(0,0)$ to $(1,1)$ then along a straight line path connecting these points:
\begin{equation}
    \Delta g = \int_{(0,0)}^{(1,1)} y dx = \int_0^1 x dx = \frac{1}{2}
\end{equation}
whereas for a path going to $(1,0)$ and then $(1,1)$ we have:
\begin{equation}
    \Delta g = \int_{(0,0)}^{(1,0)} y dx+\int_{(1,0)}^{(1,1)} y dx = 0
\end{equation}
So clearly $\dbar g$ is an inexact differential, which is symbolized by the small cross through the $d$. Since $\Delta g$ depends on the path taken, $g$ is not a function of state. 

More generally, to test whether $df = \textbf{F} \cdot d\textbf{x}$ it is sufficient to check whether or not $\textbf{F}$ is conservative
\begin{equation}
    \nabla \times \textbf{F} = 0
\end{equation}

\section{The first law of thermodynamics}
In 1789 Lavoisier proposed a model of heat as a weightless, conserved fluid called \textbf{caloric}. It could neither be created nor destroyed, but could be released through combustion.
This however did not explain other forms of heating, such as friction which was observed by Rumsford in 1798. So in 1842 Mayer proposed the mechanical equivalent of heat by frictionally generating heat in paper. Joule performed similar experiments but more carefully. He let a mass tied to a string descend a certain height, making a paddle immersed in a mass of water turn. This motion heats the water, and after making the mass fall a certain height Joule measured the temperature rise in the water, deducing correctly the mechanical equivalent of heat. 
He also measured the heat output of a resistor, and showed that for the same energy input, the same amount of heat was produced, thus suggesting that heat was a form of energy. 

\begin{mybox}{RedBrown}{\textbf{\sffamily{The first law of thermodynamics}}}
Energy is conserved, and heat and work are forms of energy, but not functions of state. If $U$ is the \textbf{internal energy} of the system, the sum of the energy of all its degrees of freedom, then:
\begin{equation}
    dU = \dbar Q + \dbar W
\end{equation}
where $\dbar Q$ is the heat supplied to the system, and $\dbar W$ is the work done on the system. 
\end{mybox}
A \textbf{thermally isolated system} cannot exchange heat with its surroundings so that:
\begin{equation}
    dU = \dbar W
\end{equation}

Consider the work done by compressing gas of pressure $p$ and volume $V$:
\begin{equation}
    \dbar W = F dx = p A dx =- p dV 
\end{equation}
since decreasing the volume requires positive work. It is important to note that this equation is only true for reversible changes, as we will see in the following chapter. 
\section{Heat capacity}
Generally the internal energy will be a function of temperature and volume, so we may set $U=U(T,V)$ and:
\begin{equation}
    dU = \bigg(\frac{\partial U}{\partial T}\bigg)_V dT+\bigg(\frac{\partial U}{\partial V}\bigg)_T dV
\end{equation}
Inserting this into $\dbar Q = dU + pdV$ we find:
\begin{equation}
    \dbar Q = \bigg(\frac{\partial U}{\partial T}\bigg)_V dT+\Bigg[\bigg(\frac{\partial U}{\partial V}\bigg)_T+p\Bigg] dV
\end{equation}
and dividing by $dT$ we find:
\begin{equation}
    \frac{\dbar Q}{dT} = \bigg(\frac{\partial U}{\partial T}\bigg)_V +\Bigg[\bigg(\frac{\partial U}{\partial V}\bigg)_T+p\Bigg] \frac{dV}{dT}
\end{equation}

Using this we can now calculate the amount of heat we must add to have a change in temperature.
If we keep the volume constant:
\begin{equation}
    C_V = \Big(\frac{\partial Q}{\partial T}\Big)_V=\bigg(\frac{\partial U}{\partial T}\bigg)_V
\end{equation}
since the second term in 2.3.3 vanishes at constant volume. 
If we keep the pressure constant:
\begin{equation}
    C_p = \Big(\frac{\partial Q}{\partial T}\Big)_p= \bigg(\frac{\partial U}{\partial T}\bigg)_V +\Bigg[\bigg(\frac{\partial U}{\partial V}\bigg)_T+p\Bigg] \bigg(\frac{\partial V}{\partial T}\bigg)_p
\end{equation}
We then define the specific heat capacities as the heat capacity per mass of gas:
\begin{align}
    c_V = \frac{C_V}{M}\\
    c_p = \frac{C_p}{M}
\end{align}
where $M$ is the mass of the material. 
We can also define the ratio of $C_p$ to $C_V$, called the \textbf{adiabatic index} $\gamma$:
\begin{equation}
    \gamma = \frac{C_p}{C_V}
\end{equation}

\begin{mdframed}
\sffamily{\textbf{Example (5.1.1 Sh)}}


For an ideal monoatomic gas, the internal energy is all due to the kinetic energy, and can be shown (see kinetic theory of gases) to be $U=\frac{3}{2}RT$ per mole, where $R = N_A k_B$ is a constant. Therefore:
\begin{equation}
    C_V = \bigg(\frac{\partial U}{\partial T}\bigg)_V = \frac{3}{2}R
\end{equation}
per mole. Also:
\begin{equation}
    C_p = C_V + \Bigg[\bigg(\frac{\partial U}{\partial V}\bigg)_T+p\Bigg] \bigg(\frac{\partial V}{\partial T}\bigg)_p
\end{equation}
but $\bigg(\frac{\partial U}{\partial V}\bigg)_T=0$, and using the ideal gas law for one mole of gas:
\begin{equation}
    pV = RT \implies \bigg(\frac{\partial V}{\partial T}\bigg)_p=\frac{R}{p}
\end{equation}
so that:
\begin{equation}
    C_p = C_V + R = \frac{5}{2}R
\end{equation}
per mole. Therefore the adiabatic index is given by:
\begin{equation}
    \gamma = \frac{5}{3}
\end{equation}
\end{mdframed}
It follows that for an ideal gas:
\begin{equation}
    \dbar Q = C_V dT + p dV \implies dU = C_V dT
\end{equation}
We can also use the result that for an ideal gas $C_p=C_V+R$ to write that:
\begin{equation}
    \dbar Q = C_V dT + p dV=(C_p-R) dT + pdV
\end{equation}
Hence using the ideal gas law $pV = RT$ $pdV + Vdp = RdT$ we find that:
\begin{equation}
    \dbar Q = C_p dT - V dp
\end{equation}

\begin{mybox}{RedBrown}{\textbf{\sffamily{{First Law for Ideal Gases}}}}
For an ideal gas undergoing a reversible process, we have that:
\begin{equation}
    dU = C_V dT, \ dW = -pdV
\end{equation}
and consequently:
\begin{align}
         \dbar Q = C_V dT + p dV \\
             \dbar Q = C_p dT - V dp
\end{align}
\end{mybox}
\section{Reversible processes}
Pretty much all processes in the universe are governed by time-reversible laws of physics. This is given by the fact that if we looked at a video of the entire universe, we would not be able to realize whether it is in reverse or not.  The reason we don't see a broken egg reassemble itself and float on top of a table is due to energy dissipation. When an egg falls off a table and breaks, part of its potential energy is dissipated into its surroundings as heat. This \textbf{dissipation} can't be traced back exactly (this has to do with entropy). Similarly, when we delete a file on our computers, we're not truly deleting any information, but converting it into other forms of energy (mostly heat). Tracing back the heat to the original 1s and 0s that encoded the information is impossible, the process is thus irreversible. 

For a process to be reversible we require that it be:
\begin{enumerate}
    \item \textbf{Quasistatic}: the system is always in an equilibrium state or infinitesimally close to one (so that any changes in the state variables are infinitesimal). Suppose for example that we push a piston down a column of gas. If we do this slowly enough, the gas will have enough time to adjust to the moving piston, and hence it will always be in an equilibrium state. If instead we introduce a rapid perturbation, this will send shock waves through the gas, exciting its constituent atoms and creating regions of higher temperature. There is no way to extract the sound waves from these regions of high temperature by moving the piston back.
    \item \textbf{No hysteresis}: hysteresis occurs when a systems time evolution depends on its history. The most common example is the magnetization of iron, where if we remove the magnetizing field slowly, the iron won't go back to its original state, but will instead follow a hysteresis loop as shown below.
\end{enumerate}
\begin{figure}[h!]
    \centering
    \includegraphics[width=10cm]{chapter 2/hysteresis.jpg}
    \caption{Hysteresis loop for the magnetization of iron}
    \label{fig:my_label}
\end{figure}

\begin{mybox}{RedBrown}{\textbf{\sffamily{Reversibility and dissipation}}}
A process involving dissipation is \textbf{irreversible}. 
A reversible process, called \textbf{quasistatic} is a process whose direction can be reversed by an infinitesimal change. A reversible process is therefore \textbf{quasistatic} and without \textbf{hysteresis} or heat dissipation. 
\end{mybox}

The property that the system is always 
in an equilibrium state or infinitesimally close to one is analogous to a ball free to roll on a smooth table. If we want to maintain the ball in an equilibrium state, so that it does not slip, we must move the table very, very, very slowly. In real life forces such as friction make it so that it doesn't take an infinite amount of time, but in an idealized world this process would take infinitely long. 

Since we may define equilibrium as the set of points on an equilibrium surface defined by an equation of state, reversible processes may be defined as processes on this surface. Irreversible processes on the other hand leave this surface. 

Hence when systems approach equilibrium, their trajectory is outside of the equilibrium surface and intersects it once equilibrium has been reached. This process is therefore irreversible, as expected from the second law of thermodynamics. 

\section{Isothermal expansion}
Consider the heat change in a reversible \textbf{isothermal} expansion of an ideal gas, where the temperature remains constant. For an ideal gas we have that $dU = C_V dT = 0$. Inserting this into the first law of thermodynamics:
\begin{equation}
    \dbar W = -pdV=- \dbar Q
\end{equation}
Therefore for an isothermal expansion from $V_1$ to $V_2$, the heat that must be added is:
\begin{equation}
    \Delta Q = \int \dbar Q = \int_{V_1}^{V_2} p dV = \int_{V_1}^{V_2} \frac{RT}{V} dV = RT \ln \frac{V_2}{V_1}
\end{equation}
Hence, for an expansion, $V_2 > V_1$, and $\Delta Q > 0$, and heat must therefore be added to the system, whereas for a compression $\Delta Q$ heat must be taken from the system.

\section{Adiabatic expansion}
An \textbf{adiathermal} process involves no flow of heat, so a system bounded by adiathermal walls is said to be thermally isolated. A process is further classified as \textbf{adiabatic} if it is also reversible. In such a case we have that for an ideal gas:
\begin{equation}
    \dbar Q = 0 \ \text{ and } \ dU = C_V dT
\end{equation}
Hence, using the first law of thermodynamics:
\begin{equation}
    C_V dT = -p dV = - \frac{RT}{V}dV \implies \ln \frac{T_2}{T_1} = -\frac{R}{C_V} \ln \frac{V_2}{V_1}
\end{equation}
but using the adiabatic index:
\begin{equation}
    \gamma = 1+\frac{R}{C_V} 
\end{equation}
we can write:
\begin{equation}
    TV^{\gamma-1} = \text{cnst.}
\end{equation}
or alternatively:
\begin{equation}
    pV^{\gamma}=\text{cnst.}
\end{equation}
The work done on the system is given by:
\begin{equation}
    W = \Delta U = c_v \Delta T = c_v \bigg(\frac{p_2V_2}{R}-\frac{p_1V_1}{R}\bigg)=\frac{1}{\gamma -1}(p_2V_2 - p_1V_1)
\end{equation} 
Using $pV^\gamma = cnst.$ we then find that:
\begin{equation}
    \Delta U = W = \frac{p_1 V_1^\gamma}{\gamma-1}\bigg(V_2^{1-\gamma}-V_1^{\gamma-1}\bigg)
\end{equation}
Therefore during an adiabatic process, reducing the internal energy of a system allows for work to be done on the surrounding environment. 
\begin{mdframed}
\textbf{Example. Adiabatic atmosphere}
The pressure due to a thickness $dz$ of atmosphere (which we model as nitrogen gas) with density $\rho$ is given by:
\begin{equation}
    dp = - \rho g dz \implies \frac{dp}{dz}=-\rho g
\end{equation}
and since $\rho = \frac{m N}{V}$ and $p=\frac{Nk_BT}{V}$ where $m$ is the mass of one molecule, $n$ is the number of nitrogen molecules, we can write:
\begin{equation}
    \frac{dp}{dz}=-\frac{mgp}{k_B T} \implies T \frac{dp}{p} = -\frac{mg}{k_B}dz
\end{equation}
If we model the atmosphere to be adiabatic, so that each parcel of air does not exchange heat with its surrounding, then we can use:
\begin{equation}
    p^{1-\gamma}T^{\gamma}=\text{cnst.}
\end{equation}
to find differentiating:
\begin{equation}
    (1-\gamma)p^{-\gamma}T^\gamma \frac{dp}{dT} + \gamma p^{1-\gamma}T^{\gamma-1} = 0 \implies (1-\gamma)\frac{dp}{p}+\gamma \frac{dT}{T}=0
\end{equation}
Combining this with 2.6.7 we can finally find:
\begin{equation}
\frac{dT}{dz}=-\Big(\frac{\gamma-1}{\gamma}\Big)\frac{mg}{k_B}    
\end{equation}
but using the relation $R=N_A k_B = C_p-C_V$ we quickly find that:
\begin{equation}
    \frac{dT}{dz}=-\frac{Mg}{C_p}=-\frac{g}{c_p}
\end{equation}
where $c_p=\frac{C_p}{M}$ is the specific heat capacity and $M$ is the molar mass of nitrogen. 
\end{mdframed}
\section{Isochoric and Isobaric expansions}
In an \textbf{Isochoric process}, the gas has a constant volume throughout. Therefore, the work done on the system is null, because $\dbar W = p dV = 0$. Therefore, we find that the heat released is:
\begin{equation}
    Q = C_V \Delta T
\end{equation}
and from the first law of thermodynamics:
\begin{equation}
\Delta U = Q = C_V \delta T
\end{equation}

So in an isochoric process, heat enters (leaves) the system thus increasing (decreasing) the internal energy. No work is done on the system, and no work is therefore obtainable from the process. 

Instead, in an \textbf{Isobaric process}, the gas is kept at a constant pressure throughout. Therefore, the work done on the system is:
\begin{equation}
    W = -\int_{V_1}^{V_2} p dV = -p\Delta V
\end{equation}
The heat supplied is then equal to:
\begin{equation}
    Q = C_p \Delta T = (C_V+R) \Delta T
\end{equation}
and from the first law of thermodynamics we find that:
\begin{equation}
    \Delta U = \Delta Q + \Delta W = (C_V+R) \Delta T - p \Delta V
\end{equation}
In an isobaric process, heat enters (leaves) the system. Part of the heat is used by the system to increase (decrease the internal energy)  and the rest is used to do work on the surrounding environment. \begin{mybox}{RedBrown}{\textbf{\sffamily{Thermodynamic quasi-static processes}}}
 \begin{tabular}{|c|c|c|c|c|}
    \hline
       \small \textbf{Process} & \textbf{Condition}&$\Delta U$&$\Delta Q$&$\Delta W$  \\
       \hline \hline 
       isothermal & $dT =0$ & $\Delta U = 0$ & $\Delta Q =- \Delta W$ & $W=RT\ln \frac{V_1}{V_2}$\\
       adiabatic & $ dQ=0$ & $\Delta U = \Delta W$ & $\Delta Q = 0$ & $W=\frac{1}{\gamma -1}(p_2V_2 - p_1V_1)$\\
        isochoric & $dV=0$&$\Delta U = Q$ & $\Delta Q = C_V \Delta T$ & $\Delta W = 0$ \\
        isobaric & $dp = 0$& $\Delta U = \Delta Q + \Delta W$ & $\Delta Q = C_p \Delta T$ & $\Delta W = - p \Delta V$\\
        \hline
\end{tabular}
\end{mybox}

\chapter{The Second Law}
This chapter will introduce the second law of thermodynamics and its applications in the study of engines, machines operating between two reservoirs of different temperatures. 

\section{Perpetual motion machine}
Throughout history several attempts have been made to construct a \textbf{perpetual motion machine}. Mainly two types of such machines have been though of:
\begin{enumerate}
    \item[(i)] machine that produces more energy than it uses
    \item[(ii)] machine that produces exactly the same amount of energy as it uses, but continues running indefinitely by converting its waste heat into work
\end{enumerate}
Clearly, the first type violates the first law of thermodynamics. The second type of machine however, is not in violation of this principle, and would theoretically be possible given the thermal physics we have studied. 

We are clearly missing a fundamental principle that forbids the "re-usability" of energy, so that no machine is able to convert heat purely into work indefinitely. This principle is the second law of thermodynamics. 

\section{The second law of thermodynamics}
Empirical evidence seem to show that heat always flow from hot bodies to cold bodies, and the reverse process never occurs in thermodynamic isolation. This formulation leads to the \textbf{Clausius' statement} of the second law of thermodynamics:
\begin{mybox}{ForestGreen}{\textbf{\sffamily{Clausius' statement of the second law of thermodynamics}}}
No process can have the sole net result of transferring heat from a colder to a hotter body. 
\end{mybox}
A second, seemingly unrelated statement of the same law can be made, regarding how easy energy can be converted in different forms. This is called \textbf{Kelvin's statement of the second law of thermodynamics}

\begin{mybox}{ForestGreen}{\textbf{\sffamily{Kelvin's statement of the second law of thermodynamics}}}
No process can have the sole net result of converting heat into work completely. 
\end{mybox}

To see the relation between these two statements, we must first look at \textbf{engines}.
\section{The Carnot Engine}
An \textbf{engine} is a machine operating between two reservoirs that converts some heat into work by operating through a \textbf{Carnot cycle}. 
\begin{figure}[h!]
\centering
\begin{subfigure}{0.4\textwidth}
  \centering
  \includegraphics[width=7cm]{chapter 3/carnot cycle.png}
  \label{fig:sub1}
\end{subfigure}%
\begin{subfigure}{0.4\textwidth}
  \centering
  \includegraphics[width=5cm]{chapter 3/carnot engine.png}
  \label{fig:sub2}
\end{subfigure}
\caption{Carnot cycle and engine}
\label{fig:test}
\end{figure}
The cycle consists of:
\begin{enumerate}
    \item[$1\rightarrow 2$] is an isothermal expansion of the gas operating at a higher temperature $T_h$ in which heat $Q_h$ enters from the hot reservoir. 
    \item[$2\rightarrow 3$] is an isentropic (which means adiabatic and reversible) expansion in which the gas is isolated from the reservoirs, but continues expanding by sacrificing its internal energy. The temperature therefore drops to $T_l$.
    \item[$3\rightarrow 4$] is an isothermal compression of the gas operating at a lower temperature $T_l$ in which heat $Q_l$ exits into the cold reservoir.
     \item[$4\rightarrow 1$] is an isentropic (which means adiabatic and reversible) compression in which the gas is isolated from the reservoirs, but continues compressing increasing its internal energy, and thus temperature to $T_h$. 
\end{enumerate}

Note that the process is cyclic, and therefore in one cycle the total internal energy $U$ does not change overall. The first law of thermodynamics then tells us that the work output by the engine is:
\begin{equation}
W= Q_h-Q_l
\end{equation}
Using the results from the previous chapter on isothermal and isentropic processes:

\begin{align}
    1\rightarrow 2&: \ Q_h = RT_h \ln \frac{V_2}{V_1}\\
    2\rightarrow 3&: \ \frac{T_h}{T_l}=\Big(\frac{V_3}{V_2}\Big)^{\gamma -1}\\
    3\rightarrow 4&: \ -Q_l = RT_l \ln \frac{V_4}{V_3}\\
    4\rightarrow 1&: \ \frac{T_l}{T_h}=\Big(\frac{V_1}{V_4}\Big)^{\gamma -1}
\end{align}
where the negative sign in the third equation is due to the fact that the change in energy is negative, whereas the heat transfer $Q_l$ is positive. The first and second equations give:
\begin{equation}
    \frac{V_2}{V_1}=\frac{V_3}{V_4}
\end{equation}
and dividing the second equation by the fourth, and substituting 3.2.6 we find:
\begin{equation}
    \frac{Q_h}{Q_l}=\frac{T_h}{T_l}
\end{equation}
The \textbf{efficiency} $\eta$ of an engine can be seen as the ratio of the output energy and the input energy, how much work must be done to obtain a certain energy output. For the Carnot engine, we find that this efficiency is the ratio of the work output $W$ by the machine, and the heat $Q_h$ provided by the reservoirs (which must be kept at the same temperature). Therefore:
\begin{equation}
    \eta_C=\frac{W}{Q_h}=1-\frac{T_l}{T_h}
\end{equation}
\begin{mybox}{RedBrown}{\textbf{\sffamily{Carnot's Engine}}}
A Carnot engine is a machine operating between two reservoirs of temperature $T_l$ and $T_h$ through a Carnot cycle, and has efficiency $\eta_C=1-\frac{T_l}{T_h}$.
\end{mybox}
Since, by equation 3.2.1 $W<Q_h$, the efficiency of the Carnot machine is always lower than 100$\%$, and as we will see soon, no machine can be more efficient than the Carnot engine. 

It is now evident how the second law of thermodynamics can explain the impossibility of a \textit{perpetuum mobile} of the second type. 
\section{Carnot's theorem}
It turns out that the efficiency of the Carnot engine is maximized, as encapsulated in the \textbf{Carnot's theorem}.

\begin{mybox}{RedBrown}{\textbf{\sffamily{Carnot's Theorem}}}
Of all the heat engines working between two temperatures, the most efficient one is the Carnot engine. Thus all reversible engines working between two temperatures have the same efficiency $\eta_C$.
\end{mybox}
\begin{proof}
Suppose that there does exist an engine E more efficient than the Carnot engine. Because all the processes in a Carnot cycle are reversible, we run the Carnot engine in reverse and connect it to engine E as shown below. 
\begin{figure}[h!]
    \centering
    \includegraphics[width=6cm]{chapter 3/carnot theorem.png}
    \caption{Engine E connected to Carnot engine run in reverse}
    \label{fig:my_label}
\end{figure}


From the first law of thermodynamics we have:
\begin{equation}
    W = Q'_h-Q'_l = Q_h - Q_l \implies Q_h-Q'_h = Q_l-Q'_l
\end{equation}
Now because $\eta_C>\eta_E$, we must have that $\frac{W}{Q'_h}<\frac{W}{Q_h}$ and thus $Q_h > Q'_h$.

So $Q_h-Q'_h>0 \implies Q_l-Q'_l>0$. Since $Q_h-Q'_h$,the amount of heat dumped into the reservoir $T_h$ is positive and equal to $Q_l-Q'_l$,  the amount of heat extracted from reservoir $T_l$, this engine has the sole purpose of transferring heat from a cold body to a hotter body, and therefore violates Clausius's statement of the second law of thermodynamics. 

To prove that all reversible engines have the same efficiency, consider a reversible engine R connected to a Carnot engine as shown below:
\begin{figure}[h!]
    \centering
    \includegraphics[width=6cm]{chapter 3/carnot theorem 2.png}
    \caption{Reversible engine R connected to a Carnot engine.}
    \label{fig:my_label}
\end{figure}


Then its efficiency, by Carnot's theorem, is $\eta_E \leq \eta_C$. Using the same logic as before, we find that the system only dumps heat from the cold reservoir to the hot reservoir for $\eta_E < \eta_C$, thus violating the Clausius' statement. The only possibility is therefore to have $\eta_E = \eta_C=1-\frac{T_l}{T_h}$. 
\end{proof}
\section{The equivalence of Clausius' and Kelvin's statements}
To prove that the two are equivalent, it suffices to prove that:
\begin{enumerate}
    \item[(i)] if a system violates Kelvin's statement then it violates Clausius' statement.
    \item[(ii)] if a system violates Clausius' statement violates Kelvin's statement
\end{enumerate}
\begin{proof}
\begin{enumerate}
    \item[(i)] If an engine violates Kelvin's statement (so that all the heat is converted to work), then we can connect it to a Carnot engine as shown below:
    \begin{figure}[h!]
        \centering
        \includegraphics[width=6cm]{chapter 3/kelvin violator.png}
        \caption{Kelvin violator is connected to a Carnot engine}
        \label{fig:my_label}
    \end{figure}
    From the first law:
    \begin{equation}
        Q'_h = W, \ \ Q_h = W + Q_l
    \end{equation}
    So the heat dumped into the reservoir $T_h$ is $Q_h-Q_h'=Q_l$ which is the heat extracted from the reservoir $T_l$. Hence this system is transferring heat from a colder body to a warmer body. 
    \item[(ii)] If a system violates Clausius' statement, then its only effect is to transfer heat from the reservoir at $T_l$ to the reservoir at $T_h$. We can then connect it to a Carnot engine as shown:
    \begin{figure}[h!]
        \centering
        \includegraphics[width=6cm]{chapter 3/clausius violator.png}
        \caption{Clausius violator is connected to a Carnot engine}
        \label{fig:my_label}
    \end{figure}
    
    
    From the first law $Q_h - Q_l=W$, so the sole effect of the system is to convert heat into work, thus violating Kelvin's statement. 
\end{enumerate}
\end{proof}
\begin{mdframed}
\textbf{Example.} 
\begin{enumerate}
    \item[(a)] \textbf{Refrigerator}
    
    
    The refrigerator is a heat engine run in reverse. The cold reservoir is the cold food inside the refrigerator, and the hot reservoir is the kitchen. Therefore the efficiency here will be the heat sucked out of the contents (which is what we wish to get) divided by the work required to achieve this:
    \begin{equation}
        \eta = \frac{Q_l}{W}
    \end{equation}
    For a refrigerator with a Carnot engine, we can show:
    \begin{equation}
        \eta_C = \frac{T_l}{T_h-T_l}
    \end{equation}
    which is sometimes greater than $100\%$
    \item[(b)] \textbf{Heat pump}
    
    A heat pump is a refrigerator, used to pump heat from a reservoir to another place. The effect we want is to add heat $Q_h$ to this place, and $W$ is the work that must be done, so that the efficiency is:
    \begin{equation}
        \eta = \frac{Q_h}{W}
    \end{equation}
    So for a heat pump with a Carnot engine we have that:
    \begin{equation}
          \eta_C = \frac{T_h}{T_h-T_l}
    \end{equation}
    which is always greater than $100\%$. 
\end{enumerate}
\end{mdframed}
 \begin{figure}[t!]
  \centering
  \includegraphics[width=6cm]{chapter 3/refrigerator.png}
\caption{Refrigerator/heat pump designs}
\label{fig:test}
\end{figure}
\section{Clausius' theorem}
Consider a general cycle where in an infinitesimal step $i$ heat $\dbar Q_i$ enters at some point connected to  a reservoir at temperature $T_i$. The total work extracted is:
\begin{equation}
    \Delta W = \sum_{cycle} \dbar Q_i
\end{equation}
We now let each point be supplied with heat via a Carnot engine as shown in (b), operating between a reservoir at temperature $T$ providing heat $\dbar Q_i + \dbar W_i$, and a reservoir $T_i$. Each engine produces work $\dbar W_i$ such that:
\begin{equation}
    \frac{\text{heat to } T_i}{T_i}= \frac{\text{heat from } T}{T}
\end{equation}
and so:
\begin{equation}
    \frac{\dbar Q_i}{T_i}=\frac{\dbar Q_i + \dbar W}{T} \implies \dbar W_i = \dbar Q_i \bigg(\frac{T}{T_i}-1\bigg)
\end{equation}
Because we can't heat completely into work, we must have that the total work in the entire cycle is negative or zero, otherwise we would have converted heat to work.

\begin{figure}[h!]
    \centering
    \includegraphics[width=7cm]{chapter 3/clausius theorem.png}
    \caption{(a) A general cycle in which heat $\dbar Q_i$ enters (b) Same cycle, but with heat entering through a carnot engine}
    \label{fig:my_label}
\end{figure}

Therefore:
\begin{equation}
    \Delta W + \sum_{cycle} \dbar W_i \leq 0  \implies \sum_{cycle}\dbar Q_i + \dbar W_i \leq 0
\end{equation}
and therefore:
\begin{equation}
    T \sum_{cycle} \frac{\dbar Q_i}{T_i} \leq 0 \implies \frac{\dbar Q_i}{T_i} \leq 0
\end{equation}
since temperature is positive. Turning the sum into an integral:
\begin{equation}
    \oint \frac{\dbar Q}{T} \leq 0
\end{equation}
which is the \textbf{Clausius inequality}.

If the cycle is reversible, then running it in reverse $\dbar Q $ turns into $-\dbar Q$ and we find:
\begin{equation}
    \oint \frac{-\dbar Q}{T} \leq 0 \implies   \oint \frac{\dbar Q}{T}=0
\end{equation}


\begin{mybox}{RedBrown}{\textbf{\sffamily{Clausius' Theorem}}}
For any closed cycle:
\begin{equation}
    \oint \frac{\dbar Q}{T} \leq 0
\end{equation}
where equality only holds for reversible cycles. 
\end{mybox}

In essence, Clausius' theorem emphasizes the fact that the heat available to do work in a closed cycle must always decrease or remain constant. 

\begin{mdframed}
\textbf{Example.} Consider two bodies with heat capacities $C_h$ and $C_l$ used as reservoirs for a Carnot heat engine. We wish to find the total work obtainable. 
We have that:
\begin{align}
    \dbar Q_h &= -C_h dT_h\\
        \dbar Q_l &= -C_l dT_l
\end{align}
For a Carnot engine we also have:
\begin{equation}
    \frac{\dbar Q_l}{T_l}=\frac{\dbar Q_h}{T_h}
\end{equation}
Integrating directly we find:
\begin{equation}
    C_l \ln \frac{T_f}{T_l} = - C_h \ln \frac{T_f}{T_h} \implies T_f^{C_h+C_l}=T_h^{C_h}T_l^{C_l}
\end{equation}
where $T_f$ is the final temperature of the reservoirs. Thus:
\begin{align}
    \Delta Q_h = C_h(T_f-T_f)\\
    \Delta Q_l = C_l(T_f-T_l)
\end{align}
so that the work obtainable is:
\begin{equation}
    \Delta W = \Delta Q_h - \Delta Q_l = C_h(T_f-T_f)-C_l(T_f-T_l)
\end{equation}
\end{mdframed}
\chapter{Entropy}
\section{Definition of Entropy}
Recall that for a reversible process, we have:
\begin{equation}
    \oint \frac{\dbar Q_{rev}}{T}=0 
\end{equation}
This implies that $\frac{\dbar Q_{rev}}{T}$ is a function of state defined as \textbf{entropy} $S$ as:
\begin{mybox}{ForestGreen}{\textbf{\sffamily{Definition:} Entropy}}
The entropy $S$ is a function of state:
\begin{equation}
    dS = \frac{\dbar Q_{rev}}{T}
\end{equation}
so that the change in entropy between two points $A$,$B$ is:
\begin{equation}
    \Delta S = \int_A^B \frac{\dbar Q_{rev}}{T}
\end{equation}
\end{mybox}

For an irreversible change $A \rightarrow B$, and let us form a closed loop by joining it with a reversible change $B \rightarrow A$. Then the Clausius' inequality reads:
\begin{equation}
    \oint \frac{\dbar Q}{T} = \int_A^B \frac{\dbar Q}{T}+\int_B^A \frac{\dbar Q_{rev}}{T}\leq 0
\end{equation}
and rearranging gives:
\begin{equation}
    \int_A^B \frac{\dbar Q}{T} \leq \int_A^B \frac{\dbar Q_{rev}}{T}
\end{equation}
Therefore:
\begin{equation}
    dS \geq \frac{\dbar Q}{T} 
\end{equation}
For a thermally isolated system $\dbar Q = 0$ always and thus:
\begin{equation}
    \boxed{dS \geq 0}
\end{equation}
If we assume that the universe is isolated, then we can write that the total entropy tends to a maximum. 


\begin{mybox}{RedBrown}{\textbf{\sffamily{Entropic statement of the second law}}}
For a thermally isolated system, the entropy must always increase for an irreversible change and stay the same for a reversible change. 

\end{mybox}

\begin{strategy}
\textbf{Example.}
\end{strategy}
\begin{mdframed}
A large reservoir at constant temperature $T_R$ is placed in thermal contact with a small system at temperature $T_S$, and both end up with temperature $T_R$. 


The heat transferred to the system is therefore equal to $\Delta Q =C(T_R-T_S)$. The entropy change in the reservoir can then be calculated as:
\begin{align}
    \Delta S_{res} &= \int \frac{\dbar Q}{T_R} \\
                 &= -\frac{\Delta Q}{T_R}\\
                 &= \frac{C(T_S-T_R)}{T_R}
\end{align}

Instead, the entropy change of the system is :
\begin{align}
    \Delta S_sys &= \int_{T_S}^{T_R} \frac{C dT}{T} = C \ln \frac{T_R}{T_S}
\end{align}

Therefore, the total change in entropy is:
\begin{equation}
    \Delta S = C \bigg(C \ln \frac{T_R}{T_S}+ \frac{T_S}{T_R} - 1\bigg)
\end{equation}

From the plot below, we can see that the entropy is always non-negative, and equal to zero when $T_S = T_R$, which intuitively makes sense since there would be no heat flow.  

\begin{center}
\begin{minipage}[h!]{6cm}
    \centering
        \includegraphics[width=5cm]{chapter 4/entropy plot.png}
    \end{minipage}
\end{center}
\end{mdframed}

\section{Reformulating the First Law}
For a reversible change, we can write $\dbar Q = T dS$, $\dbar W = - p dV$. We can then write:
\begin{equation}
    dU = T dS - p dV
\end{equation}

Luckily, this equation actually holds for irreversible processes as well! This is fundamental equation of thermodynamics:
\begin{mybox}{RedBrown}{\textbf{\sffamily{Fundamental equation of thermodynamics}}}
$$\boxed{dU = T dS - p dV}$$
\end{mybox}

Note that we may write $dU$ using the chain rule as:
\begin{equation}
    dU = \bigg(\frac{\partial U}{\partial S}\bigg)_V dS + \big(\frac{\partial U}{\partial V}\big)_S dV
\end{equation}

from which it follows that:
\begin{equation}
    \begin{cases}
    T&= \big(\frac{\partial U}{\partial S}\big)_V\\
    p&=\big(\frac{\partial U}{\partial V}\big)_S
    \end{cases} \implies \frac{p}{T} = - \bigg(\frac{\partial U}{\partial V}\bigg)_S \bigg(\frac{\partial S}{\partial U}\bigg)_V
\end{equation}

Now, from the reciprocity theorem we must have that $\big(\frac{\partial U}{\partial V}\big)_S \big(\frac{\partial S}{\partial U}\big)_V \big(\frac{\partial U}{\partial V}\big)_S \big(\frac{\partial V}{\partial S}\big)_U=1$ and thus:
\begin{equation}
    \boxed{\frac{p}{T} = -\bigg(\frac{\partial S}{\partial V}\bigg)_U}
\end{equation}

In summary.
\begin{equation}
\begin{cases}
dU = \dbar Q+ \dbar W &\text{ is always true }\\
    \dbar Q = T dS &\text{ is only true for reversible changes}\\
    \dbar W = - pdV &\text{ is only true for reversible changes}\\
    dU = TdS - pdV & \text{ is always true}
\end{cases}
\end{equation}
\section{Joule Expansion}
We now go back to the problem of expanding a gas freely and quasistatically in a container, and try to answer the question of why the particles do not reassemble themselves. 

Indeed, if this were to happen, then the second law of thermodynamics, $dS \geq 0$, would be violated. We see this by noting that since the system is thermally isolated, $dU=0$, and if the particles reassemble themselves into their original volume, $dV \leq 0$. Then, using the fundamental equation of thermodynamics we get $dS < 0$, a contradiction. 

Physically, this means that the particles reassembling themselves does not satisfy the maximum entropy principle. 
This explains why, if we let a gas expand freely in a container, this process is irreversible, and we do not observe the gas particles neatly reassemble themselves in the smaller volume they were initially contained within.
 
 \begin{figure}[h!]
    \centering
    \includegraphics[width=5cm]{chapter 4/Joule_expansion_quasi-static_but_irreversible.svg.png}
    \label{fig:my_label}
\end{figure}
 
 The expansion of the gas is therefore irreversible, because $\Delta S >0$. 
 
 
 Let us calculate the entropy change, we consider one mole of an ideal gas confined to a container of volume $V_0$, which freely expands  isothermically, upon the opening of a tap, into a container of volume $V_1$ \footnote{the fact that the process is isothermal is irrelevant, since $dS$ is a function of state and is independent of path}. 
 
 
 The system is thermally isolated, so $\Delta U = 0$ and since $C_V \Delta T = \Delta U = 0 \implies \Delta T = 0$ so the initial and final temperatures are the same, $T$. Also, $dU = 0 = TdS - p dV$.
 
 
Now using the ideal gas law, $p_iV_0 = RT =p_f V_1 \implies p_f = \frac{V_0}{V_1} p_i$. Now we can write:
\begin{equation}
    \Delta S = \int \frac{\dbar Q}{T} = \int_{V_0}^{V_1} \frac{p(V) dV}{T} = \int_{V_0}^{V_1} \frac{R dV}{V} = R \ln \frac{V_1}{V_0} 
\end{equation}

We now write that $\Delta S = \Delta S_{universe}>0$ since there can be no heat transfer to the surroundings. Hence, the Joule expansion is irreversible.
\section{Entropy change for ideal gas}
For an ideal gas we have that:
\begin{equation}
    \dbar Q = dU - dW = C_V dT + pdV = C_V dT + \frac{nRT}{V} dV
\end{equation}
We cannot integrate this without knowing how the volume changes with temperature, however, if we divide by $T$ we find that:
\begin{equation}
    \frac{\dbar Q}{T} = \frac{C_V dT}{T} + \frac{nR}{V} dV
\end{equation}
which upon integration gives:
\begin{equation}
    \boxed{\Delta S = C_V \ln \frac{T_2}{T_1} + nR \ln \frac{V_2}{V_1}}
\end{equation}

\section{Statistical interpretation of entropy}
In the Statistical Mechanics part of these lectures we will see that entropy may alternatively be defined as
\begin{equation}
    \boxed{S= k_B \ln \Omega}
\end{equation}
called the \textbf{Boltzmann-Planck equation}, where $\Omega$ is the number of microstates consistent with the given macrostate.

\begin{strategy}
\textbf{Example.}
\end{strategy}
\begin{mdframed}
Consider two ideal gases, separated in two containers of volumes $xV$ and $(1-x)V$ at pressure $p$, temperature $T$. From the ideal gas law, $p=\frac{Nk_B T}{V}$, so there are $xN$ moles of gas 1 and $(1-x)N$ moles of gas 2. 

If we now open the tap between the two containers, the gases will mix resulting in an increasing entropy. If we assume an isothermal expansion, $\Delta U = 0$ so $T dS = p dV \implies dS = \frac{p dV}{T}$ and thus:
\begin{align}
    \Delta S &= x N k_B \int_{xV}^V \frac{dV_1}{V_1} + (1-x) N k_B \int_{(1-x)V}^V \frac{dV_2}{V_2}\\
             &= -Nk_B(x\ln x + (1-x)\ln (1-x))
\end{align}
We plot the entropy of mixing below:
\begin{center}
\begin{minipage}[h!]{6cm}
    \centering
        \includegraphics[width=5cm]{chapter 4/entropy mixing plot.png}
    \end{minipage}
\end{center}

We see that entropy is maximum when $x=\frac{1}{2}$ with value $Nk_B \ln 2$, and is lowest when $x=0,1$ obviously. The plot is symmetric about $x=\frac{1}{2}$, since the system too is symmetric (we can exchange the two containers, switch $x \leftrightarrow (1-x))$. 


In the case where $x=\frac{1}{2}$, the two containers have equal volume. So, when the tap is opened, $\Omega$, the number of microstates that can exist must be multiplied by $2^N$, since every molecule can be in either container 1 or container 2, and not only in its original container. So, the change in entropy is $k_B \ln 2^N = N k_b \ln 2$ as required. 
\end{mdframed}

\subsection{Gibb's paradox}
But what if the two gases were identical? In this previous example we assumed that the gases had some distinctive quality that allowed us to say that each molecule could be in either one container or the other. However, with indistinguishable molecules, it does not make sense anymore in the case where $x=\frac{1}{2}$ that the number of microstates is multiplied by $2^N$. The change in entropy would therefore be: $\Delta S = 0$, and not $\Delta S = N k_b \ln 2$. 


We see that the results for distinguishable particles cannot be used for identical particles. 

\subsection{Maxwell's demon}
Another paradox that attempts to contradict the second law of thermodynamics is \textbf{Maxwell's demon}. A gas is initially in a chamber, and performs a Joule expansion into a second connected chamber. This process, as we explained earlier through the maximum entropy principle, must be irreversible. However, Maxwell came up with the idea of inserting an intelligent valve (the so-called demon), that upon sensing a particle directed towards the initial container, opens only allowing that particle to return to its original vessel. If we repeat this process for long enough, we should get all the particles back to the original container, thus reversing the Joule expansion and reducing the entropy of the system. 


The same demon could be used to partition fast gas molecules into one container, and slower gas molecules into the other, thereby transferring heat without doing work. 


It turns out that to function properly, the demon must store information to understand where the molecules are going. This leads to an increase in entropy, called the \textbf{Shannon entropy}.

\section{Gibbs expression for entropy}
Consider a system with $N$ different microstates. Each $i$th microstate has $n_i$ contained within it, all identical to each other, and not directly measurable. The probability of observing the system in the $i$th microstate is $P_i =  \frac{n_i}{N}$, with $N = \sum_i n_i$. 

The total entropy is, of course, $S_{tot} = k_B \ln N$. However, we may express this total entropy as the sum of the entropy $S$ due to the freedom of choice of each microstate, and the entropy $S_{micro}$ due to the different sub-microstates. So:
\begin{equation}
    S_{tot} = S + S_{micro}
\end{equation}
We cannot measure $S_{micro}$ directly, we can take the expectation value:
\begin{equation}
    S_{micro} = \sum_i P_i S_i = \sum_i k_B \ln n_i
\end{equation}
so that:
\begin{equation}
    S = k_B \bigg(\ln N - \sum_i P_i \ln n_i  \bigg)
\end{equation}
and since $\ln \frac{n_i}{N} = \ln P_i$ we find that:
\begin{equation}
    \boxed{S = - k_B \sum_i P_i \ln P_i}
\end{equation}
It follows that the configuration with the most possible microstates will also maximize the entropy, and lead to equilibria. Systems approaching a state of equilibrium will therefore maximize the number of microstates they can occupy. 
\chapter{Thermodynamic potentials}
\section{Definitions}
We have seen that the internal energy of a system is a very useful function of state. It turns out that there are several other such quantities with units of energy, called \textbf{thermodynamic potentials}. They can all be found by adding to $U$ several combinations of $p,V,T,S$. We define them below:
\begin{mybox}{ForestGreen}{\textbf{{Definition}: Thermodynamic potentials}}
We define the following thermodynamic potentials:
\begin{equation}
    \begin{cases}
    \text{Internal energy}: & U\\
    \text{Enthalpy}: & H = U+pV\\
    \text{Hemholtz function}: & F = U-TS\\
    \text{Gibbs function}: & G = U+pV-TS 
    \end{cases}
\end{equation}
\end{mybox}
\section{Differentials}
\subsection*{Internal energy}
The internal energy may be interpreted as the energy needed to create a system, with all its internal degrees of freedom. 
We find that:
\begin{align}
    dU &= TdS-pdV\\
    &= \bigg(\frac{\partial U}{\partial S}\bigg)_V dS + \bigg(\frac{\partial U}{\partial V}\bigg)_S dV  
\end{align}
which implies:
\begin{equation}
    \begin{cases}
    T = \big(\frac{\partial U}{\partial S}\big)_V \\
    p = -\big(\frac{\partial U}{\partial V}\big)_S\\
\end{cases}
\end{equation}
This suggests that we use $S,V$ as the pair of independent thermodynamic variables to define the internal energy. Such pairs will be known as \textbf{natural variables}.
\begin{mybox}{ForestGreen}{Definition: Natural variables}

The natural variables of a quantity are a set of independent thermodynamic variables that fully specify it through partial differentiation. 
\end{mybox}


\subsection*{Enthalpy}
Enthalpy is interpreted as the energy needed to create a system and the work needed to make space for it, at constant pressure. It is therefore the heat released/absorbed by a system undergoing reversible isobaric processes. It is particularly useful in laboratories, where pressure is almost always held constant. 
We find that:
\begin{align}
    dH &= TdS - pdV + pdV +  Vdp \\
    &= TdS + Vdp\\
    &= \bigg(\frac{\partial H}{\partial S}\bigg)_p dS + \bigg(\frac{\partial H}{\partial p}\bigg)_S dp  
\end{align}
which implies:
\begin{equation}
    \begin{cases}
    T = \big(\frac{\partial H}{\partial S}\big)_p \\
    V = -\big(\frac{\partial H}{\partial p}\big)_S\\
\end{cases}
\end{equation}
\subsection*{Hemholtz function}
The Hemholtz function may be interpreted as the maximum amount of work you can get out of a system at constant temperature and volume. Alternatively, it is the energy needed to create a system, minus the energy that can be supplied by the environment. 


We find that:
\begin{align}
    dF &= TdS - pdV - TdS - S dT \\
    &= -S dT - p dV\\
    &=\bigg(\frac{\partial F}{\partial T}\bigg)_V dT + \bigg(\frac{\partial H}{\partial V}\bigg)_T dV  
\end{align}
which implies:
\begin{equation}
    \begin{cases}
    S = -\big(\frac{\partial F}{\partial T}\big)_V \\
    p = -\big(\frac{\partial F}{\partial V}\big)_T\\
\end{cases}
\end{equation}

\subsection*{Gibb's function}
Finally, we may interpret the Gibb's function as the energy needed to make a system and make room for it, minus the amount of work available from the surroundings. It is alternatively the maximum amount of work that may be performed at constant temperature and pressure. 
We find that:
\begin{align}
    dG &= TdS + Vdp - TdS - SdT \\
    &= Vdp - S dT\\
    &=\bigg(\frac{\partial G}{\partial p}\bigg)_T dp + \bigg(\frac{\partial G}{\partial T}\bigg)_p dT
\end{align}
which implies:
\begin{equation}
    \begin{cases}
    S = -\big(\frac{\partial G}{\partial T}\big)_p \\
    V = \big(\frac{\partial G}{\partial p}\big)_T\\
\end{cases} 
\end{equation}


In summary:
{\renewcommand{\arraystretch}{3}
\begin{table}[h!]
    \centering
    \begin{tabular}{c|c|c}
      Potential & First law & Other fundamental variables \\
      \hline \hline 
       $U(S,V) = TS-pV$  &  $dU = TdS - pdV$ &  $\begin{cases}
    T = \big(\frac{\partial U}{\partial S}\big)_V \\
    p = -\big(\frac{\partial U}{\partial V}\big)_S
    \end{cases}$\\
    $H(S,p)=U+pV$ & $dH = TdS+Vdp$ &    $\begin{cases}
    T = \big(\frac{\partial H}{\partial S}\big)_p \\
    V = -\big(\frac{\partial H}{\partial p}\big)_S\\
\end{cases}$\\
$F(T,V) = U-TS$ & $dF = -SdT - pdV$ & $\begin{cases}
    S = -\big(\frac{\partial F}{\partial T}\big)_V \\
    p = -\big(\frac{\partial F}{\partial V}\big)_T\\
\end{cases}$\\
$G(T,p)=U-TS+pV$ & $dG = -SdT + Vdp$ & $\begin{cases}
    S = -\big(\frac{\partial G}{\partial T}\big)_p \\
    V = \big(\frac{\partial G}{\partial p}\big)_T\\
\end{cases}$
    \end{tabular}
    \label{tab:my_label}
\end{table}}
\section{Integrals under constant variable}
\subsection*{Isochoric processes}
At constant volume, we have an isochoric process. Here the internal energy $U$ is the most useful, since the expression for $U$ simplifies to:
\begin{equation}
    dU = TdS 
\end{equation}
and \begin{equation}
    T dS = T \bigg(\frac{\partial S}{\partial T}\bigg)_V = \bigg(\frac{\partial Q_{rev}}{\partial T}\bigg)_V \equiv C_V
\end{equation}. Therefore the internal energy change:
\begin{equation}
    \Delta U = \int_{T_1}^{T_2} C_V dT  
\end{equation}
represents the heat reversibly absorbed by the system. 

\subsection*{Isobaric processes}

At constant pressure, we have an isobaric process. Here the enthalpy is the most useful, since the expression for $H$ simplifies to:
\begin{equation}
    dH = T dS
\end{equation}
and \begin{equation}
    T dS = T \bigg(\frac{\partial S}{\partial T}\bigg)_p = \bigg(\frac{\partial Q_{rev}}{\partial T}\bigg)_p \equiv C_p
\end{equation} 
so
\begin{equation}
    \Delta H = \int_{T_1}^{T_2} C_p dT  
\end{equation}
is the heat reversibly absorbed by the system. 
\begin{enumerate}
    \item[(i)] If heat enters the system, $\Delta H > 0$ and we have an \textbf{endothermic} reaction. 
    \item[(ii)] If heat leaves the system, $\Delta H<0$ and we have an \textbf{exothermic} reaction. 
\end{enumerate}

\subsection*{Isothermal processes}
At constant temperature, we have an isothermal process. Here the hemholtz function is the most useful, since the expression for $F$ simplifies to:
\begin{equation}
    dF = -p dV
\end{equation}
so:
\begin{equation}
    \Delta F = \int_{V_1}^{V_2}-pdV  
\end{equation}
is the work done reversibly on the system (remember $dW=-pdV$ only for reversible processes).

\subsection*{Isothermal and isobaric processes}
If we keep both pressure and temperature constant, then $dG = 0$ so that:
\begin{equation}
    \Delta G = 0
\end{equation}
so the Gibbs function is a conserved quantity. 



Note that if we know any one of the thermodynamic potentials, we get the other free for free. Consider for example the case where the Hemholtz free energy $F(T,V)$ is known. Then we have found that:
\begin{equation}
    \begin{cases}
    S = -\big(\frac{\partial F}{\partial T}\big)_V \\
    p = -\big(\frac{\partial F}{\partial V}\big)_T\\
\end{cases}
\end{equation}
The internal energy is then:
\begin{equation}
    U(S,V) = F+TS = F-T\bigg(\frac{\partial F}{\partial T}\bigg)_V  = -T^2 \frac{\partial}{\partial T}\bigg(\frac{F}{T}\bigg)_V
\end{equation}
and similarly the enthalpy will be:
\begin{equation}
    H(S,p) = U+pV = F-T\bigg(\frac{\partial F}{\partial T}\bigg)_V -\bigg(\frac{\partial F}{\partial V}\bigg)_T
\end{equation}
Finally the Gibbs free energy will be:
\begin{equation}
    G(T,p) = F+pV = F-V\bigg(\frac{\partial F}{\partial V}\bigg)_T = -V^2 \frac{\partial}{\partial V}\bigg(\frac{F}{V}\bigg)_T
\end{equation}
\section{Legendre transformations}
\begin{mybox}{RedBrown}{\textbf{The Legendre Transformations}}

Consider a function $f(x)$ such that $\frac{df}{dx} = u(x)$ with $u(x)$ invertible. Let us then define:
\begin{equation}
    g(u) = f(x(u)) - u\cdot x(u)  \implies \frac{dg}{du} = x(u)
\end{equation}
One calls $f$ and $g$ \textbf{Legendre transforms} of each other, and to transform from one to the other we simply exchange $x \longleftrightarrow u$. Legendre transforms are such that their derivatives are inverses of each other.
\end{mybox}

Let us take for example $f(x) = x^3$. Then $f'(x) = u(x)=3x^2 \implies x(u) = \sqrt{\frac{u}{3}}$ and hence $g(u) =\Big(\sqrt{\frac{u}{3}}\Big)^3-u\sqrt{\frac{u}{3}} = \big(\frac{u}{3}\big)^{3/2}$. Hence the Legendre transform of $f$ will be $g(x) = \big(\frac{x}{3}\big)^{3/2}$, which is indeed $f(x(u))$.  


Legendre transformations are particularly relevant when discussing thermodynamic potentials, which are all Legendre transforms of each other. 


Indeed, starting from $dU= TdS -p dV$ suppose we wish to instead find some potential $H$ with natural variables $S,p$ rather than $S,V$. Then we clearly must interchange $p,V$, and we may do so by applying the Legendre transformation with $x=-p$ and $u=V$. We find that:
\begin{equation}
    dH = dU+d(pV) = TdS - pdV + pdV + Vdp = TdS + VdP
\end{equation}
as we found earlier. This process can be repeated to find the other potentials. 


Not only, suppose we had a system with more than two degrees of freedom, such as a rod. In the next chapter we will see that the first law reads:
\begin{equation}
    dU = TdS + fdL - pdV
\end{equation}
Suppose we want a new potential $X$ with natural variables $T,L,p$ rather than $S,L,V$. Then we must write that:
\begin{equation}
    dX = TdS + fdL - pdV - d(TS)+d(pV)=-SdT + fdL + Vdp
\end{equation}
implying that we use:
\begin{equation}
    X = U-TS+pV
\end{equation}
\section{Equilibrium conditions}
It is still unclear how these quantities may be put to use effectively. We will see however that the thermodynamic potentials provide an effective way to define how equilibrium may be reached in different conditions. 


Consider a system with $T,p$ absorbing heat $\dbar Q$. The entropy of the surroundings changes by $dS_0 = - \frac{\dbar Q}{T}$, where all the heat exchange is reversible since the surroundings are in thermal equilibrium. Since $dS_{tot} = dS_0 + dS \geq 0$, where $dS$ is the change in entropy of the system, we must have that $T dS \geq \dbar Q$. Using the first law:
\begin{equation}
    \dbar W \geq dU - TdS
\end{equation}
Now notice that $\dbar W = -p_0 dV + \dbar W_{mech}$ where $\dbar W_{mech}$ is any mechanical work that can be done on the system, and $-p_0 dV$ is the work done by the surroundings on the system as its volume changes. 

So, we find that:
\begin{equation}
    \dbar W_{mech} \geq dU + p_0 dV - T_0dS = d(U+p_0V-T_0S)
\end{equation}
since $p_0, T_0$ are constants (the surroundings are in thermodynamic equilibrium).
\begin{mybox}{ForestGreen}{Definition: Availability}
We define the availability as:
\begin{equation}
    \boxed{A \equiv U+p_0V-T_0S }
\end{equation}
which is the maximum amount of work which we can extract from a system in surroundings with temperature $T_0$ and pressure $p_0$. 
\end{mybox}

To see why we can interpret the avialability as such, suppose we have a system with temperature $T$ and pressure $p$ fitted with a piston, in surroundings with temperature $T_0$ and pressure $p_0$. The maximum amount of work is extracted when the process is reversible, hence when:
\begin{equation}
    dU = TdS - pdV
\end{equation}
and thus 
\begin{equation}
    dA = (T-T_0) dS - (p-p_0) dV
\end{equation}
The second term is the net mechanical work done on the piston through a change of volume. To see the significance of the first term, suppose we run a reversible Carnot engine between the system and the surroundings. We must have that the work done by the engine be:
\begin{equation}
    \dbar W_e = T_0dS-TdS = -(T-T_0) dS
\end{equation}
Therefore the first term represents the maximum work we may extract from the system through a change of entropy. 


We can finally write:
\begin{equation}
    \boxed{\dbar W \geq dA}
\end{equation}

so the mechanical work done on a system must always be greater than or equal to the available work the system can do. Normally, the system is mechanically isolated, so we may write:
\begin{equation}
    dA \leq 0
\end{equation}


Equilibrium is reached when $dA = 0$, that is, when we minimize $A$, as a direct result of maximizing the global entropy (we started with the inequality $dS_{tot}\geq 0$). 

In the following we consider mechanically isolated systems, so that we may assert $dA\leq 0$. 


\subsection*{Thermally isolated system at constant volume}
No heat can leave, and the volume can't change so that no work can be done by the system. Therefore $dU = 0$ and so $dA = -T_0 dS$, the availability is minimized when the system's \textbf{entropy is maximum}.

\subsection*{Thermally isolated system at constant pressure}
Since no heat can leave we have that $dS = 0$, as well as $dp=0$ due to the constant pressure constraint. Hence $dA = dU + p_0dV=dU + pdV + Vdp = dH$. Consequently, the availability is minimized when the system's \textbf{Enthalpy is minimum}. 

\subsection*{Constant volume and temperature}
Since $dV = 0$ and $dT = 0$ we have that $dA = dU -T_0dS= dU - TdS = dF$. Consequently the availability is minimized when the system's \textbf{Helmholtz function is minimum}

\subsection{Constant pressure and temperature}
We have that $dp=dT=0$ so  $dA = dU+p_0dV- T_0 dS = dU + pdV + TdS=dG$, the availability is minimized when the system's \textbf{Gibb's function is mimimum}.

Note that these conditions are all equivalent formulations of the maximum \textit{global} entropy principle, but are more or less useful in different situations where different constraints are imposed.

Our set of equilibrium conditions therefore reads:
\begin{align}
    dS = 0, \ dV = 0, \ dU = 0\\
    dS = 0, \ dp = 0, \ dH = 0\\
    dT = 0, \ dV = 0, \ dF = 0\\
    dT = 0, \ dp = 0, \ dG = 0
\end{align}
If any two of the quantities in an above row is specified and kept constant, then the third must be minimized (or maximized for entropy). We therefore get the following conditions (only the most useful) for equilibria:
\begin{table}[h!]
    \centering
    \begin{tabular}{c|c}
    Variables & Equilibrium condition \\
    \hline 
        $T,p$ & $G$ minimized \\
        $T,V$ & $F$ minimized \\
        $U,V$ & $S$ maximized \\
        $S,p$ & $H$ minimized
    \end{tabular}
    \label{tab:my_label}
\end{table}
\section{Connecting the potentials: Maxwell's relations}
Consider a state function $f(x,y)$, then we may write that:
\begin{equation}
    df = \bigg(\frac{\partial f}{\partial x}\bigg)_y dx + \bigg(\frac{\partial f}{\partial y}\bigg)_x dy = F_x dx + F_y dy
\end{equation}
where $F_x = \big(\frac{\partial f}{\partial x}\big)_y$ and $F_y = \big(\frac{\partial f}{\partial y}\big)_x$. Also, by symmetry of the second derivative:
\begin{equation}
    \frac{\partial^2 f}{\partial x \partial y}= \frac{\partial^2 f}{\partial y \partial x}
\end{equation}
so that:
\begin{equation}
    \bigg(\frac{\partial F_x}{\partial y}\bigg)_x =   \bigg(\frac{\partial F_y}{\partial x}\bigg)_y
\end{equation}


We can apply this idea to the thermodynamic potentials to obtain Maxwell's relations. 


For example, the Maxwell relation based on $G$, the Gibb's function, can be found as follows. Firstly, we write down the exact differential equation:
\begin{equation}
    dG = -SdT + Vdp
\end{equation}
Hence:
\begin{equation}
    -  \bigg(\frac{\partial S}{\partial p}\bigg)_T =   \bigg(\frac{\partial V}{\partial T}\bigg)_p
\end{equation}

Repeating this for the other three potentials, we find that:
\begin{subequations}
\begin{empheq}[box=\widefbox]{align}
  -  \bigg(\frac{\partial S}{\partial p}\bigg)_T &=   \bigg(\frac{\partial V}{\partial T}\bigg)_p\\
    -  \bigg(\frac{\partial p}{\partial S}\bigg)_V &=   \bigg(\frac{\partial T}{\partial V}\bigg)_S\\
  \bigg(\frac{\partial V}{\partial S}\bigg)_p&=  \bigg(\frac{\partial T}{\partial p}\bigg)_S\\
    \bigg(\frac{\partial S}{\partial V}\bigg)_T &=   \bigg(\frac{\partial p}{\partial T}\bigg)_V
\end{empheq}
\end{subequations}

To remember these, the following mnemonic may be used:
\begin{enumerate}
    \item[(i)] Cross mutliplication of the variables gives $(TS) = (pV)$. 
    \item[(ii)] Opposite pairs of variables (both in numerator or both in denominator) are set as constants. 
    \item[(iii)] The sign is positive when $T$ appears with $p$ in the derivative. 
\end{enumerate}
The Maxwell's equations are supremely powerful, because they relate partial differentials that are easy to measure, shown to the right of the equality, with partial differentials that are very difficult to measure shown to the left of the equality. 


\begin{strategy}
\textbf{Example.} Find expressions for $\Big(\frac{\partial C_p}{\partial p}\Big)_T$ and $\Big(\frac{\partial C_V}{\partial V}\Big)_T$. 
\end{strategy}
\begin{mdframed}
\begin{solution}
Firstly, note that:
\begin{equation}
    C_V = \bigg(\frac{\partial Q}{\partial T}\bigg)_V
\end{equation}
Also:
\begin{equation}
    dU = \dbar Q + dW = TdS - pdV
\end{equation}
Taking the volume to be constant and differentiating with respect to temperature
\begin{equation}
   \bigg( \frac{\partial U}{\partial T}\bigg)_V = T \bigg(\frac{\partial S}{\partial T}\bigg)_V
\end{equation}
Since in a reversible process $dU = dQ$, we find that:
\begin{equation}
   \bigg( \frac{\partial Q}{\partial T}\bigg)_V = C_V =T \bigg(\frac{\partial S}{\partial T}\bigg)_V
\end{equation}
and similarly:
\begin{equation}
    C_p = T\bigg(\frac{\partial S}{\partial T}\bigg)_p
\end{equation}
Therefore:
\begin{align}
    \bigg(\frac{\partial C_p}{\partial V}\bigg)_T&=\Bigg[\frac{\partial}{\partial p} T  \bigg(\frac{\partial S}{\partial T}\bigg)_p\Bigg]_T\\
    &=T\Bigg[\frac{\partial}{\partial T}   \bigg(\frac{\partial S}{\partial p}\bigg)_T\Bigg]_p
    = -T \bigg(\frac{\partial^2 V}{\partial T^2}\bigg)_p
\end{align}
Similarly:
\begin{equation}
     \bigg(\frac{\partial C_V}{\partial V}\bigg)_T = T \bigg(\frac{\partial^2 p}{\partial T^2}\bigg)_V
\end{equation}
\end{solution}
\end{mdframed}

In general, the following strategies may help in solving problems regarding Maxwell's relations:
\begin{enumerate}
    \item[(i)] write down the differential of a thermodynamic potential:
    \begin{equation}
         df = \bigg(\frac{\partial f}{\partial x}\bigg)_y dx + \bigg(\frac{\partial f}{\partial y}\bigg)_x dy
    \end{equation}
    \item[(ii)] use the reciprocal theorem:
    \begin{equation}
        \bigg(\frac{\partial x}{\partial z}\bigg)_y=\frac{1}{ \big(\frac{\partial z}{\partial x}\big)_y}
    \end{equation}
    \item[(iii)] Use the reciprocity theorem:
    \begin{equation}
         \bigg(\frac{\partial x}{\partial y}\bigg)_z \bigg(\frac{\partial y}{\partial z}\bigg)_x \bigg(\frac{\partial z}{\partial x}\bigg)_y=-1
    \end{equation}
    \item[(iv)] Use the heat capacities:
    \begin{equation}
        \frac{C_V}{T}= \bigg(\frac{\partial S}{\partial T}\bigg)_V, \  \frac{C_p}{T}= \bigg(\frac{\partial S}{\partial T}\bigg)_p 
    \end{equation}
    \item[(v)] Define a generalized susceptibility as how a variable changes when a generalized force, a variable which is a partial derivative of the internal energy, such as T or p. 
    \begin{equation}
    \begin{cases}
    \text{Isobaric expansivity}: & \beta_p = \frac{1}{V}\big(\frac{\partial V}{\partial T}\big)_p \\[15pt]
    \text{Adiabatic expansivity}: & \beta_V = \frac{1}{V}\big(\frac{\partial V}{\partial T}\big)_S\\[15pt]
    \text{Isothermal compressibility}: & \kappa_T = -\frac{1}{V}\big(\frac{\partial V}{\partial p}\big)_T \\[15pt]
    \text{Adiabatic compressibility}: & \kappa_S = -\frac{1}{V}\big(\frac{\partial V}{\partial p}\big)_S
    \end{cases}
\end{equation}
Note the expansivities express fractional change in volume as temperature varies, whereas compressibilities fractional change in volume as pressure varies. 
\end{enumerate}

\begin{strategy}
\textbf{Example.} Show that $C_p- C_V = \frac{VT\beta_p^2}{\kappa_T}$ by considering $S=S(T,V)$. 
\end{strategy}
\begin{mdframed}
\begin{solution}
We begin by writing the differential entropy as:
\begin{equation}
dS = \bigg(\frac{\partial S}{\partial T}\bigg)_V dT + \bigg(\frac{\partial S}{\partial V}\bigg)_T dV
\end{equation}
so that:
\begin{equation}
    \bigg(\frac{\partial S}{\partial T}\bigg)_p=\frac{C_p}{T} = \bigg(\frac{\partial S}{\partial T}\bigg)_V + \bigg(\frac{\partial S}{\partial V}\bigg)_T \bigg(\frac{\partial V}{\partial T}\bigg)_p = \frac{C_V}{T} + \bigg(\frac{\partial S}{\partial V}\bigg)_T \bigg(\frac{\partial V}{\partial T}\bigg)_p 
\end{equation}
Using Maxwell's relations and the reciprocity theorem:
\begin{equation}
    \bigg(\frac{\partial S}{\partial V}\bigg)_T = + \bigg(\frac{\partial p}{\partial T}\bigg)_V = -\bigg(\frac{\partial p}{\partial V}\bigg)_T\bigg(\frac{\partial V}{\partial T}\bigg)_p 
\end{equation}
so that:
\begin{equation}
  \bigg(\frac{\partial S}{\partial V}\bigg)_T \bigg(\frac{\partial V}{\partial T}\bigg)_p   = \frac{(V \beta_p)^2}{V\kappa_T} = \frac{V \beta_p^2}{\kappa_T}
\end{equation}
Finally we find that:
\begin{equation}
    C_p - C_V = T\Bigg[\bigg(\frac{\partial S}{\partial V}\bigg)_T \bigg(\frac{\partial V}{\partial T}\bigg)_p\Bigg] = \frac{VT \beta_p^2}{\kappa_T}
\end{equation}
as desired. 
\end{solution}
\end{mdframed}

\begin{strategy}
\textbf{Example.} Find the entropy of 1 mole of ideal gas.
\end{strategy}
\begin{mdframed}
\begin{solution}
For this quantity of an ideal gas, the state equation is $pV = RT$. Hence, we may write $S=S(T,V)$, or, in differential form:
\begin{align}
dS &= \bigg(\frac{\partial S}{\partial T}\bigg)_V dT + \bigg(\frac{\partial S}{\partial V}\bigg)_T dV\\
&= \frac{C_V}{T}dT + \bigg(\frac{\partial p}{\partial T}\bigg)_V dV
\end{align}
Also:
\begin{equation}
    \bigg(\frac{\partial p}{\partial T}\bigg)_V = \frac{\partial}{\partial T} \frac{RT}{V} = \frac{R}{V}
\end{equation}
so that:
\begin{equation}
    dS = \frac{C_V}{T}dT + \frac{R}{V} dV
\end{equation}
which upon integration gives:
\begin{equation}
    S= C_V \ln T + R \ln V + cnst
\end{equation}
\end{solution}
\end{mdframed}

\chapter{Rods, bubbles and gases}
\section{Elastic materials}
We look at a rod of cross-section $A$ and length $L$, held at a constant temperature $T$. If the rod is made of elastic material, then it can be placed under tension $df$. We then define the isothermal Young's modulus $E_T$ as the ratio of stress $\sigma = df/A$ and strain $\epsilon = dL/L$, so:
\begin{equation}
    E_T = \frac{L}{A} \bigg(\frac{\partial f}{\partial L}\bigg)_T
\end{equation}

We also define the linear expansivity as:
\begin{equation}
    \alpha_f= \frac{1}{L}\bigg(\frac{\partial L}{\partial T}\bigg)_f
\end{equation}

It follows that a wire held at constant length is characterized by a tension-temperature relation:
\begin{equation}
    \bigg(\frac{\partial f}{\partial T}\bigg)_L = -\bigg(\frac{\partial f}{\partial L}\bigg)_T\bigg(\frac{\partial L}{\partial T}\bigg)_f = -AE_T \alpha_f
\end{equation}

So, for a material with positive linear expansivity, tension decreases as tempearture increases. This effect is particularly familiar to stringed-instrument players, where the metal strings slacken when placed in hot environments. 


Rubber bands, on the other hands, have negative expansivity, so that as we increase the temperature, the tension too increases. Consequently, if we hang a weight at the end of a rubber band, and subsequently heat it up, the increase in tension will cause the weight to rise. 

Further, we may work with the Hemholtz potential and write:
\begin{equation}
    dF = -SdT + fdL = -\bigg(\frac{\partial S}{\partial T}\bigg)_L dT + \bigg(\frac{\partial f}{\partial L}\bigg)_T dL
\end{equation}
implying that:
\begin{align}
    S &= -\bigg(\frac{\partial S}{\partial T}\bigg)_L \\
    f &= \bigg(\frac{\partial f}{\partial L}\bigg)_T
\end{align}

Following the derivation of the Maxwell relations, we may then write:
\begin{equation}
    \bigg(\frac{\partial S}{\partial L}\bigg)_T = -   \bigg(\frac{\partial f}{\partial T}\bigg)_L = A E_T \alpha_f
\end{equation}

Therefore, for materials such as a metal wire, the act of stretching it isothermally results in an increase in entropy. Indeed, the expansion distorts the crystallites which were previously carefully aligned in a steady state. 


More specifically, in the case of an isothermal, reversible extension, the heat absorbed is:
\begin{equation}
    \Delta Q = T \Delta S = A E_T T \alpha_f \Delta L
\end{equation}

However, in the case of a rubber band, $\alpha_f < 0$,so that an isothermal extension results in entropy decrease. Indeed, unlike the case of metal wires, the rubber molecules are initially coiled up, and stretching it aligns these molecules introducing more order to the system and reducing entropy. 


One can indeed show that increasing the link distance between rubber molecules decreases the entropy. 


One final consideration that must be made is what happens to the internal energy of the rod when it is extended isothermally? 


We can write the first law of thermodynamics as:
\begin{equation}
    dU = TdS + fdL \implies   \bigg(\frac{\partial U}{\partial L}\bigg)_T = T   \bigg(\frac{\partial S}{\partial L}\bigg)_T + f = ATE_T\alpha_f + f
\end{equation}

This term is the sum of the work done on the rod by the tension force, and the heat flowing into the rod due to the change of length. 


\section{Surface tension}
Consider the surface of some liquid surface of area $A$. To change the area of the surface by $dA$, work must be done against the cohesive forces of the interface. This work can be quantified as:
\begin{equation}
    \dbar W = \gamma dA
\end{equation}
where $\gamma$ is known as the surface tension. Now consider the set up shown below, with a piston pushing down and doing work $\dbar W = pdV$ on an incompressible liquid. 

\begin{figure}[h!]
    \centering
    \includegraphics[width=5cm]{chapter 6/surface tension.png}
    \caption{Piston pushing on spherical droplet held by surface tension}
    \label{fig:my_label}
\end{figure}



The liquid forms a spherical droplet suspended from the end of a thin pipe connected to the piston, whose change in volume is $dV = 4\pi r^2 dr$. Then:
\begin{equation}
    dA = 4\pi (r+ dr)^2-4\pi r^2 \approx 8 \pi r dr \implies \dbar W = 8 \pi \gamma r dr
\end{equation}
At the same time,  $\dbar W = p dV = 4 \pi p r^2 dr$, and equating the two expressions for the work done we find that:
\begin{equation}
    8 \pi \gamma r dr = 4 \pi p r^2 dr \implies p = \frac{2\gamma}{r}
\end{equation}
This is the pressure difference between inside and outside the droplet. 



For the case of a spherical bubble, then the surface tension acts on two surfaces. Hence, $da = 16 \pi \gamma r dr$ and thus the pressure difference is $p=\frac{4 \gamma}{r}$. 


The first law of thermodynamics for our surface takes the following form:
\begin{equation}
    dU = TdS + \gamma dA
\end{equation}
so that:
\begin{equation}
     \bigg(\frac{\partial U}{\partial A}\bigg)_T =  T\bigg(\frac{\partial S}{\partial A}\bigg)_T + \gamma
\end{equation}

The Hemholtz function can be written as:
\begin{equation}
    dF = -SdT + \gamma dA
\end{equation}

The Maxwell's relation deriving from these expressions is:
\begin{equation}
    \bigg(\frac{\partial S}{\partial A}\bigg)_T = -  \bigg(\frac{\partial \gamma}{\partial T}\bigg)_A
\end{equation}
Therefore, (6.2.5) can be written as:
\begin{equation}
    \bigg(\frac{\partial U}{\partial A}\bigg)_T =  \gamma-  \bigg(\frac{\partial \gamma}{\partial T}\bigg)_A
\end{equation}

Note that usually, surface tension decreases as we increase temperature, so that both terms in the above expression are positive. The first term describing the energy flowing in the surface due to the external work, and the second expressing the heat flow into the surface. 

For an isothermal stretching of a surface, the heat absorbed by the surface is:
\begin{equation}
    \Delta Q = T  \bigg(\frac{\partial S}{\partial A}\bigg)_T \Delta A = - T \Delta A  \bigg(\frac{\partial \gamma}{\partial T}\bigg)_A >0
\end{equation}
\section{Magnetization}
We know from the electromagnetism course that the work supplied to a magnetic dipole immersed in a magnetic field $d\textbf{B}$ is:
\begin{equation}
    \dbar W = - \textbf{m} \cdot d\textbf{B}
\end{equation}

A system of non-interacting magnetic moments is called a paramagnet. When we apply a magnetic field in the vicinity of a paramagnet, these magnetic moments line up exhibiting paramagnetism. 

The first law of thermodynamics can then be written as:
\begin{equation}
    dU = T dS - m dB
\end{equation}
where $m=MV$ is the magnetic moment, $M$ the magnetization and $V$ the volume. $B$, the magnetic field is expressed as $B = \mu_0(H+M)$. The magnetic susceptibility is defined as:
\begin{equation}
    \chi = \lim_{H \rightarrow 0} \frac{M}{H}
\end{equation}
and is usually much much smaller than $1$. Consequently $B \approx \mu_0 H$, and the suscepbility may be approximated as:
\begin{equation}
    \chi \approx \frac{\mu_0 M}{B}
\end{equation}
Now the Hemholtz differential may be expressed as:
\begin{equation}
    dF = - SdT - mdB
\end{equation}
yielding:
\begin{equation}
     \bigg(\frac{\partial S}{\partial B}\bigg)_T =  \bigg(\frac{\partial m}{\partial T}\bigg)_B \approx \frac{VB}{\mu_0} \bigg(\frac{\partial \chi}{\partial T}\bigg)_B
\end{equation}

Therefore for an isothermal change of the magnetic field:
\begin{equation}
    \Delta Q = T  \bigg(\frac{\partial S}{\partial B}\bigg)_T \Delta B = \frac{TVB}{\mu_0}\bigg(\frac{\partial \chi}{\partial T}\bigg)_B
\end{equation}

Now, paramagnetic systems obey Curie's law $\chi \propto \frac{1}{T}$ as shall be shown later. It follows that $\bigg(\frac{\partial \chi}{\partial T}\bigg)_B<0$ and that consequently $\Delta Q <0$, heat is emitted by the paramagnet. 

The change in temperature in an adiabatic change, instead, is:
\begin{equation}
    \bigg(\frac{\partial T}{\partial B}\bigg)_S = - \bigg(\frac{\partial T}{\partial S}\bigg)_B\bigg(\frac{\partial S}{\partial B}\bigg)_T
\end{equation}
where we used the reciprocity theorem. Since by definition:
\begin{equation}
    C_B = T \bigg(\frac{\partial S}{\partial T}\bigg)_B
\end{equation}
we may finally write:
\begin{equation}
     \bigg(\frac{\partial T}{\partial B}\bigg)_S  = -\frac{TVB}{\mu_0 C_B} \bigg(\frac{\partial \chi}{\partial T}\bigg)_B>0
\end{equation}
It follows that adiabatic demagnetization leads to a drop in temperature. This method may be used in experiments requiring temperatures as low as mK or even $\mu$K


Our physica intuition backs up this result. Indeed, consider a sample of paramagnetic material. Before applying a magnetic field, all the magnetic moments have a stochastic distribution (assuming there is no interaction between them). As we apply a magnetic field, the moments will tend to line up, decreasing the entropy of the system. However, as we raise the temperature, the thermal energy $k_B T$ begins to contrast the magnetic energy, until at some point we reach a random distribution again. This state is characterized by a very high entropy. 


At a low temperature instead, all the magnetic moments align. The ways we can arrange the system is therefore unique, and hence:
\begin{equation}
    S=k_B \ln \Omega = k_B \ln 1 = 0
\end{equation}
as required. 


How does this magnetic cooling occur? The typical process is depicted below:
\begin{figure}[h!]
    \centering
    \includegraphics[width=6cm]{chapter 6/magnetic cooling.png}
    \label{fig:my_label}
\end{figure}


Firstly, $(a \rightarrow b)$ isothermal magnetization is initiated. At constant temperature, the paramagnetic material is subjected to an increasing magnetic field. This leads to a decrease in entropy. 

Then, $(b \rightarrow c)$ adiabatic demagnetization occurs. The sample is thermally isolated and the magnetic field is slowly reduced to zero. This lowers the temperature of the paramagnet, while keeping the entropy constant. 


How does this entropy remain the same? In truth, there is an entropy exchange between the phonons holding together the lattice and the spins of the magnetic moments. 
As we lower the magnetic field the phonons, the so-called "spring forces" holding together the molecules in the lattice, lose entropy, and exchange it to the magnetic moments, the spin, of the paramagnet, which randomize their orientation. 



\part{Kinetic Theory of Gases}

\chapter{Speed distribution}
\section{Energetic considerations}
Consider a sample of gas, constituted by $N$ molecules which we shall model as particles of mass $m$ moving with velocity $\BF{v} = (v_x,v_y,v_z)$. Ignoring rotational and vibrational degrees of freedom, the mean energy of is due only to the mean kinetic energy, thus:
\begin{equation}
   \braket{E} = N \big \langle \frac{mv^2}{2}\rangle 
\end{equation}

Suppose the gas overall is moving with velocity $\braket{\BF{v}} = \BF{u}$. Then, we define the individual velocity of a molecule in this gas as $\BF{w} = \BF{v}-\BF{u}$, that is, the velocity of the molecule as viewed in the frame moving with the gas. Then, since $\braket{\BF{w}}=0$ by definition, we find that:
\begin{equation}
    \braket{E} = \frac{Nm}{2}\braket{|\BF{u}+\BF{w}|^2} = \frac{Mu^2}{2} + N \bigg \langle \frac{mw^2}{2} \bigg \rangle 
\end{equation}
where $M=Nm$ is the mass of the entire gas. The first term is clearly the kinetic energy of the entire system, this is the one we commonly associate with energy. However, there is an additional energy due to the particle's random motion. This is the internal energy we studied in classical thermodynamics, which is only visible when studying the gas at a microscopic scale. 


If we assume that $\BF{u}=0$ by changing frame of reference, then:
\begin{equation}
    \braket{E}=U=N \bigg \langle \frac{mw^2}{2} \bigg \rangle 
\end{equation}

How does this compare with the exact energy of the gas? The exact energy is just the sum of the kinetic energy of each molecule, hence:
\begin{equation}
    E= \sum_i \frac{mv_i^2}{2}
\end{equation}
so that:
\begin{equation}
    N \bigg \langle \frac{mw^2}{2} \bigg \rangle = \sum_i \bigg \langle \frac{mv_i^2}{2} \bigg \rangle
\end{equation}
Now, since $v_i$ and $v_j$ are independent for different particles, $\braket{f(v_i) g(v_j)} = \braket{f(v_i)}\braket{g(v_j)}$. Note that:
\begin{equation}
    E^2 = \bigg(\sum_{i} \frac{mv_i^2}{2}\bigg)^2 = \sum_{i,j} \frac{m^2v_i^2v_j^2}{4} \neq \sum_{i} \bigg(\frac{mv_i^2}{2}\bigg)^2
\end{equation}
Keeping this in mind, we get that:
\begin{align}
    \braket{E^2}-U^2 &= \sum_{i,j} \bigg \langle \frac{m^2v^2_i v^2_j}{4}\bigg \rangle  - \Bigg(\sum_i \bigg \langle \frac{mv_i^2}{2}\bigg \rangle \Bigg)^2\\
    &=\sum_i \bigg \langle \frac{m^2 v_i^4}{4}\bigg \rangle + \sum_{i \neq j} \bigg \langle \frac{mv_i^2}{2}\bigg \rangle \bigg \langle \frac{mv_j^2}{2}\bigg \rangle -  \Bigg(\sum_i \bigg \langle \frac{mv_i^2}{2}\bigg \rangle \Bigg)^2\\
    &= N \bigg \langle \frac{m^2 v^4}{4}\bigg \rangle + N(N-1)\bigg \langle \frac{m v^2}{2}\bigg \rangle^2 - \bigg(N\bigg \langle \frac{m v^2}{2}\bigg \rangle\bigg)^2\\
    &= \frac{Nm^2}{4}(\braket{v^4}-\braket{v^2}^2)
\end{align}
Hence the standard deviation in the energy is:
\begin{align}
    \sigma_E &= \sqrt{\braket{E^2}-\braket{E}^2} =\sqrt{ \frac{Nm^2}{4}(\braket{v^4}-\braket{v^2}^2)}
\end{align}
The relative error is then:
\begin{equation}
    \frac{\sigma_E}{U} = \frac{\sqrt{ \frac{Nm^2}{4}(\braket{v^4}-\braket{v^2}^2)}}{N \braket{\frac{mv^2}{2}}} = \frac{1}{\sqrt{N}}\sqrt{\frac{\braket{v^4}}{\braket{v^2}^2}-1}
\end{equation}

For large numbers of particles, this relative error is very very small. Hence, we can indeed regard large distributions of particles as sharply peaked near $\braket{E}$. In other words, as the number of particles increases, the relative amount that each individual particle's properties varies from the averaged value of the property decreases. This is exactly the prediction made by taking the thermodynamic limit. 

\section{Pressure}
Consider a container immersed in a gas, how can we model the pressure exerted on the walls of the container? 


Let's assume that the particles hitting the wall are colliding elastically, and let the $z$-axis be aligned with the normal to one of the container walls. Then, the velocity components of any given particle before and after the collision are given by:
\begin{equation}
    v_z^{\text{after}}=-v_z^{\text{before}} \implies \Delta p_m = 2mv_z
\end{equation}
where $\Delta p_m$,not to be confused with pressure, is the change in momentum. For the sample of $N(v_z)$ particles whose $v_z$ lie in the infinitesimal interval $[v_z, v_z+dz]$, we define the differential particle flux as:
\begin{equation}\label{flux}
    d \Phi(v_z) = \frac{dN(v_z)}{At}
\end{equation}
that is, the number of particles with such speeds hitting the box per unit time per unit area. Their contribution to the pressure is:
\begin{equation}
  dp(v_z) =  \underbrace{\frac{\Delta p_m N(v_z)}{t}}_{\text{force on wall}} \frac{1}{A} = 2mv_z d\Phi(v_z)     
\end{equation}
Suppose the ratio of particles with speed in the interval $[v_z, v_z+dv_z]$ is given by $f(v_z) dv_z$, so that:
\begin{equation}
    N(v_1,v_2) = \int_{v_1}^{v_2} f(v_z) dv_z
\end{equation}
is the number of particles with speed between $v_1$ and $v_2$. It is the probability density function of velocities, also known as the \textbf{velocity distribution}. Then, $dN(v_z)$ is the density of particles in the gas, $n=\frac{N}{V}$, times the volume in which the particles must reside for them to hit the container in time $t$, $A v_z t$, times the ratio of particles with speed in the interval $[v_z, v_z+dv_z]$, $f(v_z) dv_z$:
\begin{equation}
    dN(v_z) = A v_z t \cdot n \cdot f(v_z) dv_z 
\end{equation}
Inserting this into \eqref{flux} we find:
\begin{equation}
    dp(v_z) = 2mnv_z^2f(v_z)dv_z
\end{equation}
and integrating for all particles (so with $v_z$ ranging from $0$, still particles, to $\infty$):
\begin{equation}
    p = \int_0^\infty 2mnv_z^2 f(v_z) dv_z = \int_{-\infty}^\infty mnv_z^2 f(v_z) dv_z = mn \braket{v_z}^2
\end{equation}
where we assumed that the velocity distribution is symmetrical, $f(v_z)=-f(v_z)$. This is justified by the fact that there should be no preferred motion for the particles to move. Alternatively, one could also consider a rotation in frame of reference, observing a particle hitting the container's wall from two opposite points of view makes no change physically. 

In three dimensions, we can introduce the velocity distribution $f(\BF{v})$. The derivation is exactly the same, and we find that:
\begin{equation}
    \boxed{p=mn \int v_z^2 f(\BF{v}) d^3\BF{v} = mn \braket{v_z^2}}
\end{equation}


Furthermore, for isotropic distributions, where, in addition to orientations, no directions are preferred, then:
\begin{equation}
    \braket{v_z^2}= \braket{v_x^2}= \braket{v_y^2} = \frac{1}{3} \braket{v^2}
\end{equation}
so the pressure is:
\begin{equation}\label{press}
    p = \frac{1}{3}mn\braket{v^2}
\end{equation}
The internal energy is:
\begin{equation}
    U = \frac{1}{2}mN\braket{v^2} = \frac{3 pV}{2} \iff \boxed{p=\frac{2U}{3V}=\frac{2}{3}u}
\end{equation}
where $u$ is the energy density.


Moreover, isotropy forces the velocity distribution to be independent of direction. Using spherical coordinates in the $(v_x,v_y,v_z)$-space:
\begin{equation}
    f(\BF{v}) \rightarrow f(v, \theta, \phi)
\end{equation}
and thus:
\begin{align}
    f(\BF{v}) dv_x dv_y dv_z &= f(\BF{v}) \bigg|\frac{\partial(v_x, v_y, v_z)}{\partial(v,\theta,\phi)}\bigg| dv d\theta d\phi\\
    &=f(\BF{v})  v^2 \sin \theta dv d\theta d\phi = f(v,\theta,\phi) dv d\theta d\phi
\end{align}
implying that:
\begin{equation}
    f(v, \theta, \phi) = f(\BF{v}) v^2 \sin \theta 
\end{equation}
The particle speed distribution is then:
\begin{equation}
    \boxed{g(v) = \int_0^\pi \int_0^{2\pi} f(v, \theta, \phi) d\phi d\theta = 4\pi v^2 f(\BF{v})}
\end{equation}

We can use some geometric insight to get to the same result. In the velocity space, the ratio of particles with speed between $v$ and $v+dv$ is given by the volume of the spherical shell of width $dv$:
\begin{equation}
   g(v) dv = 4 \pi v^2 dv \cdot f(\BF{v})
\end{equation}
coinciding with the results found above. 

\begin{strategy}
\sffamily  \textbf{Example.} Let us consider an anisotropic system, where there exists one special direction in space (call it $z$), which affects the distribution of particle velocities. 
\begin{enumerate}
    \item[a)] What is the expression for pressure $p_{\parallel}$ on the wall perpendicular to the $z$-axis? What is the expression for pressure $p_{\perp}$ on a wall parallel to the $z$-axis? 
    \item[b)] What is a pressure normal to $z$ at an angle $\theta$? 
\end{enumerate}
\end{strategy}
\begin{mdframed}
\begin{solution}
\begin{enumerate}
    \item[a)] Since we now have a special direction $z$, we may use cylindrical coordinates- The velocity distribution will depend on $v_s$ and $v_z$. Then:
    \begin{equation}
        f(\BF{v}) = dv_x dv_y dv_z = f(\BF{v}) v_s d\phi dv_s dv_z
    \end{equation}
    The pressure on a wall with normal along $z$ evaluates to:
    \begin{align}
        P = mn \braket{v_z^2}
    \end{align}
    For a wall whose normal is parallel to $z$ instead, we need to repeat our calculations, but this time with $v_z \mapsto v_s$. We find that:
    \begin{equation}
        \Delta p = 2mv_s \implies dP(v_s) = 2mv_s d\Phi(v_s)
    \end{equation}
    Since $d\Phi(v_s) = v_s n f(v_s) dv_s$ we get $dP(v_s) = 2mv_s^2 nf(v_s) dv_s$. Finally, using the fact that the distribution in $xy$ plane is isotropic, $f(v_s)=f(-v_s)$:
    \begin{equation}
           P=mn \int_{-\infty}^\infty v_s^2 f(v_s) dv_s = mn \braket{v_s^2}
    \end{equation}
   \item[b)] Consequently, for a wall at an angle $\theta$ to $z$.
    \begin{align}
        P = P_{\parallel} \cos \theta + P_\perp \sin \theta \\
        = mn(\braket{v_z^2} \cos \theta + \braket{v_s^2} \sin \theta)
    \end{align}
    where we decomposed the pressure on the wall into a component along $z$ and perpendicular to $z$.
\end{enumerate}
\end{solution}
\end{mdframed}
\section{Deriving the Boltzmann velocity distribution}

We now face the task of determining what form $f(\BF{v})$ takes. We will assume that:
\begin{enumerate}
    \item[(i)] particle-particle interactions are negligible, except for ideal elastic collisions
    \item[(ii)] they are point particles
    \item[(iii)] classical, non-relativistic particles
\end{enumerate}

Suppose then that we have taken this gas in a container of volume $V$, and we have waited for macroscopic equilibrium to occur, so that the velocity distribution is time-independent. 


We can argue that in space there are no preferred directions, so we can use isotropy. Moreover, we may also argue that the velocity vector's components must be independent:
\begin{equation}
    f(\BF{v}) = g(v_x^2)g(v_y^2)g(v_z^2) = h(v^2) \iff \ln (g(v_x^2)g(v_y^2)g(v_z^2) ) = \ln h(v^2)
\end{equation}
If we let $\phi(v_x^2) = \ln g(v_x^2)$ and $\varphi(v^2)=\ln h(v^2)$:
\begin{equation}
    \phi(v_x^2)+\phi(v_y^2)+\phi(v_z)^2 = \varphi(v_x^2+v_y^2+v_z^2)
\end{equation}
If we take the derivative with respect to $v_x^2$:
\begin{equation}
    \partial v_x^2\phi(v_x^2) = \partial v_x^2 \varphi(v_x^2+v_y^2+v_z^2)
\end{equation}
Similarly:
\begin{equation}
    \partial v_y^2\phi(v_x^2) = \partial v_y^2 \varphi(v_x^2+v_y^2+v_z^2)=\partial v_x^2 \varphi(v_x^2+v_y^2+v_z^2)
\end{equation}
and 
\begin{equation}
    \partial v_z^2\phi(v_x^2) = \partial v_z^2 \varphi(v_x^2+v_y^2+v_z^2)=\partial v_x^2 \varphi(v_x^2+v_y^2+v_z^2)
\end{equation}
implying that:
\begin{equation}\label{eqlin}
      \partial v_x^2\phi(v_x^2) =   \partial v_y^2\phi(v_y^2) =  \partial v_z^2\phi(v_z^2) =
\end{equation}
The only possible family of functions $\phi$ satisfying \eqref{eqlin} are functions linear in their argument (we use $-\alpha$ as a convention, it turns out that $\alpha$ is positive):
\begin{equation}
    \phi(v_x^2) = -\alpha v_x^2+\beta,  \ \phi(v_y^2) = -\alpha v_y^2+\beta,  \ \phi(v_z^2) = -\alpha v_z^2+\beta, 
\end{equation}
so that:
\begin{equation}
    \varphi(v^2) = -\alpha v^2 + 3\beta 
\end{equation}

Thus:
\begin{equation}
    f(\BF{v}) = e^{3\beta} e^{-\alpha v^2} = ce^{-\alpha v^2}
\end{equation}

We can now use the normalization condition:
\begin{align}
    1 &= \int f(\BF{v}) d^3\BF{v}\\
    &= c \int e^{\alpha v_x^2} dv_x\int e^{\alpha v_y^2} dv_y \int e^{\alpha v_z^2} dv_z\\
    &= c\bigg(\frac{\pi}{\alpha}\bigg)^{3/2}\\
    \iff c &= \bigg(\frac{\alpha}{\pi}\bigg)^{3/2}
\end{align}
Thus, the velocity distribution becomes:
\begin{equation}
    f(\BF{v}) = \bigg(\frac{\alpha}{\pi}\bigg)^{3/2}e^{\alpha v^2}
\end{equation}

Let us analyze the dimensions of $\alpha$. Clearly, $\frac{1}{\sqrt{\alpha}} = v_{th}$ where $v_{th}$ is some velocity characteristic of our gas sample. Then:
\begin{equation}
    \boxed{f(\BF{v}) = \frac{1}{(\sqrt{\pi}v_{th})^3}e^{-v^2/v_{th}^2}}
\end{equation}
and the related speed distribution is:
\begin{equation}
    \boxed{g(v) = \frac{4\pi v^2}{(\sqrt{\pi}v_{th})^3}e^{-v^2/v_{th}^2}}
\end{equation}
Note that:
\begin{align}
    \braket{v} &= \int_{0}^\infty  v g(v) dv  = \int_{0}^\infty \frac{4\pi v^3}{(\sqrt{\pi}v_{th})^3}e^{-v^2/v_{th}^2} dv \\
    &= \frac{4\pi v_{th}^4}{2(\sqrt{\pi}v_{th})^3}\\
    &=\frac{2}{\sqrt{\pi}} v_{th}
\end{align}
Moreover:
\begin{equation}
    \frac{dg(v)}{dv} = \frac{4}{\sqrt{\pi}v_{th}}\frac{2ve^{-v^2/v_{th}^2}(v_{th}^2-v^2)}{v_{th}^2}=0
    \iff v=v_{th}
\end{equation}
In other words, $v_{th}$ is the most likely speed, since the speed distribution is peaked there. 

Now note that:
\begin{equation}
    P = \frac{1}{3} mn \int v^2 \frac{1}{(\sqrt{\pi}v_{th})^3}e^{-v^2/v_{th}^2}  d^3 \BF{v} = \frac{nmv_{th}^2}{2}
\end{equation}
so we have that:
\begin{equation}
    v_{th}=\sqrt{\frac{2P}{nm}}
\end{equation}
For an ideal gas:
\begin{equation}
    P = n k_B T \implies \boxed{\frac{mv_{th}^2}{2}=k_B T}
\end{equation}
This is a monumental result. The temperature of an ideal gas is the kinetic energy of a particle moving with the most likely speed of the Maxwell speed distribution. Using this expression, the Maxwell distribution takes its final form:
\begin{equation}
    \boxed{f(\BF{v}) = \bigg(\frac{m}{2\pi k_B T}\bigg)^{3/2}\text{exp}\bigg(-\frac{mv^2}{2k_B T}\bigg)}
\end{equation}

Also, we may rewrite $\braket{v}$ as:
\begin{equation}
    \braket{v} = \sqrt{\frac{8k_BT}{\pi m}}
\end{equation}
and similarly:
\begin{equation}
    \braket{v^2} = \frac{3k_BT}{m}
\end{equation}

Then, we may retrieve the ideal gas law:
\begin{equation}
    p = \frac{1}{3}Nm\braket{v^2} = nk_B T \implies pV = Nk_B T
\end{equation}

\section{Effusion}
We place an ideal gas in a container with a small hole in it. The diameter $d$ of the hole is much smaller than the average path that particles travel between each collision, known as the particle mean free path $\lambda_{mfp}$. This ensures that the particles are removed randomly, without changing the overall velocity distribution. 


The differential of the particle flux through the hole (which is the number of particles per unit area per unit time with velocities lying in $[\BF{v}, \BF{v}+d\BF{v}]$) is given by:
\begin{equation}
    d\Phi(\BF{v}) = nv_zf(\BF{v}) d^3\BF{v}=n v^3 f(\BF{v}) dv \cos \theta\sin \theta d\theta d\phi
\end{equation}
since $v_z=v\cos \theta$, and $d^3 \BF{v} = v^2 \sin \theta dv d\theta d\phi$ for an isotropic distribution inside the container. For an isotropic sample, we may further assume that $f(\BF{v}) = f(v)$. 


It follows that the distribution of the particles exiting the container is not isotropic, due to the extra $\cos \theta$ factor. This is physically interpreted by the fact that particles with a small $\theta$, that is perpendicular to the wall with the hole, are more likely to be ejected. Similarly, the distribution is not Maxwellian due to the extra $v$ factor, since faster particles are more likely to exit the container. 


The flux of the particles with speed within $[v, v+dv]$ can be found by integrating\footnote{note that the upper limit for $\theta$ is $\frac{\pi}{2}$, since we only consider the particles moving towards the hole}:
\begin{equation}
    d\hat{\Phi}(v)=nv^3 f(v)dv \int_0^\frac{\pi}{2} d\theta \cos \theta \sin \theta \int_0^{2\pi} d\phi=\pi nv^3f(v)dv = \frac{1}{4}nvg(v)dv
\end{equation}
The total flux of effusing particles is then:
\begin{equation}
    \Phi = \int_0^\infty dv \frac{1}{4}nvg(v) = \frac{1}{4}n\braket{v}
\end{equation}
Since we assume a Maxwell speed distribution inside the container, we find that:
\begin{equation}
    \boxed{\Phi = \frac{n}{4}\sqrt{\frac{8k_BT}{\pi m}} = \frac{P}{\sqrt{2\pi m k_B T}}}
\end{equation}
This is the number of particles effusing per unit time, per unit area through the hole. The total effusion rate is given by $\Phi \cdot A$. 
\begin{strategy}
\sffamily \textbf{Example.} 


a) A gas effuses into a vacuum through a small hole of area $A$. and collimated by passing through a very small circular hole of radius $a$, in a screen a distance $d \gg a$ from the first hole. Calculate the rate at which particles emerge from the circular hole. 
b) Show that if a gas were allowed to leak into an evacuate sphere and the particles condensed where they first hit the surface they would form a uniform coating.
\end{strategy}
\begin{mdframed}
\begin{solution}
\begin{enumerate}
    \item[a)] We know that the number of particles passing through the first hole per unit time is the particle flux times the surface of the first hole:
    \begin{equation}
        T' = \Psi \cdot a = \frac{1}{4}nA \braket{v}
    \end{equation}
    The ratio of the particles which then pass through the second collimating hole is:
    \begin{align}
        T &= A n \int_0^\infty v^3 \int_0^\pi \int_0^{\theta'} vf(v) \cdot v^2 \sin \theta dv d\theta d\phi \\
        &= 2 \pi A n \int_0^\infty v^3 f(v) \bigg[\frac{1}{2} \sin^2 \theta \bigg]_0^{\theta'} dv\\
        &= \frac{An}{4} \sin^2 \theta' \int_0^\infty v g(v) dv\\
        &= \frac{An\braket{v}}{4} \sin^2 \theta'
    \end{align}
    Since $d \gg a$, we may use small angle approximation to write $\sin \theta' \approx \frac{a}{d}$ so that:
    \begin{align}
        T = \frac{Ana^2\braket{v}}{4d^2}
    \end{align}
    \item[b)] The differential flux through the hole into the evcuated sphere is:
    \begin{align}
        d\Phi &= nv^3 f(v) \sin \theta \cos \theta dv d\theta d\phi\\
        &= \frac{1}{2} nv^3 f(v) \sin 2\theta dv d\theta d\phi\\
        &= \frac{1}{4} nv^3 f(v) \sin 2\theta' dv d\theta' d\phi
    \end{align}
    where $\theta'=2\theta$. Taking the origin to be centered at the sphere's center, then we see that $\theta'$ becomes the angular measure and replaces $\theta$. We see that this does indeed produce a uniform coating. 
\end{enumerate}
\end{solution}
\end{mdframed}
\begin{strategy}
\sffamily \textbf{Example.}  
A gas is a mixture of hydrogen and deuterium in the proportion $7000:1$. As the gas effuses through
a small hole from a vessel at constant temperature into a vacuum, the composition of
the remaining mixture changes. By what factor will the pressure in the vessel have
fallen when the remaining mixture consists of hydrogen and deuterium in the proportion $700:1$.
\end{strategy}
\begin{mdframed}
\begin{solution}
The flux of particles effusing through the hole is:
\begin{align}
    &\Phi_{H} = \frac{1}{4} n_H \braket{v_H} = n_H \sqrt{\frac{k_BT}{2\pi m_H}} = \frac{p_H}{\sqrt{2\pi m_H k_B T}}\\
    &\Phi_{D} = \frac{1}{4} n_{D} \braket{v_{D}} = n_{D} \sqrt{\frac{k_BT}{2\pi m_{D}}} = \frac{p_{D}}{\sqrt{2\pi m_{D} k_B T}}
\end{align}

Using the definition of particle flux as the number of particles moving through a surface of unit area per unit time, then taking a volume $V$ and recalling $N=n \cdot V$ for a homogeneous system:
\begin{align}
    &\frac{1}{A} \frac{dN_{H}}{dt} =- \frac{N_H}{V} \sqrt{\frac{k_BT}{2\pi m_H}}\\
    &\frac{1}{A} \frac{dN_{D}}{dt} = -\frac{N_{D}}{V} \sqrt{\frac{k_BT}{2\pi m_{D}}}
\end{align}
Therefore, defining the characteristic time constants $\tau_H = \frac{V}{A}\sqrt{\frac{2\pi m_H}{k_B T}}$ and $\tau_{D} = \tau_H = \frac{V}{A}\sqrt{\frac{2\pi m_D}{ k_B T}}$ then:
\begin{align}
    &N_H = N_{0H} e^{-t/\tau_{H}}\\
    &N_D = N_{0D} e^{-t/\tau_{D}}
\end{align}
It follows that if at time $t=0$ we have $\frac{N_{0H}}{N_{0D}}=7000$, and at $t$ we have $\frac{N_{H}}{N_D}=700$ then:
\begin{equation}
    e^{t/\tau_{H}-t/\tau_{D}} = 10 \implies t = \ln 10 \frac{\tau_H \tau_D}{\tau_D - \tau_H} 
\end{equation}
Using the ideal gas law, $\frac{pV}{k_BT}=N$ we find that:

    \begin{align}
    &p_H = \frac{k_BTN_{0H}}{V} e^{-t/\tau_{H}}=p_{0H}e^{-t/\tau_{H}}\\
    &p_D = \frac{k_BTN_{0D}}{V} e^{-t/\tau_{D}} = p_{0D}e^{-t/\tau_{H}}
\end{align}
so that:
\begin{align}
    \frac{p}{p_0} &=  \frac{p_{0H} e^{-t/\tau_{H}} + p_{0D}\frac{k_BTN_{0D}}{V} e^{-t/\tau_{D}}}{p_{0H}+p_{0D}}\\
    &= \frac{p_{0H}10^{-\frac{\tau_D}{\tau_D-\tau_H}}+p_{0D}10^{-\frac{\tau_H}{\tau_D-\tau_H}}}{p_{0H}+ p_{0D}}\\
    &= \frac{N_{0H}}{N_0}10^{-\frac{\tau_D}{\tau_D-\tau_H}} + \frac{N_{0D}}{N_0}10^{-\frac{\tau_H}{\tau_D-\tau_H}}\\
    &\approx 0.000412
\end{align}
\end{solution}
\end{mdframed}

\section{Collisions}
In the previous section, we mentioned the particle mean free path, but how would be calculate such a quantity?


Let us model the particles in the gas as rigid spheres of diameter $d$. For each particle, we can construct a virtual cylinder whose axis is aligned with $\BF{v}$, and whose cross section is $\sigma=\pi d^2$. Any particle whose center is within this cylinder must therefore collide with this particle, which is why $\sigma$ is known as the collisional cross section. 


After some time $t$ each particle will have swept a volume $\sigma vt$, and since the average number of particles in this volume is $n_c=\sigma v t n$, we define the collision time $t=\tau_c$ such that $n_c=1$, that is:
\begin{equation}
    \sigma v \tau_c n = 1 \implies \tau_c = \frac{1}{\sigma n v}
\end{equation}
But which velocity to use? It is common notation to use the thermal velocity, thus defining:
\begin{equation}
    \boxed{\tau_c = \frac{1}{\sigma n}\sqrt{\frac{m}{2k_BT}}}
\end{equation}
It is then straightforward to calculate the mean free path, that is, the average distance travelled by a particle between collisions:
\begin{equation}
    \boxed{\lambda_{mfp} = v_{th}\tau_C = \frac{1}{\sigma n}}
\end{equation}
This is also the length of the collision cylinder such that at least one particle is included in it.


\chapter{Heat transport phenomena}
\section{Particle distribution function}
Let us define the \textbf{particle distribution function in the phase space} as the average number of particles with velocities in the velocity space volume $d^3 \BF{v}$ and in the spatial volume $d^3 \BF{r}$, denoted as:
\begin{equation}
    F(t,\BF{r},\BF{v}) d^3 \BF{r} d^3 \BF{v}
\end{equation}
Normalization thus requires that for a system of $N$ particles in total:
\begin{equation}
    \iint d^3 \BF{r} d^3 \BF{v}  F(t,\BF{r},\BF{v}) = N
\end{equation}

In a homogeneous system we obviously require that $F(\BF{r}, \BF{v})=F(\BF{v}) = n f(\BF{v})$ where $n = n(\BF{r})$ is the density of particles, which has to be constant. 

Indeed, if we integrate $F$ over the velocity space:
\begin{equation}
    \int d^3\BF{v} F(t,\BF{r},\BF{v}) \equiv n(t,\BF{r}) = n
\end{equation}

The \textbf{first moment} of $F$ is defined as the mean momentum density:
\begin{equation}
    \int d^3 \BF{v} m\BF{v} F(t,\BF{r}, \BF{v}) = mn(t,\BF{r}) \BF{u}(t,\BF{r}
\end{equation}
where $\BF{u}(t,\BF{r})$ is the mean velocity of the gas. Indeed $F$ is the number of particles per unit volume with speed in the neighborhood of $\BF{v}$, so multiplying it by the momentum carried by such a particle, $m\BF{v}$, and integrating gives the average momentum density. 


In a similar fashion, the \textbf{second moment} is defined as the mean energy density:
\begin{align}
    \int d^3\BF{v}  \frac{mv^2}{2} F(t,\BF{r}, \BF{v}) &= \int d^3 \BF{w} \frac{m|\BF{u}+\BF{w}|^2}{2} F(t,\BF{r}, \BF{v})\\
    &= \frac{mu^2}{2} \int d^3 \BF{w} F + \cancelto{0}{m\BF{u} \cdot \int d^3 \BF{w} \BF{w} F} + \int d^3\BF{w} \frac{mw^2}{2} F \\
    &= \frac{mnu^2}{2} + \bigg \langle{\frac{mw^2}{2}\bigg \rangle}n
\end{align}
where $\BF{u}$ is defined as in the first moment, and $\BF{w} = \BF{v} - \BF{u}$ is the peculiar velocity of each particle in the gas frame. Indeed:
\begin{equation}
    \int d^3 \BF{w} \BF{w} F = \int d^3 \BF{v} (\BF{v} - \BF{u}) F = \BF{u}n(t,\BF{r}) -n(t, \BF{r}) \BF{u} =0
\end{equation}

Now the term $\frac{mnu^2}{2}$ may be interpreted as the kinetic energy density due to the gas' mean motion $\BF{u}$. Instead, the second term $\bigg \langle{\frac{mw^2}{2}\bigg \rangle}n$ is the internal energy density due to the gas' internal degrees of freedom. We may then define:
\begin{equation}
    K = \int d^3 \BF{r} \frac{mnu^2}{2}
\end{equation}
as the total kinetic energy of the gas, and:
\begin{equation}
    U = \int d^3 \BF{r} \bigg \langle{\frac{mw^2}{2}\bigg \rangle}n
\end{equation}
as the total internal energy. 

All this is nice and dandy, but we are assuming that we are given an expression for $F(t,\BF{r}, \BF{v})$. 

\section{Maxwellian equilibrium}
In most classical scenarios the dependence of $F$ on position and time is associated with macroscopic perturbations, such as an oven or an open window. Therefore, the scales of these inhomogeneities are much larger than $\tau_c$ and $\tau_{mfp}$. 


If we then break up the gas into small elements of size $\delta l$ in time intervals $\delta t$ such that:
\begin{equation}
    l \gg \delta l \gg \lambda_{mfp}, \ t \gg \delta t \gg \tau_c
\end{equation}

Each one of these small gas elements thus behaves as a homogeneous gas, and may be modelled with the local Maxwellian distribution:
\begin{equation}
    F_M(t, \BF{r}, \BF{v}) = n(t,\BF{r}) \bigg(\frac{m}{2\pi k_B T(t,\BF{r})}\bigg) \exp \bigg[-\frac{m|\BF{w}(t,\BF{r})|^2}{2k_B T(t,\BF{r})}\bigg]
\end{equation}
where $\BF{w}(t,\BF{r})-\BF{v}-\BF{u}(t,\BF{r})$ is the constituent particle's particular velocity in the gas element's center of mass frame. 

As before, if we define $v_{th} = \sqrt{\frac{2k_BT}{m}}$, then the maxwellian distribution reduces to:
\begin{equation}
    F_M(t, \BF{r}, \BF{v})  = \frac{n(t,\BF{r})}{(\sqrt{\pi} v_{th}(t,\BF{r}))^3} e^{-w^2/v_{th}^2}
\end{equation}

Local pressure is then
\begin{equation}
    P(t,\BF{r}) = n(t,\BF{r}) k_B T(t,\BF{r}) = \frac{2}{3}\varepsilon(t, \BF{r}
\end{equation}
so that:
\begin{equation}
    k_B T(t,\BF{r}) = \frac{2}{3}\frac{\epsilon(t,\BF{r})}{n(t,\BF{r})} = \frac{2}{3} \bigg \langle \frac{mw^2}{2} \bigg \rangle
\end{equation}
\section{Conservation laws}
Gases can transport momentum, energy of particles through their motion. The properties of momentum, energy and particle transport are called transport properties of gases, and are closely related to conservation laws.

One quantity that must always be conserved is the total number of particles:
\begin{equation}
    \int d^3 \BF{r} n = N
\end{equation}
We must also have conservation of total momentum, which we can set to $0$ by moving to an appropriate reference frame:
\begin{equation}
    \int d^3\BF{r} mn \BF{u} = 0
\end{equation}
Finally, the total energy must also be conserved:
\begin{equation}
    \int d^3 \BF{r} \BF{r} \frac{mnu^2}{2} + \int d^3 \BF{r} \bigg \langle{\frac{mw^2}{2}\bigg \rangle}n = K+U
\end{equation}

Now how do we evolve the variables $n, \BF{u}, T$ in such a way as to satisfy these conservation laws? 


Let us initially consider a gas with $\BF{u} = 0$ and constant density. All the energy is clearly in the internal energy:
\begin{equation}
    \epsilon = nc_1 T(t,\BF{r})
\end{equation}
where $c_1$ is the heat capacity per particle. Furthermore we also assume that the system is one dimensional with $z$-dependence. 


If we have a hot region and a cold region, then there will be a flux of heat $J(z)$, defined as the internal energy flowing per unit time through unit area perpendicular to $z$-axis. 


So for a small volume $A \times \bigg[z-\frac{dz}{2}, z+\frac{dz}{2}\bigg]$ then:
\begin{equation}
    \frac{\partial}{\partial t}(nc_1T \cdot Adz) = AJ_z\bigg(z-\frac{dz}{2}\bigg) - AJ_z\bigg(z+\frac{dz}{2}\bigg)
\end{equation}
which yields:
\begin{equation}
    nc_1 \frac{\partial T}{\partial t} = -\frac{J_z(z+dz/2)-J_z(z-dz/2)}{dz} = -\frac{\partial J_z}{\partial z}
\end{equation}
In three dimensions this turns into:
\begin{equation}
    \frac{\partial}{\partial t} \int_V d^3\BF{r} nc_1T = - \int_{\partial V} d\BF{A} \cdot \BF{J} = - \int_V d^3 \BF{r} \nabla \cdot \BF{J}
\end{equation}
for all volumes $V$, so that:
\begin{equation}
    \boxed{nc_1\frac{\partial T}{\partial t}+\nabla \cdot \BF{J} = 0}
\end{equation}
The total flux of heat out of a system is equal to the rate of change of its internal energy, and thus is proportional to the rate of change of its temperature. This is just a restatement of conservation of energy, heat leaving the system must be equal to the energy being lost.


Similarly, the rate of change of a volume $V$ of a gas is:
\begin{equation}
    \frac{\partial}{\partial t}\int_V d^3 \BF{r} mn\BF{u}
\end{equation}

We now define the momentum flux as $\Pi$ as a tensor with components $\Pi_{ij}$ as the momentum component $j$ leaving through a surface of unit area perpendicular to $i$ per unit time. 


Analogously to energy conservation, momentum conservation then requires:
\begin{equation}
     \frac{\partial}{\partial t}\int_V d^3 \BF{r} mn\BF{u} = - \int_{\partial V} d\BF{A} \cdot \BF{\Pi} 
\end{equation}

We may not apply Gauss' law directly since we have a tensor, instead we may take the $j$-th component of momentum:
\begin{equation}
     \frac{\partial}{\partial t}\int_V d^3 \BF{r} mnu_j = - \int_{\partial V} d\BF{A} \cdot \BF{\Pi}_j = -\int_V d^3 \BF{r} \nabla \cdot \BF{\Pi}_j 
\end{equation}
Consequently:
\begin{equation}
    mn  \frac{\partial u_j}{\partial t}  + \nabla \cdot \BF{\Pi}_j = 0
\end{equation}

But how do we find the momentum flux? Momentum fluxes are usually due to velocity gradients, and in gases one significant contribution is due to viscosity. 


Viscosity is a fluid's resistance to deformation through shear stress. That is, if we place a fluid between two straight, parallel plates, and move the top plate, then viscosity will cause layers of gas closer to the top plate to move with greater speed compared to layers farther from the top plate. 

Suppose the plates lie in the $xy$ plane and the top one is moved in the $x$ direction, causing shear stress $\tau_{xz}=\frac{F}{A}$. Here $A$ is the area of the plate and $F$ the force applied. Suppose the plate moves at speed $u$, then this will produce a velocity gradient $\frac{d\braket{u_x}}{dz}$ in the gas, because each layer in the gas will try to drag the neighbouring layers with it. Viscosity $\eta$ is then defined as:
\begin{equation}
    \tau_{xz} = \frac{F}{A} = \eta \frac{d \braket{u_x}}{dz}
\end{equation}

Momentum conservation yields:
\begin{equation}
    mn \frac{\partial u_x}{\partial t} + \eta \frac{\partial^2 \braket{u_x}}{\partial z^2}
\end{equation}

\part{Statistical Physics}
\chapter{The microcanonical ensemble}
\section{What is statistical physics?}
The aim of statistical physics is to derive the laws of physics at the macro-scale, using the governing laws of molecular dynamics. It provides a link between the behaviour of the micro-scale and the resulting effects in bulk properties at the macro-scale we are familiar with. 

For example, what makes the specific heat of materials different from each other? 


To do so we will need to find an intermediary connection between classical mechanics and thermodynamics, entropy and the phase space. 
\section{The classical Phase Space}
Recall that in classical mechanics the coordinates $(\textbf{q}(t), \textbf{p}(t))$ are enough to fully specify the state of a system of $N$ particles. These coordinates belong to an alternate space, called the \textbf{phase space}. For a three-dimensional \enquote{real space} with $N$ particles, it follows that the phase space will be $6N$-dimensional. 


Any point in the phase space is associated to a micro-state of the whole system's behaviour, that is, an instant in the motion of the system. The way this micro-state behaves through time is given by Hamilton's equations:
\begin{subequations}
\begin{empheq}[box=\widefbox]{align}
  \dot{q}_i = \frac{\partial H}{\partial p_i}, \ \dot{p}_i = -\frac{\partial H}{\partial q_i}
\end{empheq}
\end{subequations}

The solutions to the above equations are curves $(\textbf{q}(t), \textbf{p}(t))$ in the phase space. These curves are known as phase-space trajectories. 

\begin{figure}[h!]
    \centering
    \includegraphics[width=7cm]{microstates/phase space.png}
    \caption{The phase space}
    \label{fig:my_label}
\end{figure}


For an observable $A(\textbf{q}(t), \textbf{p}(t),t)$ its time-evolution is given by:
\begin{align} \label{poisson}
    \frac{dA}{dt} = \frac{\partial A}{\partial t} + \sum_{i=1}^{3N} \bigg(\frac{\partial A}{\partial q_i} \dot{q}_i + \frac{\partial A}{\partial p_i} \dot{p}_i\bigg)
\end{align}
It is now convenient to introduce the Poisson bracket $\{A,H\}=\sum \limits_{i=1}^{3N} \big(\frac{\partial A}{\partial q_i} \dot{q}_i + \frac{\partial A}{\partial p_i} \dot{p}_i\big)$ to simplify the sum \eqref{poisson} to write:
\begin{equation}
    \frac{dA}{dt} = \frac{\partial A}{\partial t} + \{A,H\}
\end{equation}

Just like normal three dimensional space may be divided into infinitesimal volume elements, one may also partition the phase space into infinitesimal phase space volume elements $d \omega$ defined by:
\begin{equation}
    d\omega = d^{3N} \textbf{q} d^{3N}\textbf{p}
\end{equation} 

Now if we consider two energy surfaces in the phase-space, with energies $E$ and $E+\Delta e$, then the corresponding phase volume will be given by:
\begin{equation}
    \Delta \omega = \omega(E+\Delta E) - \omega(E) = \frac{\partial \omega}{\partial E}\bigg|_{V,N} \Delta E
\end{equation}
However, we also have that if the area of the energy surface $H(\textbf{q},\textbf{p})=E$ is defined as $\sigma(E)$, the density of states at energy $E$, then:
\begin{equation}
    \sigma(E) = \int_{H=E} d\sigma
\end{equation}
so that:
\begin{equation}
     d\omega = \sigma(E) dE \implies \boxed{\sigma(E) = \frac{\partial \omega}{\partial E}\bigg|_{V,N}}
\end{equation}


We could also see this using the fundamental theorem of calculus:
\begin{equation}
    \omega(E,V,N) = \int_{E'<E} \sigma(E',V,N) dE' \iff \sigma(E,V,N) = \frac{\partial \omega}{\partial E}\bigg|_{V,N}
\end{equation}
from which it follows that:
\begin{equation}
    \int_{E<H<E+\Delta E} d\omega = \sigma(E) \Delta E
\end{equation}
Now for a closed, isolated system we have that the variables $E$ (energy), $N$ (number of particles) and $V$ (volume) are enough to specify its macroscopic properties. They specify a \textbf{macrostate}.
\begin{figure}[h!]
    \centering
    \includegraphics[width=5cm]{microstates/phase volume.png}
    \caption{Phase space volume between energy surfaces $H(q_i,p_j)=E$ and $H(q_i,p_j)=E+\Delta E$.}
    \label{fig:my_label}
\end{figure}

However, this macrostate is highly degenerate. There are several ways one could arrange a system to give the corresponding values of $E,V,N$, which we denote by $\Omega(E,V,N)$, Each one of these states is called a \textbf{microstate}. We expect that $\Omega(E,V,N)$ will be proportional to $\sigma(E,V,N)$
\begin{equation}\label{cavalieri}
   \boxed{\Omega(E,V,N) = \frac{\sigma(E,V,N)}{\sigma_0} = \frac{1}{\sigma_0} \frac{\partial \omega}{\partial E}\bigg|_{V,N}}
\end{equation}


It turns out these microstates all have equal probabilities. Indeed, consider a isolated system which we subdivide into two subsystems with macroscopic variables $E_i, V_i, N_i$. Because the original system is closed, we require that:
\begin{align}
    &dE = 0 \implies dE_1 = - dE_2\\
    &dV = 0 \implies dV_1 = - dV_2\\
    &dN = 0 \implies dN_1 = - dN_2
\end{align}

\begin{figure}[h!]
    \centering
    \includegraphics[width=6cm]{microstates/dist gas.png}
    \caption{Composite system state counting}
    \label{fig:my_label}
\end{figure}
We must also have from elementary combinatorics that:
\begin{align}
    &\Omega(E,V,N) = \Omega_1(E_1,V_1,N_1) \Omega_1(E_2, V_2, N_2)\\
    \implies &d\Omega = \Omega_1 d\Omega_2 + \Omega_2 d\Omega_1\\
     \implies &d \ln \Omega =  d\ln \Omega_2 +  d\ln \Omega_1 \label{microstates}
\end{align}
Now equilibrium occurs whenever we have a maximum number of possible microstates, so when $d\Omega=0$. Therefore $d \ln \Omega = 0$. In classical thermodynamics, one may recall that equilibrium results in maximal entropy so $dS = dS_1 + dS_2 = 0$. Comparison with \eqref{microstates} suggests the relation:
\begin{equation}
    \boxed{S(E,V,N) = k_B \ln \Omega(E,V,N)}
\end{equation}
where $k_B$ is Boltzmann's constant. This is a remarkable important law because it allows us to find all other thermodynamic variables which we define as:
\begin{equation}
    \frac{1}{T} = \frac{\partial S}{\partial E}\bigg|_{V,N}, \ \frac{p}{T} = \frac{\partial S}{\partial V}\bigg|{E,N}
\end{equation}

\section{Probability density and expectation values}
For isolated systems we have assumed that all microstates on the same energy surface have equal probability of being occupied. This is no longer true for open systems, where we must introduce a probability density $\rho(\textbf{q}, \textbf{p})$ which corresponds to the probability density of the system to occupy the phase state $(\textbf{q},\textbf{p})$. Normalization is required as always:
\begin{equation}
    \int \rho(\textbf{q},\textbf{p})  \ d^{3N} \textbf{q}\  d^{3N} \textbf{p} = 1
\end{equation}
Suppose we have some observable $A$ of the system, then its expectation value is given by:
\begin{equation}
    \braket{f} =  \int \rho(\textbf{q},\textbf{p}) f(\textbf{q},\textbf{p})\ d^{3N} \textbf{q}\  d^{3N} \textbf{p} 
\end{equation}
For an isolated system for example, we have that:
\begin{equation}
    \rho(\textbf{q},\textbf{p})=\frac{1}{\sigma(E)}\delta(E-H(\textbf{q},\textbf{p}))
\end{equation}
This however is a very complicated expression due to the Dirac delta function. We can simplify it by allowing for a small energy deviation $\Delta E$, for which:
\begin{equation}
    \rho(\textbf{q},\textbf{p})=\begin{cases}
    \rho_0, \ E\leq H(\textbf{q},\textbf{p}) \leq E+\Delta E \\
    0, \ \text{ otherwise}
    \end{cases}
\end{equation}
Then normalization requires:
\begin{align}
    &\int \limits_{E\leq H(\textbf{q},\textbf{p}) \leq E+\Delta} \rho_0 \ d^{3N} \textbf{q}\ d^{3N} \textbf{p}  = 1\\
    \iff & \rho_0 \sigma(E,V,N) \Delta E = 1 \\
    \iff &\rho_0 \Omega(E,V,N) h^{3N} \Delta E  = 1 \\
    \iff &\rho = \frac{1}{\Omega(E,V,N) h^{3N} \Delta E}
\end{align}
since the energy shell volume in the phase space is $\sigma(E,V,N) \Delta E = h^{3N} \Omega(E,V,N) \Delta E$
We may take $\frac{1}{h^{3N}}$ out of the integral by by defining:
\begin{equation}
    \frac{1}{h^{3N}} \int \rho(\textbf{q},\textbf{p}) \ d^{3N} \textbf{q} \ d^{3N} \textbf{p}  = 1
\end{equation}

An ensemble is then a very very large collection of identical copies of a system with distribution function $\rho(\textbf{q},\textbf{p})$. There are special types of ensembles which we will study:
\begin{enumerate}
    \item[(i)] micro-canonical ensemble
    \item[(ii)] canonical ensemble
    \item[(iii)] grand canonical ensemble
\end{enumerate}

In the case of the micro-canonical ensemble it is defined by:
\begin{equation}\label{microprob}
    \rho_{mc}(\textbf{q},\textbf{p})=\begin{cases}
    \frac{1}{\Omega \Delta E}, \ E\leq H(\textbf{q},\textbf{p}) \leq E+\Delta E \\
    0, \ \text{ otherwise}
    \end{cases}
\end{equation}
\section{Louiville's theorem}
Hamilton's equations demand that:
\begin{equation}
    \frac{d\rho}{dt} = \frac{\partial \rho}{\partial t} + \{\rho, H\}
\end{equation}
Now consider the set of systems in the volume element $\omega$ at time $t$. As they evolve through time each of the phase space points will get mapped to another volume element $\omega'$ at time $t'$. However the initial and final number of phase space points must remain constant. We thus require that if we have a change in phase space volumes then a net amount of phase space points flowing out of $\omega$:
\begin{equation}
    \frac{\partial}{\partial t} \int d\omega \rho  + \int \rho (\textbf{v} \cdot \textbf{n}) \cdot d\sigma 
\end{equation}
Here $\textbf{v}$ is the\enquote{velocity} in the phase space of the trajectories, given by $(\dot{\textbf{q}}, \dot{\textbf{p}})$. Using the divergence theorem we find:
\begin{equation}\label{cont}
    \int_{\omega} \bigg(\frac{\partial \rho}{\partial t} + \nabla (\rho \textbf{v})\bigg) d\omega = 0
\end{equation}
where divergence is taken with respect to the phase space coordinates $\{q_1,q_2,...,q_3N, p_1, p_2,...,p_{3N}\}$. Since \eqref{cont} holds for any arbitrary volume $\omega$ we find the continuity equation
\begin{equation}
    \frac{\partial \rho}{\partial t} + \nabla (\rho \textbf{v})=0
\end{equation}
Let us expand the divergence:
\begin{align}
    \nabla (\rho \textbf{v}) &= \sum_{i=1}^{3N} \bigg[\frac{\partial}{\partial q_i} (\rho \dot{q}_i) + \frac{\partial}{\partial p_i} (\rho \dot{p}_i) \bigg]\\
    &=\sum_{i=1}^{3N} \bigg[\frac{\partial \rho}{\partial q_i} \dot{q}_i + \rho \frac{\partial \dot{q}_i}{\partial q_i} +  \frac{\partial \rho}{\partial p_i} \dot{p}_i + \rho \frac{\partial \dot{p}_i}{\partial p_i} \bigg]  \\
    &= \sum_{i=1}^{3N} \bigg[\frac{\partial \rho}{\partial q_i}\frac{\partial H}{\partial p_i} - \frac{\partial \rho}{\partial p_i} \frac{\partial H}{\partial q_i}+ \rho \cancelto{0}{\bigg( \frac{\partial^2H}{\partial q_i \partial p_i}+\frac{\partial^2 H}{\partial p_i \partial q_i}\bigg)} \bigg] \\
    &=\{\rho, H\}
\end{align}
Consequently:
\begin{equation}
    \boxed{\frac{d\rho}{dt} = \frac{\partial \rho}{\partial t} + \{\rho, H\} =\frac{\partial \rho}{\partial t} + \nabla (\rho \textbf{v}) = 0}
\end{equation}
so the time derivative of the phase-space density is null for a trajectory. Alternatively, a phase space density is constant along the trajectory of a system. This is known as \textbf{Louiville's theorem}. 
\section{Ergodic hypothesis and Equal a priori probability}
One may wonder why we should even worry about probability distributions in the phase space and taking ensemble averages. After all, when we perform experiments we are really taking a time average of one system, rather than the ensemble average of several systems. If we have some quantity $A(\textbf{q},\textbf{p},t)$ then its time average will be:
\begin{equation}
    \tilde A = \lim_{T \rightarrow \infty} \frac{1}{T} \int_0^T A(\textbf{q},\textbf{p},t) \ dt
\end{equation}
How can we justify using the ensemble average instead:
\begin{equation}
    \braket{A} = \int \limits_{\text{ensemble}} A(\textbf{q},\textbf{p},t) \rho(\textbf{q},\textbf{p}) \ d\omega
\end{equation}
The ergodic hypothesis postulates that these two averages should be equivalent in the thermodynamic limit. This hinges on the condition that the system visits the neighbourhood of all the available phase space points (gets arbitrarily close to them) after a sufficiently long time. If we average over time then under the ergodic hypothesis we are really just averaging over the ensemble. 
  
  
Also, another fundamental assumption of statistical mechanics is the postulate of equal a priori probability. It states that for a system in equilibrium the micro-states corresponding to a macro-state have the same probability of being occupied. 


The link between these two postulates is Louiville's theorem, which states that the a system takes trajectories along constant phase space density surfaces. Since systems in equilibrium have a constant phase space density for each macro-state, it follows that a system's trajectory will be in the neighborhood of all possible microstates. 
\section{The microcanonical ensemble}
The microcanonical ensemble is a statistical ensemble used to model isolated systems, where there is no exchange of particles and energy. Therefore we will take $E,V,N$ to be conserved quantities specifying an equilibrium macrostate. For such systems we previously stated without proof that for a given energy they are modelled by a constant phase-space density on the corresponding energy hypersurface, as described in \eqref{microprob}

Let us prove this result. 

Consider an ensemble made up of $\mathcal{N}$ identical copies of an isolated system, each in a different microstate on the energy hypersurface $H(\textbf{q},\textbf{p}) = E$.

Suppose we partition this surface into small elements $\Delta \sigma_i$ with $n_i$ systems each:
\begin{equation}\label{total states}
    \mathcal{N} = \sum_i n_i
\end{equation}
How many ways are there to distribute the $\mathcal{N}$ systems into the energy surfaces? For $\mathcal{N}$ systems there are $\mathcal{N}!$ different ways to enumerate them. However, since within each surface $\Delta \sigma_i$ there are $n_i!$ internal exchanges which do not alter the overall distribution, the total number of ways to distribute the systems will be:
\begin{equation}
    W\{n_i\} = \frac{\mathcal{N}!}{\prod_i n_i!}
\end{equation}
In other words, order matters when distributing the $n_i$ systems into the surfaces $\Delta \sigma_i$, but the ordering of each system within $\Delta \sigma_i$ is insignificant. 


The most probable distribution will of course maximize $W\{n_i\}$, and hence $\ln W\{n_i\}$ (so entropy will be maximized, corresponding to equilibrium). Therefore we need:
\begin{align}
    \ln W_{tot} &= \ln \mathcal{N}! -\sum_i  \ln n_i!\\
    &\approx \mathcal{N} \ln \mathcal{N} - \mathcal{N} - \sum_i (n_i\ln n_i-n_i) \label{gibbs deriv}\\
    \iff &d\ln W_{tot} = - \sum_{i} \ln n_i  \ dn_i = 0 \label{lagrange 1}
\end{align}
Since we are subject to the constraint \eqref{total states} we need to use the method of Lagrange multipliers. We get that:
\begin{equation}\label{lagrange2}
    \lambda d\mathcal{N} = \lambda \sum_i dn_i=0
\end{equation}
which we can sum to \eqref{lagrange 1} to get:
\begin{equation}
    \sum_{i} (\ln n_i-\lambda) dn_i = 0
\end{equation}
Equating the coefficients to zero one finds that:
\begin{equation}
   \boxed{ n_i = \text{cnst}.}
\end{equation}
We easily identify this distribution as a homogeneous distribution:
\begin{equation}
    \boxed{\rho_{mc}(\textbf{q},\textbf{p})=\begin{cases}
    \frac{1}{\Omega \Delta E} \ E\leq H(\textbf{q},\textbf{p}) \leq E+\Delta E \\
    0 \ \text{ otherwise}
    \end{cases}}
\end{equation}
as desired. Hence the most likely distribution in a microcanonical ensemble is the $\rho_{mc}$ distribution. 

\section{Relating the Boltzmann and Gibbs entropy}
Recall that the Boltzmann entropy was identified to be:
\begin{equation}
    S(E,V,N) = k_B \ln \Omega(E,V,N)
\end{equation}
In the microcanonical picture however (developed by Gibbs), it is seen as the ensemble average:
\begin{equation}\label{entropy average}
    S(E,V,N) = \frac{1}{h^{3N}} \int \limits_{E\leq H \leq E+\Delta E}  \rho(\textbf{q},\textbf{p})(-k_B \ln \rho(\textbf{q},\textbf{p}))\ d\omega
\end{equation}
which we recognize to be:
\begin{equation}
    S(E,V,N) = \braket{-k_B \ln \rho(\textbf{q},\textbf{p})}
\end{equation}
Indeed, inserting \eqref{microprob} into \eqref{entropy average} one finds that:
\begin{align}
    S(E,V,N) &= \frac{1}{h^{3N}} \int \limits_{E\leq H \leq E+\Delta E} \frac{1}{\Omega \Delta E} k_B \ln (\Omega \Delta E)\  d\omega
\end{align}
Since $\Omega$ is constant over the energy shell $E\leq H \leq E+\Delta E$ the integrand may be pulled out:
\begin{equation}
    S(E,V,N) = \frac{1}{\Omega \Delta E} k_B \ln (\Omega \Delta E) \frac{1}{h^{3N}} \int \limits_{E\leq H \leq E+\Delta E}\ d\omega 
\end{equation}

We now recognize:
\begin{equation}
    \Omega  \Delta E= \frac{1}{h^{3N}} \int \limits_{E\leq H \leq E+\Delta E} d\omega 
\end{equation}
so that in the microcanonical ensemble:
\begin{equation}
   \boxed{ S(E,V,N) = k_B \ln (\Omega \Delta E)= \braket{-k \ln \rho}}
\end{equation}
as desired. Alternatively, in the discrete case, we can insert $p_i=\frac{n_i}{\mathcal{N}}$ which is justified by the equal a priori probability postulate into \eqref{gibbs deriv} and find that
\begin{equation}
    S = -k_B\bigg(\mathcal{N} \ln \mathcal{N} - \mathcal{N}\sum_{i} p_i\ln (\mathcal{N}p_i)\bigg)= \sum_i p_i \ln p_i 
\end{equation}
as desired. We therefore infer that the Gibbs entropy is a generalization of the Boltzmann entropy, which is only applicable for equilibrium systems in a single macrostate, where the equal a priori probability postulate is justified.
\section{Entropy of an ideal gas (classical)}
For an ideal gas which is interaction-free, the Hamiltonian takes the form:
\begin{equation}
    {H}(\textbf{q},\textbf{p}) = \sum_{i=1}^{3N} \frac{p_i^2}{2m}
\end{equation}
The total phase space volume enclosed by the energy surface $E=H(\textbf{q}, \textbf{p})$ is
\begin{equation}\label{volume}
    \omega(E,V,N) = \int_{H\leq E} d^{3N} \textbf{q} d^{3N} \textbf{p} =V^N \int_{H\leq E} d^{3N} \textbf{p} 
\end{equation}
since there is no $q$ dependence of the Hamiltonian surface we are integrating over. In the phase-space, we also see that $E$ delineates a spherical surface of radius $p=\sqrt{2mE}$. 

\begin{figure}[h!]
    \centering
    \includegraphics[width=8cm]{microstates/sphere phase.png}
    \label{fig:my_label}
\end{figure}


So the integral in \eqref{volume} is simply the volume of an $3N$-dimensional sphere:
\begin{equation}
    V_{\text{sphere}}^N = \frac{2\pi^{N/2}}{N \Gamma\big(\frac{N}{2}\big)}R^N
\end{equation}
where $R$ is the radius. We then find that:
\begin{equation}
    \omega(E,V,N) =V^N \frac{2\pi^{3N/2}}{3N \Gamma\big(\frac{3N}{2}\big)}(2mE)^{3N/2}
\end{equation}

Using \eqref{cavalieri} one may then find the number of microstates on the energy surface:
\begin{align}
    \Omega(E,V,N) &= \frac{1}{\sigma_0} \frac{\partial}{\partial E}\bigg(\omega(E,V,N)\bigg) \\
    &=\frac{1}{\sigma_0}V^N \frac{\pi^{3N/2}}{ \Gamma\big(\frac{3N}{2}\big)}(2m)^{3N/2}E^{3N/2-1}
\end{align}
Consequently the volume occupied by the micro-canonical ensemble is:
\begin{equation}
    \Omega(E,V,N) \Delta E = \frac{1}{\sigma_0}V^N \frac{\pi^{3N/2}}{ \Gamma\big(\frac{3N}{2}\big)}(2mE)^{3N/2}\frac{\Delta E}{E}
\end{equation}
One then gets an expression, albeit very complicated, for an ideal gas:
\begin{equation}
    S(E,V,N) = k_B \ln \bigg[\frac{1}{\sigma_0}V^N \frac{\pi^{3N/2}}{ \Gamma\big(\frac{3N}{2}\big)}(2mE)^{3N/2}\frac{\Delta E}{E}\bigg]
\end{equation}
For very large systems where $N\gg 1$ then we may make the approximations $\ln \frac{\Delta E}{E} \approx 0$ (thus ignoring any terms not of order $N$). We also make use of Stirling's approximation:
\begin{equation}
    \ln \Gamma(n) \approx n \ln n - n
\end{equation}
to find that:
\begin{equation}
      S(E,V,N) \approx Nk_B \bigg[\ln \bigg(\frac{V}{\sigma}(2\pi mV)^{3/2}\bigg)+\frac{3}{2}-\frac{3}{2}\ln \frac{3N}{2}\bigg]
\end{equation}
where $\sigma = \sigma_0^{1/N}$. Further simplification gives:
\begin{equation}\label{entropy inc}
    S(E,V,N) \approx Nk_B \bigg[\frac{3}{2}+\ln\frac{V}{\sigma} \bigg(\frac{4 \pi m E}{3N}\bigg)^{3/2}\bigg]
\end{equation}

This gives the correct equations of state from classical thermodynamics:
\begin{align}
    &\frac{1}{T} = \frac{\partial S}{\partial E} \bigg|_{V,N} = \frac{3 Nk_B}{2E} \implies E = \frac{3}{2} Nk_BT\\
    &\frac{p}{T} = \frac{\partial S}{\partial V}\bigg|_{E,N} = \frac{Nk_B}{V} \implies pV = Nk_BT
\end{align}

which recast \eqref{entropy inc} into:
\begin{equation}\label{entropy ideal}
    S(E,V,N) \approx Nk_B \bigg[\frac{3}{2}+\ln\bigg( \frac{V}{\sigma}(2 \pi m k_B T)^{3/2}\bigg)\bigg]
\end{equation}
However, it turns out that \eqref{entropy inc} is incorrect. Indeed, we know that entropy is an extensive quantity, yet the additional logarithmic term produces an extra $\ln \alpha$ term when the system is scaled up by $\alpha$. 

\section{Gibb's paradox}

Let's analyze where this inconsistency comes from. We consider a container containing gas $A$, and another container containing gas $B$. Now the total entropy of the two-container system is:
\begin{equation}
    S^b = S_A^b(T,V_A,N_A) + S_B^b(T,V_B,N_B)
\end{equation}
\begin{figure}[h!]
    \centering
    \includegraphics[width=6cm]{microstates/distinct gases.png}
    \caption{Caption}
    \label{fig:my_label}
\end{figure}
Now suppose we put the containers in contact so that the gases mix together and occupy the total volume $V_A+V_B$. Once equilibrium has reached the entropy will be:
\begin{equation}
    S^a =  S_A^a(T,V_A+V_B,N_A) + S_B^a(T,V_A+V_B,N_B)
\end{equation}
The total entropy change is
\begin{equation}
    \Delta S = N_A k_B\ln \frac{V_A+V_B}{V_A} + N_B k_B\ln \frac{V_A+V_B}{V_B}  > 0
\end{equation}

Throughout this derivation however, we have been assuming that gases $A$ and $B$ are distinguishable. If we use identical gases, then
\begin{align}
    &S^b = S_A^b(T,V_A,N_A) + S_B^b(T,V_B,N_B)\\
    &S^a = S(T,V_A+V_B,N_A+N_B)
\end{align}
We have that:
\begin{align}
    S(T,V_A+V_B,N_A+N_B) &= (N_A+N_B)k_B \bigg[\frac{3}{2}+\ln \frac{V_A+V_B}{\sigma}(2 \pi m k_B T)^{3/2}\bigg] \\
    &= N_Ak_B \bigg[\frac{3}{2}+\ln \frac{V_A}{\sigma}(2 \pi m k_B T)^{3/2}\bigg] + N_Bk_B \bigg[\frac{3}{2}+\ln \frac{V_B}{\sigma}(3 \pi m k_B T)^{3/2}\bigg]\\
    &=  S^a =  S_A^a(T,V_A+V_B,N_A) + S_B^a(T,V_A+V_B,N_B)
\end{align}
so we again find that:
\begin{equation}
      \Delta S = N_A k_B\ln \frac{V_A+V_B}{V_A} + N_B k_B\ln \frac{V_A+V_B}{V_B}  > 0
\end{equation}

This is quite a problem however, because the mixing of two identical gases is completely reversible, there is no change in the gases when they are put in contact. Therefore, we should have found $\Delta S = 0$. 
\begin{figure}[h!]
    \centering
    \includegraphics[width=6cm]{microstates/identical gases.png} 
    \caption{Caption}
    \label{fig:my_label}
\end{figure}

Note that this discrepancy is directly related to the non-extensivity of our \eqref{entropy ideal}, since when we put the two dinstiguishable gas containers in contact we're essentially scaling the size of the system by two, whereas for identical gas containers we're doing absolutely nothing, yet our expressions for the entropy after mixing are the same for both situations. 


To solve this problem, we must employ tools from quantum mechanics. Indeed we know that in quantum mechanics one cannot label particles. Hence different microstates with the same particle distribution but different particle labelling in the classical picture are considered equivalent in quantum mechanics. Since there are $N!$ ways to enumerate $N$ particles it follows that:
\begin{equation}
    \Omega(E,V,N) = \frac{1}{N!} \frac{\sigma(E,V,N)}{\sigma_0}
\end{equation}
which provides the following correction to entropy:
\begin{equation}
    S(E,V,N) = Nk_B \bigg[\frac{3}{2}+\ln\frac{V}{\sigma} \bigg(\frac{4 \pi m E}{3N}\bigg)^{3/2}\bigg] - k_B \ln N!
\end{equation}
and using Stirling's formula:
\begin{equation}
    \boxed{S(E,V,N) = Nk_B \bigg[\frac{5}{2}+\ln\frac{V}{N \sigma} \bigg(\frac{4 \pi m E}{3N}\bigg)^{3/2}\bigg]}
\end{equation}
or alternatively:
\begin{equation}
    \boxed{S(E,V,N) = Nk_B \bigg[\frac{5}{2}+\ln\frac{V}{N \sigma} (2 \pi m k_B T)^{3/2}\bigg]}
\end{equation}
We see by inspection that this is indeed an extensive quantity, since the logarithm's argument is intensive overall. 

Let's check that this correction does indeed solve Gibbs' paradox. We now get that
\begin{align}
    \Delta S =& (N_A+N_B)k_B \bigg[\frac{5}{2}+\ln\frac{V_A+V_B}{N_A+N_B \sigma} (2 \pi m k_B T)^{3/2}\bigg] \\
    &- N_A k_B \bigg[\frac{5}{2}+\ln\frac{V_A}{N_A \sigma} (2 \pi m k_B T)^{3/2}\bigg]
    - N_Bk_B \bigg[\frac{5}{2}+\ln\frac{V_B}{N_B \sigma} (2 \pi m k_B T)^{3/2}\bigg]
\end{align}
Now we may impose:
\begin{equation}
    \frac{V_A}{N_A} = \frac{V_B}{N_B} =\frac{V_A+V_B}{N_A+N_B}
\end{equation}
since the density of gases will remain the same. Hence
\begin{equation}
    \Delta S = 0
\end{equation}
as desired. 
It is important to remember that:
\begin{subequations}
\begin{empheq}[box=\widefbox]{align}
 \text{The Gibbs' correction factor must not be included  for distinguishable microstates.}\nonumber
\end{empheq}
\end{subequations}
\section{Entropy of an ideal gas (quantum)}
There is one constant in this entire derivation that may be frustrating the reader, and that is the proportionality constant $\sigma_0$. We can determine its value by counting the number of states from a quantum mechanical point of view, and compare it with our semi-classical results. 


Let us try to derive an expression for entropy for an ideal gas made up of distinguishable particles. We will model the container as an infinite quantum well of size $L$, within which the particles are non-interacting. 

The individual particles are in the eigenstates:
\begin{equation}
    \psi_{n_x,n_y,n_z} = A \sin \bigg(\frac{n_x\pi x}{L}\bigg)\sin \bigg(\frac{n_y\pi y}{L}\bigg)\sin \bigg(\frac{n_z\pi z}{L}\bigg)
\end{equation}
with energy:
\begin{equation}
    \epsilon_{n_x,n_y,n_z} = \frac{h^2}{8mL^2}(n_x^2+n_y^2+n_z^2)
\end{equation}
Therefore, our quantum-mechanical phase space will consist of $(n_x,n_y,n_z)$. 
\begin{figure}[h!]
    \centering
    \includegraphics[width=9cm]{microstates/quant.png}
    \caption{Quantum phase space $(n_x,n_y,n_z)$ for an ideal gas}
    \label{fig:my_label}
\end{figure}
Now the total energy of an $N$-particle system will be given by:
\begin{equation}
    E=\frac{h^2}{8mL^2}\sum_{i=1}^{3N} n_i^2
\end{equation}
We can use this to find $\Omega(E,V,N)$, where $V=L^3$, by determining the degeneracy of the energy eigenstate with energy $E$? Clearly, we must determine how many ways one can add up a number $n$ in terms of a sum of $3N$ quadratic integer terms $\sum \limits_{i=1}^{3N} n_i^2$. 

Unfortunately, this is an impossibly difficult problem to solve. Indeed, if an exact expression does exist, it would be a very irregular function of energy. Luckily however, the average behaviour of $\Omega(E,V,N)$ can be calculated, and we expect it to be increasing with energy. The larger the energy surface, the more likely it is for integer-coordinate points to lie on it. 

\begin{figure}[h!]
    \centering
    \includegraphics[width=7cm]{microstates/sigma.png}
    \caption{Behaviour of $\Sigma$ and $\braket{\Sigma}$}
    \label{fig:my_label}
\end{figure}


Let us define the number of microstates within the energy sphere:
\begin{equation}
    \Sigma (E,V,N) = \sum_{E'\leq E} \Omega(E',V,N)
\end{equation}
This is again impossibly complicated to evaluate, but for large enough system, or systems with high enough temperature, then we can approximate it with its average $\braket{\Sigma}$. Then, we find that the density of states $g(E,V,N)$ at energy $E$ is approximately:
\begin{equation}
    g(E,V,N) = \frac{\partial}{\partial E}(\braket{\Sigma(E,V,N)})
\end{equation}
We remark the similarity between $g(E,V,N) \leftrightarrow \sigma(E,V,N)$ and $\braket{\Sigma(E,V,N)} \leftrightarrow \omega(E,V,N)$.


We also note that $\braket{\Sigma(E,V,N)}$ must be related to the volume enclosed by the energy surface of $E$. Indeed, suppose we divide this volume into individual unit cubes of unit volume. 
\begin{figure}[h!]
    \centering
    \includegraphics[width=8cm]{microstates/unit cube.png}
    \caption{Partition of phase space into unit cubes}
    \label{fig:my_label}
\end{figure}

Then energy grid point will have an associated unit cube, and the mean number of grid points will then be the total enclosed volume divided by the volume of a unit cube, which by definition is 1.

It is also important to note that since $n_i>0$ the enclosed volume will be an octant. Therefore, defining:
\begin{equation}
    R = \sqrt{\frac{8mEL^2}{h^2}}
\end{equation}
we get that $\Sigma(E,V,N)$ is the volume of a $3N$-hypersphere with this radius:
\begin{equation}
    \braket{\Sigma(E,V,N)} = V_N(R) = \bigg(\frac{1}{2}\bigg)^{3N}\frac{\pi^{3N/2}}{\frac{3N}{2}\Gamma\big(\frac{3N}{2}\big)} R^{3N} = \bigg(\frac{V}{h^3}\bigg)^N \frac{(2 \pi m E)^{3N/2}}{\frac{3N}{2} \Gamma\big(\frac{3N}{2}\big)}
\end{equation}
Therefore the mean density of states at energy $E$ is:
\begin{equation}
    g(E,V,N)= \bigg(\frac{V}{h^3}\bigg)^N \frac{(2 \pi m)^{3N/2}}{\frac{3N}{2} \Gamma\big(\frac{3N}{2}\big)}E^{\frac{3N}{2}-1}
\end{equation}
Finally, multiplying this by $\Delta E$ and approximating $\frac{3N}{2}N-1\approx \frac{3N}{2}$ gives
\begin{align}
    \braket{\Omega(E)} &= g(E) \Delta E =\bigg(\frac{V}{h^3}\bigg)^N \frac{(2 \pi m)^{3N/2}}{\frac{3N}{2} \Gamma\big(\frac{3N}{2}\big)}E^{\frac{3N}{2}} \Delta E \\
    &=\frac{\pi^{3N/2}}{\Gamma(3N/2)} \frac{1}{2^{3N}} R^{\frac{3N}{2}} \Delta R
\end{align}
Taking the logarithm, we will get an additional term $\ln \Delta R$ which we can set to zero when compared to the $\frac{3N}{2} \ln R$ term. Hence, we get that $\Delta R = 1$, as should be since the subdivision of the phase space was into unit blocks of volume $1$. Hence:
\begin{equation}
     \braket{\Omega(E)} = \frac{\pi^{3N/2}}{\Gamma(3N/2)} \frac{1}{2^{3N}} R^{\frac{3N}{2}} = \bigg(\frac{V}{h^3}\bigg)^N \frac{(2 \pi m E)^{3N/2}}{ \Gamma\big(\frac{3N}{2}\big)}
\end{equation}
Comparison with the classical derivation shows that 
\begin{equation}
    \sigma = h^3 \iff \sigma_0 = h^{3N}
\end{equation}
and hence
\begin{equation}
    \boxed{ S(E,V,N) = Nk_B \bigg[\frac{3}{2}+\ln \frac{V}{h^3}(2 \pi m k_B T)^{3/2}\bigg]}
\end{equation}
which is known as the \textbf{Sackur-Tetrode equation}.
\section{General procedure using the microcanonical ensemble}

This gives us a general procedure to derive classical thermodynamics using the micro-canonical ensemble. 
\begin{enumerate}
    \item[(i)] Find the Hamiltonian $H(\textbf{q},\textbf{p})$ of the system. 
    \item[(ii)] Find the phase space volume enclosed by the energy surface $E=H(\textbf{q},\textbf{p})$. 
    \item[(iii)] Evaluate the number of microstates on the energy surface $\tilde \Omega$:
    \begin{equation}
       \Omega(E,V,N) = \frac{1}{h^{3N}} \frac{\partial \omega}{\partial E}\bigg|_{V,N}
    \end{equation}
    \item[(iv)] Evaluate the number of microstates $\tilde \Omega$ in the ensemble $\Omega(E,V,N) \Delta E$, which is the number of states in the energy shell $E\leq H(\textbf{q},\textbf{p})\leq E+\Delta E$:
    \begin{equation}
        \tilde \Omega(E,V,N) = \Omega \delta E
    \end{equation}
 
    \item[(v)] Find the entropy:
    \begin{equation}
        S = k_B \ln \Omega \Delta E = k_B \ln \tilde \Omega
    \end{equation}
    and ignoring any terms not order $o(N)$. 
    \item[(vi)] Derive classical thermodynamic quantities and equations of state:
    \begin{align}
    &\frac{1}{T} = \frac{\partial S}{\partial E}\bigg|_{V,N}\\
    &\frac{p}{T} = \frac{\partial S}{\partial V}\bigg|_{E,N} \\
    &C_V = \frac{\partial E}{\partial T}\bigg|_V 
\end{align}
\end{enumerate}
\section{Ultra-relativistic gas}
Let us consider a gas made of $N$ massless relativistic particles moving at the speed of light $c$, and thus satisfying the energy-momentum relation:
\begin{equation}
    \epsilon=pc
\end{equation}
The hamiltonian of this system then reads:
\begin{equation}
    H(\textbf{q},\textbf{p}) = \sum_{i=1}^N \sqrt{p_{i,x}c^2 + p_{i,y}^2+p_{i,z}^2}c
\end{equation}
Now we find that since the particles are indistinguishable, we must insert the Gibb's correction factor:
\begin{equation}
    \Sigma(E,V,N)=\frac{1}{h^{3N}}\omega(E,V,N) = \frac{1}{h^{3N}}N! \int_{H\leq E} d\omega
\end{equation}
and since $H$ does not explicitly depend on $\textbf{q}$ we may integrate over spatial conjugate variables to find:
\begin{equation}
    \Sigma(E,V,N) = \frac{V^N}{h^{3N} N!} \int_{H \leq E} d^{3N} \textbf{p}
\end{equation}
We need to find the volume of the energy surface in the momentum space. The square root really complicates things, so we shall instead average the relevant quantities:
\begin{align}
    \braket{p^2} &= 3\braket{p_x^2} = 3\braket{p_y^2} = 3\braket{p_z^2}\\
    |\braket{p^2}| &= \sqrt{\frac{\braket{p_x^2}}{3}}+\sqrt{\frac{\braket{p_y^2}}{3}}+\sqrt{\frac{\braket{p_z^2}}{3}}\\
    &=\frac{\sqrt{3}}{3}(|p_x|+|p_y|+|p_z|)
\end{align}
Then the volume enclosed by the energy surface $H=E$ is the locus of points in the phase space satisfying:
\begin{equation}
    \sum_{i=1}^{3N} \frac{|p_i|c}{\sqrt{3}} \leq E
\end{equation}
Hence, substituting $x_i = \frac{p_ic}{\sqrt{3}}$ and $dp_i = \frac{\sqrt{3}E}{c}$ then:
\begin{equation}\label{ultravol}
    \Sigma(E,V,N) = \frac{V^N}{h^{3N} N!} \bigg(\frac{\sqrt{3}E}{c}\bigg)^{3N} \int \limits_{\sum_i |x_i|\leq 1} d^{3N}\textbf{x}
\end{equation}
Clearly, if a set of points $(x_1,...,x_{3N})$ satisfies the condition $\sum_i |x_i| \leq 1$ then so will $(-x_1,...,-x_{3N})$. The volume over which we are integrating in \eqref{ultravol} is therefore symmetric about the origin. We may therefore find the volume in one octant where $0\leq x_i$ and then multiply the result by $2^{3N}$. We therefore need to evaluate:
\begin{equation}
    I_{n} = \int \limits_{\sum_i x_i \leq 1} d^n x, \ x_i \in [0,1]
\end{equation}
for $n=3N$. The geometric region we are integrating over is known as a $n$-simplex, some are shown below.
\begin{figure}[h!]
    \centering
    \includegraphics[width=10cm]{microcano/simplexes.png}
    \caption{2-simplex and 3-simplex}
    \label{fig:my_label}
\end{figure}

Consider the base of a $n$-simplex, which will itself by the corresponding $(n-1)$-simplex in $n-1$ dimensions. Indeed setting $x_n=0$ then we find $\sum_{i=1}^{n-1} x_i \leq 1$, defining the lower dimensional simplex. The \enquote{volume} of this hypersurface is
\begin{equation}
    V_{n} = \frac{1}{n} A_n \cdot h = \frac{A_n}{n}
\end{equation}
where $h$ is the height (which in our case is taken to be 1 when setting $x_1=x_2=...=x_{n-1}=0$) and $A_n$ is the volume of the base. 

\begin{figure}[h!]
    \centering
    \includegraphics[width=6cm]{microcano/surface simplex.png}
    \caption{Evaluating volume of a simplex}
    \label{fig:my_label}
\end{figure}
To see why, let us consider sections of the simplex at constant $x_n$ (height). Using similarity relations we must have that the sides $\mathcal{S}(x_n)$ are all scaled by $\frac{h-x_n}{h}$, so that:
\begin{equation}
    A(x_n) = A_n \bigg(1-\frac{x_n}{\hbar}\bigg)^{n-1}
\end{equation}
Hence:
\begin{align}
    V_n &= \int_0^h A_n\bigg(1-\frac{x_n}{\hbar}\bigg)^{n-1} dx_n=\frac{h}{n} A_n
\end{align}
as desired.

However, note that $V_n$ is really just $I_{n}$ and $A_n = I_{n-1}$ so that:
\begin{equation}
    I_n = \frac{I_{n-1}}{n} \implies I_n = \frac{1}{n!}
\end{equation}
implying that:
\begin{equation}
    \int \limits_{\sum_i |x_i|\leq 1} d^{3N}\textbf{x} = \frac{2^{3N}}{(3N)!}
\end{equation}
The total number of states enclosed by the energy hypersurface is:
\begin{equation}
    \Sigma(E,V,N) = \frac{V^N}{h^{3N} N!} \bigg(\frac{\sqrt{3}E}{c}\bigg)^{3N} \frac{2^{3N}}{(3N)!}
\end{equation}
The density of states at energy $E$ is:
\begin{align}
    g(E,V,N) = \frac{\partial \Sigma(E,V,N)}{\partial E} &= \frac{V^N}{h^{3N} N!} \bigg(\frac{2\sqrt{3}}{c}\bigg)^{3N} \frac{E^{3N-1}}{(3N-1)!}\\
    &\approx\frac{V^N}{h^{3N} N!} \bigg(\frac{2\sqrt{3}}{c}\bigg)^{3N} \frac{E^{3N}}{(3N)!}\frac{1}{E}
\end{align}
Then the number of states occupied by the microcanonical ensemble is the volume of the shell $E<H<E+\Delta E$:
\begin{equation}
  \Omega \Delta E = \frac{V^N}{N!} \bigg(\frac{2\sqrt{3}}{hc}\bigg)^{3N} \frac{E^{3N}}{(3N)!}\frac{\Delta E}{E}
\end{equation}
and hence:
\begin{align}
    S(E,V,N) &= k_b \ln (\Omega \Delta E)  = k_B \ln \bigg[\frac{V^N}{N!} \bigg(\frac{2\sqrt{3}}{hc}\bigg)^{3N} \frac{E^{3N}}{(3N)!}\bigg]+k_B \ln \frac{\Delta E}{E}\\
    &=N k_B \ln \bigg[V\bigg(\frac{2\sqrt{3}}{hc}\bigg)^{3}\bigg] - k_B \ln N! - k_B \ln (3N)!+k_B\ln \frac{\Delta E}{E}
\end{align}
Using Stirling's approximation:
\begin{equation}
    S(E,V,N) =k_B\bigg(N \ln \bigg[V\bigg(\frac{2\sqrt{3}}{hc}\bigg)^{3}\bigg] - N \ln N - 3N \ln (3N) + 4N+\cancelto{0}{\ln \frac{\Delta E}{E}}\bigg)
\end{equation}
We may ignore all terms not of order $N$ (i.e. $\ln \frac{\Delta E}{E}$), giving upon some simplification
\begin{equation}
    \boxed{S(E,V,N) = Nk_B\Bigg(4+ \ln \Bigg[\frac{V}{N}\bigg(\frac{2}{\sqrt{3} Nhc}\bigg)^{3}\Bigg]\Bigg)}
\end{equation}
We may now derive the equations of state:
\begin{align}
    &\frac{1}{T} = \frac{\partial S}{\partial E}\bigg|_{V,N}=\frac{3Nk_B}{E} \iff E = 3Nk_B T\\
    &\frac{p}{T} = \frac{\partial S}{\partial V}\bigg|_{E,N} = \frac{Nk_B}{V} \iff pV = Nk_B T\\
    &C_V = \frac{\partial E}{\partial T}\bigg|_V = 3Nk_B
\end{align}
and therefore find that:
\begin{equation}
    p=\frac{1}{3}\frac{E}{V} 
\end{equation}
so the pressure is a third of the energy density. 
Suppose we want the probability distribution of $f(p_1)$. Then we need to integrate the microcanonical ensemble distribution $\rho_{mc}$ over all variables except $p_1$:
\begin{align}
f(p_1) &= \int \rho_{mc}(\textbf{q},\textbf{p})\ d^{3N} \textbf{q}\ d^{3N-1} \textbf{q}\\
&= \int \limits _{E<H<E+\Delta E} \frac{1}{\Omega \Delta E}\ d^{3N} \textbf{q}\ d^{3N-1} \textbf{q}
\end{align}
\section{Harmonic oscillators}

\section{Two-state system}
\chapter{The canonical ensemble}
\section{Canonical phase-space density}
We have encountered the microcanonical ensemble, which works perfectly for isolated systems, where the natural variables are $E,V,N$ (energy, volume and particle number are conserved). Technically, one could also model an open system as isolated by including its surroundings, however this turns out to be impractical for most cases due to the complex mathematical calculations involved in finding $\omega(E)$. Instead we take an alternative approach, using something known as the partition function $\mathcal{Z}$ to link thermodynamics with statistical mechanics, rather than $\omega(E)$. The advantage is that while the latter requires evaluating hypervolumes, the partition function is delightfully simple to solve in contrast. 


Consider a system $S$ at a fixed temperature $T$ and with energy $E_S$, connected to a heat bath $R$ at the same temperature and with energy $E_R$. Now the heat bath plus the system form a isolated system so we must have that the total energy:
\begin{equation}
    E = E_R+E_S
\end{equation}
be a constant of motion. Furthermore, since the heat bath is much larger than the system in the macroscopic limit, we may assume that $\frac{E_S}{E} \ll 1$. Consequently, the microstates will now move on temperature surfaces rather than energy surfaces. All energy surfaces will be available, but the ones with large $E_S$ will be less frequently occupied. 


Let $p_i$ be the probability of finding $S$ in a microstate $i$ with energy $E_i$, implying that the reservoir has energy $E-E_i$. The total number of microstates of the combined system is:
\begin{equation}
    \Omega(E) = \sum_i \Omega_R(E-E_i)
\end{equation}
where we sum over microstates. The entropy of the reservoir is given by the Taylor expansion:
\begin{equation}
   S_R= k_B\ln \Omega_R(E-E_i) \approx k \ln \Omega_R(E) - \frac{\partial}{\partial E}(k \ln \Omega_R(E))E_i+o(E_i^2)
\end{equation}
and since $E \approx E_R$:
\begin{equation}
    \frac{\partial S_R}{\partial E} = k_B\frac{\partial}{\partial E}(\ln \Omega_R(E)) = \frac{1}{T}
\end{equation}
we find that:
\begin{equation}
    \Omega_R(E-E_i) \approx \Omega_R(E) \exp \bigg[-\frac{E_i}{k_BT}\bigg]
\end{equation}
Hence the total number of microstates is:
\begin{equation}
    \Omega(E) = \Omega_R(E)  \sum_j \exp \bigg[-\frac{E_j}{k_BT}\bigg]
\end{equation}
We now invoke the postulate of equal a priori probabilities, states of the combined system with the same energy must be equally likely. It follows that out of the $\Omega(E)$ states we have calculated, only $\Omega_R(E) \exp \big(-\frac{E_i}{k_BT}\big)$ correspond to a microstate $i$. Hence the probability of finding the system in microstate $i$ is therefore normalized to:
\begin{equation}
    p_i = \frac{ \exp \Big(-\frac{E_i}{k_BT}\Big)}{\sum \limits_i  \exp \Big(-\frac{E_i}{k_BT}\Big)}
\end{equation}
or in more common notation, where $\beta = \frac{1}{k_BT}$:
\begin{equation}\label{discrete can}
    \boxed{p_i = \frac{ \exp (-\beta E_i)}{\sum_j  \exp (-\beta E_j)}}
\end{equation}

It is important to note that we are summing over all states $j$, rather than energies $E_j$, since there are $\Omega_S(E_j)$ possible microstates for a given energy $E_j$. If we wanted to sum over energy then we would have that the number of microstates of the combined system where the system is in microstate $i$ becomes:
\begin{equation}
    \Omega(E) = \Omega_S(E_i) \Omega_R(E-E_i)
\end{equation}
and thus:
\begin{equation}
    p_i = \frac{ \exp (-\beta E_i)}{\sum_{E_j} \Omega_S(E_j)\exp (-\beta E_j) }
\end{equation}
Furthermore, if we want the probability of the energy $E_i$ rather than the microstate $i$ then:
\begin{equation}
    p(E_i) = \frac{ \Omega_S(E_i) \exp (-\beta E_i)}{\sum_{E_j} \Omega_S(E_j)\exp (-\beta E_j) }
\end{equation}
For continuous microstates we instead have:
\begin{equation}\label{continuous can}
    \boxed{\rho_c(\textbf{q},\textbf{p}) = \frac{ \exp (-\beta {H(\textbf{q},\textbf{p})})}{\frac{1}{h^{3N}} \int \exp (-\beta H(\textbf{q},\textbf{p})) d^3\textbf{q}d^3\textbf{p}}}
\end{equation}
known as the canonical phase space density.

Two important remarks must be made about this distribution. Firstly, as expected, microstates with $E_i \geq k_B T$ are much less likely to be occupied. Furthermore, as $T \rightarrow 0$ the system is forced to occupy the ground state. Indeed, we may rearrange \eqref{discrete can} as:
\begin{equation}
    p_{GS} = \frac{1}{1+\sum_{j \neq GS} e^{-(E_j-E_{GS})/k_B T}}
\end{equation}
and since $E_j-E_{GS} > 0$:
\begin{equation}
    \lim_{T \rightarrow 0} p_{GS} = \lim_{T \rightarrow 0} \frac{1}{1+\sum_{j \neq GS} e^{-(E_j-E_{GS})/k_B T}} = 1
\end{equation}

The denominator in \eqref{discrete can} and \eqref{continuous can} is known as the canonical partition function:
\begin{equation}
    \boxed{\mathcal{Z}=\frac{1}{h^{3N}} \int \exp \Big(-\beta H(\textbf{q},\textbf{p})\Big) d^3\textbf{q} d^3\textbf{p}}
\end{equation}

Using the ensemble average definition of entropy:
\begin{align}
    S &= -k_B\braket{ \ln \rho_c} = h^{3N} \int \rho_c (-k_B \ln \rho_c) d^3 \textbf{q}d^3 \textbf{p}\\
    &=\hbar^{3N}k_B \int \rho_c (\beta H + \ln \mathcal{Z}) d^3 \textbf{q} d^3 \textbf{p}\\
    &= \frac{1}{T} \braket{H} + k_B\ln \mathcal{Z} 
\end{align}
where $k_B \ln \mathcal{Z}$ is brought out of the integral since it is independent of the phase space variables. We now note that $\braket{H}$ may be identified as the internal energy $U$ of the system, and that thus:
\begin{equation}
    \boxed{\braket{U}  = -\frac{\partial}{\partial \beta} \ln \mathcal{Z} = k_B T^2 \frac{\partial}{\partial T} (\ln \mathcal{Z}) }
\end{equation}
and that consequently:
\begin{equation}
    S = k_B \bigg(\ln \mathcal{Z} +T \frac{\partial}{\partial T} (\ln \mathcal{Z}) \bigg) 
\end{equation}
which may be elegantly put into the form:
\begin{equation}
   \boxed{S = k_B \frac{\partial}{\partial T}(T \ln \mathcal{Z})}
\end{equation}

It follows that both the average energy and the entropy may be expressed in terms of the partition function and temperature alone. 

Introducing the Hemholtz free energy:
\begin{equation}
    F= U-TS \implies dF = dU - d(TS)
\end{equation}
then we may use the first law of thermodynamics to write $dU = TdS- pdV$ and hence:
\begin{equation}
    dF = -S dT - pdV = \frac{\partial F}{\partial T}\bigg|_V dT + \frac{\partial F}{\partial V}\bigg|_T dV
\end{equation}
We then recognize that:
\begin{align}
    &S =  -\frac{\partial F}{\partial T}\bigg|_V \\
    &p = -\frac{\partial F}{\partial V}\bigg|_T
\end{align}
which should be familiar from the first part of the lecture notes. This new definition of entropy allows us to write:
\begin{equation}\label{Z hem}
    k_B \frac{\partial}{\partial T}(T \ln \mathcal{Z}) = -\frac{\partial F}{\partial T}\bigg|_V \implies \boxed{F =-k_B T \ln \mathcal{Z} = -\frac{1}{\beta} \ln \mathcal{Z}} 
\end{equation}
We could have also seen this \textit{ a priori} from:
\begin{equation}
    F(T,V,N) = E-TS=k_B T^2 \frac{\partial}{\partial T}(\ln \mathcal{Z}) - k_B T \frac{\partial}{\partial T} (T \ln \mathcal{Z}) = -k_B T \ln \mathcal{Z}
\end{equation}
Note also that systems will tend to equilibrium by minimizing their Hemholtz free energy, and thus by maximizing the partition function as can be seen from \eqref{Z hem}. In other words, systems will tend to macrostates with the largest number of microstates associated with them, thus maximizing entropy as desired. 


We see that the partition function in the canonical ensemble assumes the role of $\Omega$ in the microcanonical ensemble, where we could find $S(E,V,N)$ from $\Omega$, and then all other macroscopic quantities using Maxwell's relations. Similarly in the canonical ensemble we firstly find the Hemholtz free energy from the partition function $\mathcal{Z}$, and then derive all other microscopic quantities using Maxwell's relations.
\section{Important examples}
\subsection{The ideal gas}
Let's consider the ideal gas with Hamiltonian:
\begin{equation}
    H(\textbf{q},\textbf{p}) = \sum_{i=1}^{3N} \frac{{p}_i^2}{2m}
\end{equation}
The partition function is then (taking intoa ccount the Gibb's factor due to the particle's indistinguishability):
\begin{align}
    \mathcal{Z} &= \frac{1}{N! h^{3N}} \int \exp (-\beta  H(\textbf{q},\textbf{p})) d^3\textbf{q} d^3\textbf{p} \\
    &=\frac{1}{N! h^{3N}} \int \exp (-\beta  \sum_{i=1}^{3N} \frac{{p}_i^2}{2m})d^3\textbf{q} d^3\textbf{p}\\
    &= \frac{V^N}{N! h^{3N}} \prod_{i=1}^{3N} \int \exp \bigg(-\beta \frac{{p}_i^2}{2m}\bigg) d{p}_i
\end{align}
Substituting $x=\sqrt{\frac{\beta}{2m}} p_i$ then we find that:
\begin{equation}
    \int e^{-x^2}dx = \sqrt{\pi} \implies \int \exp \bigg(-\beta \frac{{p}_i^2}{2m}\bigg) d{p}_i = \sqrt{\frac{2\pi m}{\beta}}
\end{equation}
and thus:
\begin{equation}
    \mathcal{Z} = \frac{V^N}{N!} \bigg(\frac{2\pi m}{\beta h^2}\bigg)^{3N/2} = \frac{V^N}{N!} \bigg(\frac{2\pi mk_B T}{ h^2}\bigg)^{3N/2}
\end{equation}
We may now introduce the thermal wavelength:
\begin{equation}
    \lambda = \sqrt{\frac{h^2}{2\pi m k_BT}} \implies \mathcal{Z} = \frac{V^N}{N! \lambda^{3N}}
\end{equation}
We may thus construct the hemholtz free energy as:
\begin{align}
    F(T,V,N) = -k_B T \ln \frac{V^N}{N! \lambda^{3N}} &= -k_B T \bigg[N \ln \frac{V}{\lambda^3} - \ln N!\bigg] \\
    &=-k_B T \bigg[N \ln \frac{V}{\lambda^3} - N\ln N+N\bigg]\\
    &= -Nk_B T \bigg[1+\ln \frac{V}{N \lambda^3}\bigg]\\
    &=  -Nk_B T \Bigg\{1+\ln\bigg[ \frac{V}{N}\bigg(\frac{2\pi mk_B T}{ h^2}\bigg)^{3/2}\bigg]\Bigg\}\\
\end{align}
Now that we have found the Hemholtz free energy, we can find all other thermodynamic quantities:
\begin{align}
    &p = -\frac{\partial F}{\partial V}\bigg|_{T,N} = \frac{Nk_B T}{V} \implies pV = Nk_B T\\
    &S = -\frac{\partial F}{\partial T}\bigg|_{V,N} = Nk_B \Bigg\{\frac{5}{2} + \ln \bigg[\frac{V}{N} \bigg(\frac{2\pi m k_B T}{h^2}\bigg)^{3/2}\bigg]\Bigg\}\\
    &\mu = \frac{\partial F}{\partial N}\bigg|_{T,V} = -k_B T \ln \bigg[\frac{V}{N} \bigg(\frac{2\pi m k_B T}{h^2}\bigg)^{3/2}\bigg]
\end{align}
Also:
\begin{equation}
    U=F+TS = \frac{3}{2}Nk_B T
\end{equation}
\subsection{The ultra-relativistic gas}
The Hamiltonian for a non-interacting photon gas is given by:
\begin{equation}
    H(\textbf{q},\textbf{p}) =\sum_{i=1}^N |\textbf{p}_i|c
\end{equation}
Since the particles are indistinguishable, we must include the Gibb's correction factor to the partition function:
\begin{align}
    Z(T,V,N) &= \frac{1}{N! h^{3N}} \int \exp{-\beta H(\textbf{q},\textbf{p})} d\textbf{q}d\textbf{p}\\
    &=\frac{V^N}{N! h^{3N}} \int \exp{-\beta \sum_{i=1}^N |\textbf{p}_i|c} d\textbf{q}d\textbf{p}\\
    &= \frac{V^N}{N!h^{3N}} \prod_{i=1}^N \int \exp{-\beta |\textbf{p}_i|c} d\textbf{p}_i
\end{align}
It is now convenient to adopt spherical coordinates so that:
\begin{align}
    Z(T,V,N) &= \frac{V^N}{N!h^{3N}} \prod_{i=1}^N \int \exp{-\beta r_ic} r_i^2 \sin \theta_i dr_i d\theta_i d\phi_i\\
    &=\frac{V^N}{N!h^{3N}} \prod_{i=1}^N \frac{8\pi}{(\beta c)^3}\\
    &= \frac{(8\pi V)^N}{N!(h \beta c)^{3N}}=\frac{(8\pi V)^N}{N!\lambda^{3N}} 
\end{align}
where we define $\lambda = \frac{hc}{k_B T}$. Now that we have found the partition function it is straightforward to calculate the free energy:
\begin{align}
    F(T,V,N) &= -k_B T \ln Z = -k_B T \ln \frac{(8\pi V)^N}{N!\lambda^{3N}} \\
    &=-k_B T \ln \bigg(\frac{(8\pi V)^N}{N!\lambda^{3N}}\bigg)\\
    &=-k_BT\bigg[N \ln \bigg(\frac{8\pi V}{\lambda^3}\bigg) - N \ln N + N\bigg]\\
    &=-Nk_B T\bigg[\ln \bigg(\frac{8 \pi V}{N \lambda^3}\bigg)+1\bigg]\\
    &=-Nk_B T\bigg\{\ln \bigg[\frac{8 \pi V}{N}\bigg(\frac{k_B T}{hc}\bigg)^3\bigg]+1\bigg\}
\end{align}
The other thermodynamic quantities can be found very easily from $F(T,V,N)$ as follows:
\begin{align}
    &p = -\frac{\partial F}{\partial V}\bigg|_{T,N} = \frac{Nk_B T}{V} \implies pV = Nk_B T\\
    &S = -\frac{\partial F}{\partial T}\bigg|_{V,N} = Nk_B \bigg\{4+ \ln \bigg[\frac{8 \pi V}{N}\bigg(\frac{k_B T}{hc}\bigg)^3\bigg]\bigg\}\\
    &\mu = \frac{\partial F}{\partial N}\bigg|_{T,V} = -k_B T \ln \bigg[\frac{8 \pi V}{N}\bigg(\frac{k_B T}{hc}\bigg)^3\bigg]
\end{align}
giving us an expression for the internal energy:
\begin{equation}
    U = F+TS=3Nk_BT
\end{equation}

\subsection{The harmonic oscillator}
Consider a collection of $N$ indistinguishable harmonic oscillators in three dimensions. The Hamiltonian for this system reads:
\begin{equation}
    H(\textbf{q},\textbf{p}) = \sum_{i=1}^{3N} \bigg(\frac{{p}_i^2}{2m} + \frac{1}{2}m\omega^2{q}_i^2\bigg)
\end{equation}
The partition function reads:
\begin{align}
    \mathcal{Z}(T,V,N) &= \frac{1}{N! h^{3N}} \int \exp{-\beta \sum_{i=1}^{3N}\bigg(\frac{{p}_i^2}{2m} + \frac{1}{2}m\omega^2{q}_i^2\bigg)} d\textbf{p}d\textbf{q}\\
    &=\frac{1}{N! h^{3N}} \prod_{i=1}^{3N} \int \exp{-\beta \frac{{p}_i^2}{2m}} dp_i\int \exp{-\frac{1}{2}\beta m\omega^2{q}_i^2} dq_i\\
    &=\frac{1}{N! h^{3N}} \prod_{i=1}^{3N} \sqrt{\frac{2m \pi}{\beta}} \sqrt{\frac{2\pi}{\beta m \omega^2}}\\
    &= \frac{1}{N!} \bigg(\frac{2\pi}{h\beta \omega}\bigg)^{3N}=\frac{1}{N!} \bigg(\frac{1}{\hbar \beta \omega}\bigg)^{3N}
\end{align}
Consequently, the free energy is found to be:
\begin{align}
    F(T,V,N) &= -k_B T \ln \mathcal{Z} = -k_B T \ln \bigg[\frac{1}{N!} \bigg(\frac{1}{\hbar \beta \omega}\bigg)^{3N}\bigg]\\
    &=-Nk_B T\bigg\{\ln \bigg[\frac{1}{N(\hbar \beta \omega)^3}\bigg]+1\bigg\}
\end{align}
Therefore the pressure, entropy and chemical potential are found to be:
\begin{align}
    &p = - \frac{\partial F}{\partial V}\bigg|_{T,N} = 0\\
    &S = - \frac{\partial F}{\partial T}\bigg|_{V,N} = Nk_B\bigg\{\ln \bigg[\frac{1}{N(\hbar \beta \omega)^3}\bigg]+4\bigg\}\\
    &\mu = \frac{\partial F}{\partial N}\bigg|_{T,V} = -k_B T \ln \bigg[\frac{1}{N(\hbar \beta \omega)^3}\bigg]
\end{align}
and the internal potential thus takes the form of $U = F+TS=3Nk_BT$. 

\section{The equipartition and virial theorems}
\subsection*{Derivation in the canonical ensemble}
Let $H(q_i,p_i)$ be the Hamiltonian of a system and let $x_i$ denote all phase coordinates $\{q_i,p_i\}$. Therefore:
\begin{align}
    \braket{x_i \frac{\partial H}{\partial x_k}} &= \frac{1}{h^{3N}} \int \rho(\textbf{x}) x_i \frac{\partial H}{\partial x_k} d\textbf{x}\\
    &= \frac{1}{h^{3N}\mathcal{Z}} \int x_i \exp(-\beta H(\textbf{x})) \frac{\partial H}{\partial x_k} d\textbf{x}\\
    &= -\frac{1}{\beta h^{3N}\mathcal{Z}} \int x_i \frac{\partial}{\partial x_k}(x_i\exp(-\beta H(\textbf{x}))) d\textbf{x}
    \end{align}
We can now integrate by parts. First, we define $d\textbf{x}'$ to be the infinitesimal volume element excluding $dx_k$. Moreover, we also assume that $x_i\exp{-\beta H}$ vanishes at the boundaries $\min x_k$ and $\max x_k$ (usually because $H\rightarrow \infty$). 
    \begin{align}
  \braket{x_i \frac{\partial H}{\partial x_k}}  &=  -\frac{1}{\beta h^{3N}\mathcal{Z}} \bigg[\int \big[x_i \exp(-\beta H(\textbf{x}))\big]^{\max x_k}_{\min x_k} d\textbf{x}'-\int \delta_{ik} \exp(-\beta H(\textbf{x})) d\textbf{x}\bigg]\\
    &= \frac{1}{h^{3N} \beta}\delta_{ik} \int\frac{\exp(-\beta H(\textbf{x}))}{\mathcal{Z}} d\textbf{x}\\
    &= \delta_{ik} k_B T
\end{align}
It follows from that if we choose canonically conjugate variables, the expectation value of their product will be equal to $k_B T$. For example:
\begin{align}
    &\braket{q_i \frac{\partial H}{\partial q_i}} = -\braket{q_i \dot{p}_i} =k_B T\\
    &\braket{p_i\frac{\partial H}{\partial p_i}} = \braket{p_i \dot{q}_i} = k_B T \implies \braket{\frac{p_i^2}{2m}} = \frac{1}{2}k_B T
\end{align}
The latter can be used to find the mean kinetic energy of the $i$th particle in three dimensions:
\begin{equation}
    \braket{T_i} = \frac{3}{2}k_B T
\end{equation}
so for a collection of $N$ particles the mean kinetic energy is:
\begin{equation}
      \braket{T} = \frac{3}{2}Nk_B T
\end{equation}
Suppose the $i$th particle is acted on by a conservative force $\textbf{F}_i = - \nabla V_i$. Then, we see that:
\begin{equation}
    -\braket{\textbf{r}_i\cdot \textbf{F}_i} = \braket{x_i \frac{\partial V_i}{\partial x_i}}+\braket{y_i \frac{\partial V_i}{\partial y_i}}+\braket{z_i \frac{\partial V_i}{\partial z_i}} = 3k_B T
\end{equation}
so for a central potential of the form $V_i \propto r_i^\alpha$ then $\textbf{r}_i \cdot \textbf{F}_i = - \alpha V_i$ giving:
\begin{equation}
    \braket{\alpha V_i} = 3k_B T = 2\braket{T_i} \implies \braket{T}_i = \frac{\alpha}{2} \braket{V_i}
\end{equation}
which is known as the Virial theorem. Note that in this derivation we assumed that integration in $x$ could be done. While in classical mechanics there is no ambiguity in this assumption, in quantum mechanics this implies that the temperature $k_BT$ must be larger than the quanta of energy in the system. In the special case where $\alpha=2$ we get that $\braket{T_i}=\braket{V_i}$, the potential and kinetic energies contribute equally to the total energy.

More specifically, we have that for a hamiltonian with $f$ degrees of freedom containing only quadratic terms:
\begin{equation}
    H = \sum_{i=1}^{f}(a_iq_i^2+b_ip_i^2)
\end{equation}
then clearly:
\begin{align}
    &H = \frac{1}{2}\sum_{i=1}^{f}\bigg(q_i\frac{\partial H}{\partial q_i}+p_i\frac{\partial H}{\partial p_i}\bigg)\\
\iff &U =\braket{H} = \frac{1}{2}\sum_{i=1}^f\bigg(\braket{q_i\frac{\partial H}{\partial q_i}}+\braket{p_i\frac{\partial H}{\partial p_i}}\bigg) = fk_B T 
\end{align}
In other words, since there are $2f$ quadratic terms, each quadratic term contributes $\frac{1}{2}k_B T$ to the internal energy. This result is known as the \textbf{equipartition theorem}. For $f=3N$ then we get the familiar:
\begin{equation}
    U=\braket{H} = 3Nk_B T
\end{equation}
which we have found several times already in the previous sections.
\subsection*{Derivation in the microcanonical ensemble}
Let $H(q_i,p_i)$ be the Hamiltonian of a system and let $x_i$ denote all phase coordinates $\{q_i,p_i\}$. In the microcanonical ensemble, we have that:
\begin{align}
    \braket{x_i \frac{\partial H}{\partial x_j}} &= \frac{1}{h^{3N}} \int \rho(\textbf{x})x_i \frac{\partial H}{\partial x_j}d\omega\\
    &= \frac{1}{h^{3N}}\frac{1}{\Omega \Delta E} \int_{\Delta E}  x_i \frac{\partial H}{\partial x_j} d\omega\\
    &=\frac{1}{h^{3N}} \frac{1}{\Omega} \frac{\partial}{\partial E}\int_{H<E}  x_i \frac{\partial H}{\partial x_j}d\omega\\
    &= \frac{1}{h^{3N}} \frac{1}{\Omega}  \frac{\partial}{\partial E}\int_{H<E} x_i \frac{\partial (H-E)}{\partial x_j}d\omega
\end{align}
where in the last line we used $\frac{\partial E}{\partial x_j}=0$ where $x=q,p$. This allows us to integrate by parts:
\begin{align}
    \frac{1}{h^{3N}} \frac{1}{\Omega} \frac{\partial}{\partial E}\int_{H<E}  x_i \frac{\partial (H-E)}{\partial x_j}d\omega=  \frac{1}{h^{3N}} \frac{1}{\Omega} \frac{\partial}{\partial E}\bigg[&\int_{H<E} x_i\cancelto{0}{[(H-E)]}_{\min{x_j}}^{\max{x_j}} d\omega'\\ \nonumber
    &- \int_{H<E} (H-E)\frac{\partial x_i}{\partial x_j}d\omega\bigg]
\end{align}
where $d\omega'$ integrates over all variables except $x_j$. If $x_j$ has an extremal value, then this means that it must lie on the energy surface $H=E$, and consequently the integrand vanishes. 

The integral thus simplifies to:
\begin{align}
    -\frac{1}{h^{3N}} \frac{1}{\Omega} \frac{\partial}{\partial E}\int_{H<E} (H-E)\delta_{ij}d\omega &=  \frac{1}{h^{3N}} \frac{1}{\Omega}\delta_{ij} \int_{H<E}d\omega\\
    &=\frac{1}{h^{3N}} \frac{1}{\Omega}\delta_{ij} \omega\\
    &=\frac{\omega'}{\Omega}\delta_{ij} 
\end{align}
where we defined $\omega' = \frac{1}{h^{3N}} \omega$. Using the definition of $\Omega = \frac{\partial \omega'}{\partial E}$ we retrieve the remarkable result:
\begin{align}
     \braket{x_i \frac{\partial H}{\partial x_j}} &= \frac{\omega'}{\frac{\partial \omega'}{\partial E}}\delta_{ij}=\frac{\omega'}{1}{\frac{\partial}{\partial E}(\ln \omega')}\delta_{ij}\\
     &=\frac{\omega'}{k_B}{\frac{\partial S}{\partial E}}\delta_{ij}= k_B T \delta_{ij}
\end{align}
as found previously.
\chapter{Canonical examples of the canonical ensemble}
\section{The Quantum Harmonic Oscillator}
\section{The Diatomic gas}
\section{The Paramagnet}
\chapter{The Grand Canonical Ensemble}
\section{Adding the chemical potential}
We have seen how to describe isolated systems which conserve both energy and particle number using the microcanonical ensemble, as well as closed systems which conserve only particle number, but allow variable energies, using the canonical ensemble. It is thus fitting to have a third ensemble which can handle open systems where neither energy nor particle number are constant. Such an ensemble is of particular use when describing systems where heat and particles can be exchanged through contact with a heat and particle bath. 

The natural variables to be used are $T,V$ and $\mu$, the chemical potential. 

The chemical potential is defined as the amount by which the internal energy will change when adding a particle to the system isentropically and isochorically (constant entropy and volume). We must therefore revisit our First Law of Thermodynamics and add a new term:
\begin{equation}
    \boxed{dU = TdS - pdV + \mu dN}
\end{equation}
giving us the following definition of $\mu$:
\begin{equation}
    \mu = \bigg(\frac{\partial U}{\partial N}\bigg)_{S,V}
\end{equation}
Recalling the definitions of the other thermodynamic potentials:
\begin{align}
    &F=U-TS \implies dF = -pdV - SdT + \mu dN\\
    &G=F+pV \implies dG = Vdp - SdT + \mu dN
\end{align}
and equivalently:
\begin{equation}
    \mu = \bigg(\frac{\partial F}{\partial N}\bigg)_{V,T}, \hspace{1cm} \mu = \bigg(\frac{\partial G}{\partial N}\bigg)_{p,T}
\end{equation}
The chemical potential can also be related to entropy and the second law of thermodynamics. Using $U,V,N$ as natural variables then:
\begin{equation}
    dS = \bigg(\frac{\partial S}{\partial U}\bigg)_{V,N}+\bigg(\frac{\partial S}{\partial V}\bigg)_{U,N}+\bigg(\frac{\partial S}{\partial N}\bigg)_{V,U}
\end{equation}
which when compared with 
\begin{equation}
    dS =\frac{dU}{T}+\frac{p}{T}dV - \frac{\mu}{T}dN
\end{equation}
yields:
\begin{equation}\label{entropy derivs}
   \boxed{\bigg(\frac{\partial S}{\partial U}\bigg)_{V,N} = \frac{1}{T}, \hspace{0.5cm}  \bigg(\frac{\partial S}{\partial V}\bigg)_{U,N} = \frac{p}{T}, \hspace{0.5cm}\bigg(\frac{\partial S}{\partial N}\bigg)_{V,U} = -\frac{\mu}{T}}
\end{equation}
This can be used to investigate how and when a system reaches equilibrium. Indeed, suppose we have two systems decoupled to their surroundings which exchange heat and particles. It is clear that $dU_1 = - dU_2 = dU$,$dV_1 = - dV_2 = dV$,$dN_1 = -dN_1=dN$  since the two systems combined form an isolated system. Then:
\begin{align}
    dS_{tot} &= \frac{dU_1}{T_1}+\frac{p_1}{T_1}dV_1 - \frac{\mu_1}{T_1}dN_1 + \frac{dU_2}{T_2}+\frac{p_2}{T_2}dV_2 - \frac{\mu_2}{T_2}dN_2\\
    &=\bigg(\frac{1}{T_1}-\frac{1}{T_2}\bigg)dU +\bigg(\frac{p_1}{T_1}-\frac{p_2}{T_2}\bigg)dV+\bigg(\frac{\mu_1}{T_1}-\frac{\mu_2}{T_2}\bigg)dN
\end{align}
For equilibrium to be established we need $dS_{tot}=0$ and hence:
\begin{equation}
    \boxed{T_1=T_2,\hspace{0.4cm} p_1=p_2,\hspace{0.4cm}\mu_1=\mu_2}
\end{equation}
The first imposes \textbf{thermal equilibrium}, the second imposes \textbf{mechanical equilibrium} and the third imposes \textbf{chemical equilibrium}. Now suppose we have just one system which can exchange $U,V,N$ with a reservoir. We then see that:
\begin{equation}
     dS_{tot} = dS - \frac{dU}{T_R}-\frac{p_R}{T_R}dV+ \frac{\mu_R}{T_R}dN
\end{equation}
Define the availability as we have previously as $dA = -T_R dS_{tot}$, then:
\begin{equation}
    dA = -T_R dS + dU + p_R dV - \mu_R dN
\end{equation}
and inserting $dU = TdS - p dV + \mu dN$ we finally find:
\begin{equation}
    \boxed{dA = (T-T_R)dS -(p-p_R)dV + (\mu-\mu_R)dN}
\end{equation}
From our definition of the availability, equilibrium is established when availability is minimized (entropy maximized).

\section{Euler's relation and the Gibbs-Duhem equation}
After having re-introduced a myriad of new notation it may be interesting to know how the intensive variables $\mu$, $T$ and $p$ are related. Note that the internal energy is an extensive quantity and thus satisfies
\begin{equation}
    U(\lambda S, \lambda V, \lambda N) = \lambda U(S,V,N)
\end{equation}
for any scaling factor $\lambda$. Differentiating with respect to $\lambda$ we find that
\begin{align}
   U(S,V,N) &= \frac{\partial U(\lambda S)}{\partial (\lambda S)}\bigg|_{V,N}S+\frac{\partial U(\lambda V)}{\partial (\lambda V)}\bigg|_{S,N}V+\frac{\partial U(\lambda N)}{\partial (\lambda N)}\bigg|_{V,S}N\\
   &=TS-pV+\mu N
\end{align}
so we have found that
\begin{equation}
    \boxed{U=TS-pV+\mu N} \implies dU = d(TS)-d(pV)+d(\mu N)
\end{equation}
which is known as Euler's relation. The same could have been derived using any other extensive state function (e.g. entropy). Note however that from the first law, $dU = TdS - pdV + \mu dN$ so it follows that
\begin{equation}
    \boxed{SdT - Vdp+Nd\mu = 0}
\end{equation}
which is known as the Gibbs-Duhem equation. It shows that $T,p,\mu$ are not independent and any one of them can be expressed as a function of the other two!
\section{The Grand canonical phase space density}
We consider an open system $S$ with energy $E_S$ and number of particles $N_S$ at temperature $T$ and chemical potential $\mu$ surrounded by a reservoir $E_R$ and number of particles $N_R$ at the same temperature and chemical potential. The system plus the reservoir form an isolated system which we refer to as the total system. Thus we define $E=E_S+E_R$ and $N=N_S+N_R$ to be the energy and number of particles of the total system, and assume that:
\begin{equation}
    \frac{N_S}{N}\ll 1, \ \frac{E_S}{E}\ll 1
\end{equation}

The number of microstates of the system consistent with the macrostate $E,N$ is:
\begin{equation}
    \Omega(E,N) = \sum_{ij} \Omega_R(E-E_i,N-N_j)
\end{equation}
where we sum over microstates with energy $E_i$ with particle number $N_j$. Defining the Boltzmann entropy of the reservoir as $S_R = k_B \ln \Omega_R$ we find that:
\begin{align}
    S_R &= k_B \ln \Omega_R(E-E_i,N-N_j) \\
    &\approx k_B \ln \Omega_R(E,N) - E_i \frac{\partial S}{\partial E} - N_j\frac{\partial S}{\partial N}\\
    &\approx k_B \ln \Omega_R(E,N) - -\frac{E_i}{T}+\frac{\mu N_j}{T}
\end{align}
where we used \eqref{entropy derivs}. Consequently we find that:
\begin{equation}
    \Omega_R(E-E_i,N-N_j) = \Omega_R(E,N) \exp(-\beta(E_i-\mu N_j))
\end{equation}
giving the following probability distribution:
\begin{equation}\label{grand prob}
    \boxed{p_{ij} = \frac{1}{\mathcal{Z}} \exp(-\beta(E_i-\mu N_j)), \ \mathcal{Z} = \sum_{ml}\exp(-\beta(E_m-\mu N_l))}
\end{equation}
We define $\mathcal{Z}$ as the grand partition function. 

An alternative way to derive \eqref{grand prob} is by maximizing the Gibbs entropy.


We can extend \eqref{grand prob} to the continuous case, letting $\alpha = \beta \mu$ it is given by:
\begin{equation}
    \boxed{\rho(\textbf{p},\textbf{q},N) = \frac{1}{\mathcal{Z}} \exp(-\beta H+\alpha N), \ \mathcal{Z} = \sum_N \frac{1}{h^{3N}} \int \exp(-\beta H + \alpha N)\ d^{3N} \textbf{p} \ d^{3N} \textbf{q}}
\end{equation}

Using the ensemble average definition of entropy we get that:
\begin{align}
    S &= - k_B \braket{\ln \rho} = -\frac{k_B}{\mathcal{Z}} \int \exp(-\beta H+\alpha N)[-\ln \mathcal{Z}-\beta H + \alpha N] d\omega \\
    &= k_B[\ln \mathcal{Z}+\beta U - \alpha \braket{N}]
\end{align}
Let $N=\braket{N}$ to be the average particle number, then we can define the grand canonical potential $\Phi_G$ as:
\begin{equation}
    \boxed{\Phi_G = -\frac{1}{\beta} \ln \mathcal{Z}=-TS+U-\mu N = F-\mu N}
\end{equation}
Alternatively, using Euler's relation we can write
\begin{equation}
\Phi_G = -pV  
\end{equation}
which tells us that the Gibbs free energy takes the simple form:
\begin{equation}
    G = U + pV - TS = \mu N
\end{equation}
Finally, the thermodynamic variables can be expressed in terms of the grand potential:
\begin{align}
d\Phi_G = & dF - \mu dN -Nd\mu= -SdT-pdV-Nd\mu\\
    \implies & \boxed{\mu = -\frac{\partial \Phi_G}{\partial N}\bigg|_{V,S}, \ p = -\frac{\partial \Phi_G}{\partial V}\bigg|_{S,N}, \ S = -\frac{\partial \Phi_G}{\partial T}\bigg|_{N,V}}
\end{align}
\section{The ideal gas in the GCE}

\section{Particle number fluctuation}
The canonical partition function $Z_N$ for an $N$ particle system can be related to the grand partition function of the same system with variable particle number via a discrete Laplace transform:
\begin{equation}
    \boxed{\mathcal{Z} = \sum_N z^N Z_N, \ z=e^{\mu \beta}}
\end{equation}
where $z$ is known as the fugacity. We see that, much like the Boltzmann weight is used to average over microcanonical ensembles at different $E$ to get the canonical ensemble, the fugacity is the weight used to average over the canonical ensembles at different $N$ to get the grand canonical ensemble. 

The fugacity may be used to express $N$ in a different way. Indeed (we use $\braket{N}$ instead of $N$ to avoid confusion):
\begin{equation}
    \braket{N} = \sum_N N p_N = \sum_N N \sum_i \frac{\exp(-\beta E)}{\mathcal{Z}} z^N = \frac{1}{\mathcal{Z}} \sum_N N z^N Z_N
\end{equation}
Note also that:
\begin{equation}
    \frac{\partial \mathcal{Z}}{\partial z}\bigg|_{T,V} = \sum_N N z^{N-1}Z_N(T,V)
\end{equation}
which allows us to write the particle number as a derivative with respect to fugacity rather than chemical potential:
\begin{equation}\label{N fuga}
    \braket{N} = z\frac{\partial}{\partial z}(\ln \mathcal{Z})\bigg|_{T,V} = -\frac{z}{\beta} \frac{\partial \Phi_G}{\partial z}\bigg|_{T,V}
\end{equation}
We would also like to find the fluctuation in $\braket{N}$ so we need to calculate $\braket{N^2}$:
\begin{equation}
    \braket{N}^2 = \frac{1}{\mathcal{Z}} \sum_N N^2 z^N Z_N
\end{equation}
To do so we investigate the second order derivative of $\ln \mathcal{Z}$:
\begin{align}
        \frac{\partial^2 \mathcal{Z}}{\partial z^2}\bigg|_{T,V} &= \frac{\partial}{\partial z}\bigg(\frac{1}{\mathcal{Z}} \sum_N N z^{N-1}Z_N\bigg)\bigg|_{T,V}\\
        &=-\bigg(\frac{1}{\mathcal{Z}} \frac{\partial \mathcal{Z}}{\partial z}\bigg|_{T,V}\bigg)^2+\frac{1}{\mathcal{Z}} \sum_N N(N-1) z^{N-2} Z_N\\
        &=-\bigg(\frac{\braket{N}}{z}\bigg)^2+\frac{1}{\mathcal{Z}} \sum_N N^2 z^{N-2} Z_N-\frac{1}{\mathcal{Z}} \sum_N z^{N-2} Z_N\\
        \implies z^2  \frac{\partial^2 \mathcal{Z}}{\partial z^2}  &= \braket{N^2}-\braket{N}^2-\braket{N}
\end{align}
Substituting \eqref{N fuga} into the above we find that
\begin{equation}
    (\Delta N)^2= z^2  \frac{\partial^2 \mathcal{Z}}{\partial z^2}\bigg|_{T,V}  -  z\frac{\partial}{\partial z}(\ln \mathcal{Z})\bigg|_{T,v} = \bigg(z\frac{\partial}{\partial z}\bigg)^2(\ln \mathcal{Z})\bigg|_{T,V}
\end{equation}
We can also use $\braket{N} = z\frac{\partial}{\partial z} =  z\frac{\partial}{\partial z}(\ln \mathcal{Z})\big|_{T,V} = -\frac{z}{\beta} \frac{\partial \Phi_G}{\partial z}\big|_{T,V}$ to write
\begin{equation}
    \boxed{(\Delta N)^2 = z\frac{\partial \braket{N}}{\partial z}\bigg|_{T,V}=\frac{1}{\beta} \frac{\partial \braket{N}}{\partial \mu}\bigg|_{T,V}}
\end{equation}
since $\frac{\partial \mu}{\partial z} = \frac{1}{\beta}$. Also, note that
\begin{equation}
    \braket{N} = -\frac{\partial \Phi_G}{\partial \mu} \bigg|_{T,V} = V \frac{\partial p}{\partial \mu}\bigg|_{T,V}
\implies
    \frac{\partial ^2 p}{\partial \mu^2}\bigg|_{T,V} = \frac{\beta}{V} (\Delta N)^2
\end{equation}
Note that the LHS is an intensive quantity, so for the RHS to also be intensive we should require
\begin{equation}
    \frac{\Delta N}{\braket{N}} \sim \frac{\sqrt{V}}{\braket{N}} \sim \frac{1}{\sqrt{\braket{N}}}
\end{equation}
so the fluctuations in particle number are of the order of magnitude of the inverse root of the average particle number. In the thermodynamic limit the number fluctuations thus vanishes thus showing the the canonical and grand canonical ensembles are indeed equivalent.
\section{Mechanical stability condition}
In the canonical ensemble we showed that the heat capacity was proportional to the energy fluctuation squared, and thus had to be non-negative yielding a thermal stability condition. Similarly, we can derive a mechanical stability condition, namely that the thermal compressibility must be non-negative.

Let us define $f=F(T,V,N)/N$ to be the free energy per particle. Note that
\begin{equation}
    \mu = \frac{\partial F}{\partial N}\bigg|_{V,T} =  f + N \frac{\partial f}{\partial N}\bigg|_{V,T}
\end{equation}
and letting $v=\frac{V}{N}$ then $\frac{\partial f}{\partial N}\big|_{V,T} = -\frac{V}{N^2}\frac{\partial f}{\partial v}\big|_{V,T}$ which implies
\begin{equation}
   \mu = f - v\frac{\partial f}{\partial v} \bigg|_{V,T}\implies \frac{\partial \mu}{\partial v}\bigg|_{V,T} =-v \frac{\partial^2 f}{\partial v}\bigg|_{V,T}
\end{equation}
We can do the same for pressure
\begin{equation}
    p = -\frac{\partial F}{\partial V}\bigg|_{N,T} =-N\frac{\partial f}{\partial V}\bigg|_{N,T} = -\frac{\partial f}{\partial v}\bigg|_{N,T}
\end{equation}
implying that
\begin{equation}
    \frac{\partial p}{\partial v}\bigg|_{N,T} = -\frac{\partial^2 f}{\partial v^2}\bigg|_{N,T} \implies \frac{\partial \mu}{\partial v}\bigg|_{V,T} = v \frac{\partial p}{\partial v}\bigg|_{N,T} 
\end{equation}

These relations can be used to express the particle fluctuation as
\begin{align}
    \frac{\beta}{V}(\Delta N)^2 &=\frac{\partial^2 p}{\partial \mu^2}\bigg|_{T,V} = \frac{\partial v}{\partial \mu}\bigg|_{T,V} \frac{\partial}{\partial v}\bigg(\frac{\partial p}{\partial \mu}\bigg)\bigg|_{T,V}\\
    &=\frac{1}{v}\frac{\partial v}{\partial p}\bigg|_{N,T}\frac{\partial}{\partial v}\frac{1}{v}\bigg|_{T,V}=\frac{1}{v^2} \kappa_T
\end{align}
where we defined the thermal compressibility
\begin{equation}
    \kappa_T =  -\frac{1}{v}\frac{\partial v}{\partial p}\bigg|_{N,T} = -\frac{1}{V}\frac{\partial V}{\partial p}\bigg|_{N,T} 
\end{equation}
since the partial derivative is at constant particle number. Thus we have found that the compressibility is related to the particle number fluctuation
\begin{equation}
    \boxed{\kappa_T = \beta V \bigg(\frac{\Delta N}{N}\bigg)^2 >0}
\end{equation}

The quantities $C_V$ and $\kappa_T$ are both response functions, they give the system's response to a temperature and pressure perturbation respectively, in the form of the heat and volume changes. What the mechanical stability condition says is that $\kappa_T \delta V>0$, namely the system's response to a volume increase $\delta V$, must be to increase the pressure by $\delta p>0$ so as to counteract the volume change. If the compressibility were negative then an increase in volume $\delta V>0$ would lead to a decrease in pressure $\delta p<0$ making $V$ spiral out of control! Such a macrostate cannot possibly exist as it is extremely unstable to perturbations, an example of this will be seen when discussing the liquid-gas phase coexistence in the Van der Waals model.

The fact that these response functions are related to fluctuations is not a coincidence at all, and goes by the name of the \textbf{fluctuation-dissipation theorem}. In general, a system where a quantity is not fluctuating will not respond to a perturbation in this quantity.
\part{Quantum statistical physics}
\chapter{Density operator methods}
\section{Making the jump to quantum}
In classical statistical mechanics we assigned probabilities to microstates $(\textbf{q},\textbf{p})$, which were points in the phase space. In quantum statistical mechanics our microstates will be quantum states $\ket{\psi}$ in the Hilbert space. 

We also measure observables $A(\textbf{q},\textbf{p})$ defined as functions in the phase space. In quantum mechanics observables are replaced by linear hermitian operators $\hat{A} \in \mathcal{L}(\mathcal{H})$ acting on the Hilbert space. The expectation value of some operator in a given state gives the expectation value, the average value measured in an experiment.
\begin{table}[h!]
    \centering
    \begin{tabular}{c|c|c}
                 & CSM  & QSM \\
                 \hline
      Microstate &
      $(\textbf{q},\textbf{p}) \in \mathcal{P}$& $\ket{\psi} \in \mathcal{H}$\\
       Observable & $A(\textbf{q},\textbf{p})$ & $\braket{A}, \ \hat{A} \in \mathcal{L}(\mathcal{H})$\\
    Canonical conjugation & $\{q_i,p_j\} = \delta_{ij}$ & $[\hat{q}_i,\hat{p}_i] = i \hbar \delta_{ij}$\\
    Time evolution & $\frac{dA}{dt} = \{A,H\} + \frac{\partial A}{\partial t}$ & $\frac{d\braket{A}}{dt} = \frac{1}{i\hbar} \braket{[\hat{A},\hat{H}]} + \big \langle \frac{\partial \hat{A}}{\partial t}\big\rangle$
        \end{tabular}
\end{table}

Consider a quantum system prepared in an ensemble of states $\{\ket{\psi_i}\}$ each with probability $p_i$. That is, not only do we have uncertainties inherent to quantum mechanics, but we also have the uncertainty in the wavefunction of the system. The state of a system prepared like this is known as a mixed state. The ensemble averaged expectation value $\bar{\hat{f}}$ of some operator $\hat{f}$ is given by the weighted average of the expectation value for each state in the ensemble:
\begin{align}
    \bar{\braket{f}} &= \sum_i p_i \braket{\psi_i|\hat{f}|\psi_i}
\end{align}
We can expand this in a given orthonormal basis as:
\begin{equation}
     \bar{\braket{f}} = \sum_{imn} p_i \braket{\psi_i|n}\braket{n|\hat{f}|m}\braket{m|\psi_i} = \sum_{mn} \braket{n|\hat{f}|m} \sum_i \braket{m|\psi_i}\braket{\psi_i|n}
\end{equation}
We define the density matrix related to the state of this system is given by:
\begin{equation}
    \hat{\rho} = \sum_i p_i \ket{\psi_i}\bra{\psi_i}
\end{equation}
which allows us to write:
\begin{equation}\label{exp dens}
    \Bar{\braket{f}}  = \sum_{mn} \braket{n|\hat{f}|m} \braket{m|\rho|n} = \Tr (\hat{\rho} \hat{f})
\end{equation}
So the expectation value of an observable $f$ for a system in a mixed state defined by a density matrix $\rho$ can be expressed as a trace:
\begin{equation}
    \boxed{\Bar{\braket{f}} = \Tr (\hat{\rho} \hat{f})}
\end{equation}
\eqref{exp dens} is reminiscent of the classical formula:
\begin{equation}
    \braket{f} = \int f(\textbf{p},\textbf{q}) \rho(\textbf{p},\textbf{q}) d\omega
\end{equation}
so we should identify the density matrix with the ensemble probability density. In other words $\rho(\textbf{p},\textbf{q})$ got promoted to $\hat{\rho}$. 

It is easy to see that:
\begin{enumerate}
    \item[(i)] the density matrix has unit trace:
    \begin{equation}
        \Tr \hat{\rho} = \sum_j \sum_i p_i \braket{j|i}\braket{i|j} = \sum_j p_j = 1
    \end{equation}
    corresponding to the normalization of classical probability densities.
    \item[(ii)] the density matrix is hermitian:
    \begin{equation}
        \hat{\rho}^\dagger = \sum_i p_i (\ket{i} \bra{i})^\dagger = \rho
    \end{equation}
    corresponding to the fact that classical probabilities are real numbers.
    \item[(iii)] The density operator is positive definite:
        \begin{equation}
         \braket{\psi|\hat{\rho}|\psi} = \sum_i p_i |\braket{\psi|\psi_i}|^2>0, \ \forall \ket{\psi} \in \mathcal{H}
        \end{equation}
        corresponding to the fact that the classical probability density must be positive.
\end{enumerate}
Recall also that in classical physics the evolution of $\rho$ was given by Louiville's theorem:
\begin{equation}
    \frac{\partial \rho}{\partial t} = \{H,\rho\}
\end{equation}
Note that in the Schrodinger picture we have a completely analogous result:
\begin{align}
    i \hbar \frac{d\hat{\rho}}{dt} = \hat{H} \sum_i p_i(\ket{\psi_i} \bra{\psi_i} &- \ket{\psi_i}\bra{\psi_i})\hat{H} = \hat{H} \hat{\rho} - \hat{\rho}\hat{H}\\
    \implies& \boxed{\frac{d \hat{\rho}}{dt} = \frac{1}{i\hbar}[\hat{H},\hat{\rho}]}
\end{align}
which is known as the Von Neumman equation. It is important to notice the remarkable duality between Poisson brackets and commutators. Here we assumed that $p_i$ does not change, this is reasonable as long as the system we consider is not interacting with its surroundings. 

The equilibrium density operator must be time-independent so we need $[\hat{H},\hat{\rho}]=0$. Thus in statistical mechanics we require $\rho$ to be a function of operators commuting with $\hat{H}$, thus corresponding to conserved quantities. A completely analogous statement applied to classical statistical mechanics.

The time evolution of the density operator is generally non-linear, unlike the Schrodinger evolution which only applies to pure states. It follows that:
\begin{align}
    i \hbar \frac{d\braket{f}}{dt} = i\hbar \frac{d}{dt}[\Tr (\hat{\rho}\hat{f})]= \Tr \bigg([\hat{H},\hat{\rho}] \hat{f} + i\hbar \hat{\rho} \frac{\partial \hat{f}}{\partial t}\bigg)=\Tr \big([\hat{H},\hat{\rho}] \hat{f} \big) + i \hbar \bigg\langle\frac{\partial \hat{f}}{\partial t}\bigg \rangle
\end{align}
so in equilibrium:
\begin{equation}
\bigg\langle\frac{\partial \hat{f}}{\partial t}\bigg \rangle = 0
\end{equation}
\section{Quantum statistical ensembles}
\subsection*{Microcanonical ensemble}
We find that:
\begin{equation}
    \rho_{mc} = \frac{1}{\Omega(E)} \delta_{H,E}  
\end{equation}
so the density matrix elements are:
\begin{equation}
    \braket{m|\rho_{mc}|n} = \frac{1}{\Omega(E)}\begin{cases}
    1, \ \text{ if } E_m=E \text{ and } m=n\\
    0, \ \text{ if } E_m \neq E \text{ or } m \neq n
    \end{cases}
\end{equation}
The energy condition $E_m=E$ comes from the delta function $\delta_{H,E}$ while the diagonal condition $m=n$ comes from the orthonormality of energy eigenstates. It follows that the density matrix is diagonal whose only non-zero elements are those with energy $E$. Note that this density operator satisfies the equal-a-priori probability since eigenstates of equal energy have the same probability.
\subsection*{Canonical ensemble}
We find that:
\begin{equation}
    \hat{\rho} = \frac{1}{Z} e^{-\beta \hat{H}}, \ Z = \Tr (e^{-\beta \hat{H}})
\end{equation}
or in the energy eigenbasis $\{\ket{n}\}$:
\begin{equation}
     \hat{\rho} = \frac{1}{Z} e^{-\beta E_n}\ket{n}\bra{n}
\end{equation}
\section{Particle in a box}
Consider a free particle in a box of length $L$ with periodic boundary conditions. Then the energy eigenstates in the position basis read:
\begin{equation}
    \braket{x|k} = \frac{1}{\sqrt{V}} e^{i \textbf{k}\cdot \textbf{r}}
\end{equation}
with energy
\begin{equation}
    E_k = \frac{\hbar^2 k^2}{2m}, \ k_{x,y,z}=\frac{2\pi}{L} n_{x,y,z}
\end{equation}
We are interested in the matrix elements of the density matrix in the position representation. We look at the infinite $L$ limit where the possible momenta now form a continuum. Then we must replace:
\begin{equation}
    \frac{1}{V}\sum_\textbf{k} = \int \frac{d^3\textbf{k}}{(2\pi)^3}
\end{equation}
where $V=L^3$. Therefore:
\begin{align}
    \braket{\textbf{r}|\hat{\rho}|\textbf{r}'} &=\int \frac{d^3\textbf{k}}{(2\pi)^3} e^{-\beta E(k)} \braket{\textbf{r}|\textbf{k}}\braket{\textbf{k}|\textbf{r}'} \\
    &=\frac{1}{Z} V \int \frac{d^3\textbf{k}}{(2\pi)^3} \exp \bigg[-\frac{\beta \hbar^2 \textbf{k}^2}{2m}+i\textbf{k}\cdot (\textbf{r}-\textbf{r}')\bigg]\\
    &=\frac{1}{Z} \frac{1}{(2\pi)^3} \bigg(\sqrt{\frac{2m \pi}{\beta \hbar^2}}\bigg)^3 \exp \bigg(-\frac{m}{2\beta \hbar^2}(\textbf{r}-\textbf{r}')^2\bigg) \label{diff}
\end{align}
We define the thermal wavelength to be:
\begin{equation}
    \boxed{\lambda = \frac{\hbar}{\sqrt{2\pi m k_B T}}}
\end{equation}
which further simplifies \eqref{diff} into:
\begin{equation}
   \rho(\textbf{r},\textbf{r}')\equiv \braket{\textbf{r}|\hat{\rho}|\textbf{r}'} = \frac{1}{Z \lambda^3} \exp \bigg(-\frac{\pi(\textbf{r}-\textbf{r}')^2}{\lambda^2}\bigg)
\end{equation}
Calculating the partition function is also quite easy using the appropriate Gaussian integral:
\begin{equation}
    Z = \int \frac{d^3\textbf{k}}{(2\pi)^3} e^{-\beta \hbar^2 \textbf{k}^2/2m} = \frac{V}{\lambda^3}
\end{equation}
giving:
\begin{equation}
 \boxed{ \rho(\textbf{r},\textbf{r}') = \frac{1}{V} \exp \bigg(-\frac{\pi(\textbf{r}-\textbf{r}')^2}{\lambda^2}\bigg)}
\end{equation}
Clearly, for N distinguishable particles we would have:
\begin{equation} \rho(\{\textbf{r}_i\},\{\textbf{r}_i'\}) = \frac{1}{V^N} \exp \bigg(-\sum_i\frac{\pi(\textbf{r}_i-\textbf{r}_i')^2}{\lambda^2}\bigg)
\end{equation}
\section{Identical particles}
The situation for identical particles gets much more complicated due to the (anti)symmetrization of the (fermionic) bosonic wavefunctions:
\begin{align}
    &\ket{k_1,k_2,...,k_N}_A = \frac{1}{\sqrt{N!}}\sum_{\sigma \in S_N} \text{sgn}(\sigma)P_\sigma \ket{k_1,k_2,...,k_N}\\
    &\ket{k_1,k_2,...,k_N}_S = \frac{1}{\sqrt{N! \prod_i (n_i!)}}\sum_{\sigma \in S_N} P_\sigma \ket{k_1,k_2,...,k_N}
\end{align} 
where $n_i$ are the number of particles in state $\ket{k_i}$. For simplicity let $\ket{\psi}_\eta$ denote the fermionic wavefunction if $\eta=-1$ and the bosonic wavefunction if $\eta=1$, so that:
\begin{equation}
    \ket{k_1,k_2,...,k_N}_\eta = \frac{1}{\sqrt{N! \prod_i (n_i!)}}\sum_{\sigma \in S_N} \eta^{\text{sgn}(\sigma)}P_\sigma \ket{k_1,k_2,...,k_N}
\end{equation}
since $n_k=0,1$ for fermions. We then find that:
\begin{equation}\label{symmetriz}
    \rho_\eta(\{\textbf{r}_i\},\{\textbf{r}'_i\}) =  \frac{1}{Z}\sum_{\{\textbf{k}_i\}} \exp\bigg(-\sum_i \frac{\beta \hbar^2 k_i^2}{2m}\bigg)\braket{\textbf{r}_1',...,\textbf{r}_N'|\textbf{k}_1,...,\textbf{k}_N}_\eta \braket{\textbf{k}_1,...,\textbf{k}_N|\textbf{r}_1',...,\textbf{r}_N}_\eta 
\end{equation}
Note that the sum over $\{\textbf{k}_i\}$ is not unrestricted, since different sequences of $\textbf{k}_i$ can still symmetrize to the same wave-function. It is easy to see that there are $\frac{N!}{\prod_i (n_i)!}$ wavefunctions that symmetrize to the same state. Hence:
\begin{equation}\label{dens}
    \rho_\eta(\{\textbf{r}_i\},\{\textbf{r}'_i\})  = \sum_{\{\textbf{k}_i\}} \frac{\prod_i (n_i)!}{N!Z} \exp\bigg(-\sum_i \frac{\beta \hbar^2 k_i^2}{2m}\bigg)\braket{\textbf{r}_1',...,\textbf{r}_N'|\textbf{k}_1,...,\textbf{k}_N}_\eta \braket{\textbf{k}_1,...,\textbf{k}_N|\textbf{r}_1,...,\textbf{r}_N}_\eta 
\end{equation}
where the sum is now unrestricted. We can now insert \eqref{symmetriz} into  \eqref{dens} to find that:
\begin{align}
      \rho_\eta(\{\textbf{r}_i\},\{\textbf{r}'_i\})  = \sum_{\{\textbf{k}_i\}}\sum_{\sigma,\sigma' \in S_N}& \frac{1}{(N!)^2Z} \exp\bigg(-\sum_i \frac{\beta \hbar^2 k_i^2}{2m}\bigg)\eta^{\text{sgn}(\sigma)+\text{sgn}(\sigma')}\\
      &\braket{\textbf{r}_1',...,\textbf{r}_N'|P_{\sigma'}|\textbf{k}_1,...,\textbf{k}_N}\braket{\textbf{k}_1,...,\textbf{k}_N|P_{\sigma}|\textbf{r}_1,...,\textbf{r}_N} \notag
\end{align}
This daunting sum becomes an even more daunting integral as $L \rightarrow \infty$. We do it piece by piece. Firstly:
\begin{equation}
    \braket{\textbf{r}_1,...,\textbf{r}_N|P_\sigma|\textbf{k}_1,...,\textbf{k}_N} = \prod_i e^{i\textbf{k}_{\sigma(i)}\cdot \textbf{r}_i}
\end{equation}
so that:
\begin{align}
= \sum_{\sigma,\sigma' \in S_N}& \frac{1}{(N!)^2Z} \eta^{\text{sgn}(\sigma)+\text{sgn}(\sigma')}\int \prod_i \frac{d^3\textbf{k}_i}{(2\pi)^3}   \exp\bigg(-\frac{\beta \hbar^2 k_i^2}{2m}\bigg)e^{i\textbf{r}'_i\cdot\textbf{k}_{\sigma'(i)}}e^{-i\textbf{r}_i \cdot \textbf{k}_{\sigma(i)}}\\
= \sum_{\sigma,\sigma' \in S_N}& \frac{1}{(N!)^2Z} \eta^{\text{sgn}(\sigma)+\text{sgn}(\sigma')}\int \prod_i \frac{d^3\textbf{k}_i}{(2\pi)^3}   \exp\bigg(- \frac{\beta \hbar^2 k_i^2}{2m}\bigg)e^{i\textbf{k}_{i}\cdot(\textbf{r}_{\sigma^{-1}(i)}-\textbf{r}_{\sigma'^{-1}(i)}')}
\end{align}
where we rearrange the $e^{i\textbf{r}'_i\cdot \textbf{k}_{\sigma(i)}}$ terms so that the exponent contains $\textbf{k}_i$. Now we again have:
\begin{align}
    = \sum_{\sigma,\sigma' \in S_N}& \frac{1}{(N!)^2Z} \eta^{\text{sgn}(\sigma)+\text{sgn}(\sigma')} \prod_i \frac{1}{\lambda^3}\exp \bigg[-\frac{\pi(\textbf{r}_{\sigma^{-1}(i)}-\textbf{r}'_{\sigma^{-1}(i)})^2}{\lambda^2}\bigg]
\end{align}
Finally, note that permuting all of $\sigma(i)$ will give back $\sigma(i)$ in some order. Thus we can write the sum over two permutations as the sum over just one permutation multiplied by $|S_N|=N!$ since the sum will be repeated every time for a different $\sigma' \in S_N$:
\begin{equation}
     \boxed{\rho_\eta(\{\textbf{r}_i\},\{\textbf{r}'_i\})   = \frac{1}{Z_N \lambda^{3N} N!} \sum_{\sigma \in S_N} \eta^{\text{sgn}(\sigma)} \exp \bigg[-\sum_i\frac{\pi(\textbf{r}_{(i)}-\textbf{r}'_{\sigma(i)})^2}{\lambda^2}\bigg]}
\end{equation}
Imposing the normalization condition $\text{tr}(\rho) =\int d^{3N}\textbf{r} \braket{\textbf{r}|\rho|\textbf{r}}= 1$ we find that
\begin{equation}
    Z_N = \frac{1}{\lambda^{3N} N!} \int d^{3N}\textbf{r} \ \sum_{\sigma \in S_N} \eta^{\text{sgn}(\sigma)} \exp \bigg[-\sum_i\frac{\pi(\textbf{r}_{(i)}-\textbf{r}'_{\sigma(i)})^2}{\lambda^2}\bigg]
\end{equation}
so the partition function can be found by summing over all permutations of the $N\sim10^{23}$ particles in our system. The lowest order term comes from the identity permutation, which yields the classical result $Z = \frac{V^N}{\lambda^{3N}N!}$. Note that the Gibbs factor $\frac{1}{h^3N N!}$ is already included, it arose naturally from the quantumness of our system without the need of any artificial thought experiments.

The next lowest order term comes from the exchange of just two particles, say particle 1 and 2. There are $\frac{N(N-1)}{2}$ such permutations all yielding the same correction so we find that
\begin{align}
    Z_N &= \frac{1}{\lambda^{3N} N!} \int d^{3N}\textbf{r} \ \bigg(1+ \eta \frac{N(N-1)}{2} \exp \bigg[-\frac{2\pi(\textbf{r}_{(1)}-\textbf{r}'_{\sigma(2)})^2}{\lambda^2}\bigg]+...\bigg)\\
    &= \frac{V^N}{N! \lambda^{3N}}\bigg(1+\frac{N(N-1)}{2V}\eta \int d^3\textbf{r}_{12} e^{-2\pi r_{12}^2/\lambda^2}+...\bigg)
\end{align}
where in the last step we changed to the center of mass relative coordinates $(\textbf{R}_{12},\textbf{r}_{12}) = ((\textbf{r}_1+\textbf{r}_2)/2,\textbf{r}_1-\textbf{r}_2)$. The gaussian integral can be computed as usual and we find that
\begin{equation}
    Z_N = Z_{\text{classical}}\bigg[1+\frac{N(N-1)}{2V} \bigg(\frac{ \lambda^2}{2}\bigg)^{3/2}\eta+...\bigg]
\end{equation}
The free energy is then
\begin{equation}
    F=-k_B T \ln Z_N \approx F_{\text{classical}} + \frac{N^2}{2V}\frac{\lambda^3}{2^{3/2}} \eta+...
\end{equation}
where we taylor expanded $\ln(1+x)$ in the low density limit $\frac{N}{V}\ll1$. This finally allows us to compute the gas pressure as
\begin{equation}
    \frac{p}{k_BT} = -\frac{1}{k_BT}\frac{\partial F}{\partial V}\bigg|_T = \frac{N}{V}-\frac{N^2}{V^2}\frac{\lambda^3}{2^{5/2}} \eta+...
\end{equation}
It is interesting to note that this expansion is a virial expansion (see \eqref{virial}), and the quantum correction we have computed is the second virial coefficient
\begin{equation}
    B_2 = -\frac{\eta \lambda^3}{2^{5/2}}
\end{equation}
due to the classical potential
\begin{equation}
    U(r) =-k_B T\ln(1+\eta e^{-2\pi r^2/\lambda^2}) \approx -k_BTe^{-2\pi r^2/\lambda^2}, \ r \gg \lambda
\end{equation}
So we see that the effect of quantum statistics at high temperatures (where $\frac{\lambda}{r}$ is small) is that to add an exponential interaction between particles in a classical gas. This interaction is attractive for bosons and repulsive for fermions, just as one would expect!
\section{Boson and fermion gases in the GCE}
Consider a non-interacting quantum gas with single particle dispersion relation $\epsilon_\textbf{k}=\epsilon(\textbf{k})$. We have seen how to calculate partition function in the free particle case using the momentum basis. We can also use the occupation basis $\{n_\textbf{k}\}$ where:
\begin{equation}
    Z=\sum_{\{n_\textbf{k}\}} e^{-\beta \sum_k \epsilon(\textbf{k}) n_\textbf{k}}
\end{equation}
However, since $\sum_k n_k = N$ this sum is restricted. We can relax this condition by moving to the grand canonical ensemble, this will allow us to sum over each $n_k$ independently:
\begin{align}
    \mathcal{Z} &= \sum_N e^{\beta \mu N} \sum_{\{n_\textbf{k}\}_N} e^{-\beta \sum_k \epsilon(\textbf{k}) n_\textbf{k}}\\
    &=\sum_{\{n_\textbf{k}\}} \exp\bigg[-\beta \sum_\textbf{k}(\epsilon(\textbf{k})-\mu)n_\textbf{k}\bigg]\\
    &=\prod_\textbf{k}\sum_{\{n_\textbf{k}\}} \exp\big[-\beta(\epsilon(\textbf{k})-\mu)n_\textbf{k}\big]
\end{align}
where $\{n_\textbf{k}\}_N$ is the set of occupation numbers such that $\sum_k n_k = N$. Now for fermions $n_\textbf{k}=0,1$ so we find:
\begin{equation}
    \mathcal{Z}_- = \prod_\textbf{k} \big(1+\exp\big[-\beta(\epsilon(\textbf{k})-\mu)\big]\big)
\end{equation}
For bosons instead the occupation numbers are unrestricted $n_\textbf{k}=0,1,2,...$ so we will obtain a geometric sum which converges provided that $\epsilon(\textbf{k})>\mu$. This must be true for all $\textbf{k}$, so seeing as the ground state energy is $\epsilon_{GS}=0$ we get that if $\boxed{\mu<0}$ then:
\begin{equation}
    \mathcal{Z}_+ = \prod_\textbf{k} \frac{1}{1-\exp\big[-\beta(\epsilon(\textbf{k})-\mu)\big]}
\end{equation}
The interpretation of $\mu<0$ is that since bosons experience an attractive potential, it costs no energy to add a particle to the boson ensemble. The grand canonical potential is given by:
\begin{equation}
    \boxed{\Phi_G = \beta \eta \sum_\textbf{k} \ln\Big(1-\eta e^{-\beta(\epsilon(\textbf{k})-\mu)}\Big)}
\end{equation}
The average particle number is proportional to derivative of $\Phi_G$ with respect to $\mu$:
\begin{equation}
    \braket{N}_\eta = \frac{1}{\beta} \frac{\partial \Phi_G}{\partial \mu} =-\sum_{\textbf{k}}\frac{ e^{-\beta(\epsilon(\textbf{k})-\mu)}}{\eta e^{-\beta(\epsilon(\textbf{k})-\mu)}-1} = \sum_{\textbf{k}}\frac{1}{e^{\beta(\epsilon(\textbf{k})-\mu)}-\eta}
\end{equation}
The precise importance of $\mu$ and the differences between $\eta=1$ and $\eta=-1$ will be discussed in further detail in the next two chapters. 
\chapter{Bose-Einstein statistics}
\section{The density of states}
Consider an ideal gas inside a box of volume $V=L^3$ on which we pose periodic boundary conditions. For a free gas the particles will be in eigenstates:
\begin{equation}
    \braket{\textbf{r}|\textbf{k}} = \frac{1}{\sqrt{V}} e^{i\textbf{k}\cdot \textbf{r}}
\end{equation}
where the wave-vector $\textbf{k}=(k_x,k_y,k_z)$ satisfies the quantization condition:
\begin{equation}
    k_{x,y,z} = \frac{2\pi}{L}n_{x,y,z}, \ n_{x,y,z} \in \mathbb{Z}
\end{equation}
Each particle thus has energy:
\begin{equation}
    E_{\textbf{k}} = \frac{\hbar^2 \textbf{k}^2}{2m} = \frac{4\pi^2 \hbar^2}{2mL^2}(n_x^2+n_y^2+n_z^2) = \frac{4\pi^2 \hbar^2}{2mL^2}n^2
\end{equation}
We calculated the partition function:
\begin{equation}
    \mathcal{Z}=\sum_{\{n_i\}} e^{-\beta E_i}
\end{equation}
in the previous section for indistinguishable particles. The sum can also be done by approximating it as an integral. Indeed, note that each term in the above sum is an exponential of (up to some proportion factor) $\lambda^2n^2/L^2$. For a sufficiently large box where $\lambda \gg L$ there will be a macroscopic number of states such that $E\lessapprox k_B T$ which contribute to the sum. The sum can therefore be transformed into an integral:
\begin{equation}\label{dens sum}
    \sum_n \rightarrow \int d^3n = \frac{V}{(2\pi)^3} \int d^3\textbf{k} = \frac{2V}{(2\pi)^2} \int_0^\infty k^2 dk
\end{equation}
where $k=|\textbf{k}|$. We can use the dispersion relation
\begin{equation}
    dE = \frac{\hbar^2k}{m}dk \implies k^2 dk = \frac{m}{\hbar^2}\sqrt{\frac{2mE}{\hbar^2}} dE
\end{equation}
to write \eqref{dens sum} as an integral over energy:
\begin{equation}
     \frac{2V}{(2\pi)^2} \int_0^\infty k^2 dk=\int\frac{V}{4\pi^2}\bigg(\frac{2m}{\hbar^2}\bigg)^{3/2}\sqrt{E} dE 
\end{equation}
We define $g(E) dE$ as the number of states with energy in an interval $[E,E+dE]$ divided by $dE$. Then, we see that:
\begin{equation}
    g(E) = \frac{V}{4\pi^2}\bigg(\frac{2m}{\hbar^2}\bigg)^{3/2}\sqrt{E}
\end{equation}
This quantity $g(E)$ is known as the density of states. It has a particularly simple form for free gases. For a gas of massless particles, we should instead use the dispersion relation:
\begin{equation}\label{massless dens}
    E=\hbar c k \implies g(E) = \frac{V}{2\pi^2} \frac{E^2}{\hbar^3 c^3}
\end{equation}
\section{Photon gas - a statistical perspective}
One might naively think that the calculation in \eqref{massless dens} applies to photons as well. This is not true however, since photons can be polarized in two different states ($\textbf{E}$ parallel or $\textbf{E}$ perpendicular) so each $\textbf{k}$ state is two-fold degenerate. It follows that the density of states for photons is twice that for a normal massless relativistic particle:
\begin{equation}
    g(E)dE = \frac{V}{\pi^2} \frac{E^2}{\hbar^3 c^3}dE
\end{equation}
Introducing the frequency $\omega = E/\hbar$ we find that:
\begin{equation}
    \boxed{g(\omega) d\omega = \frac{V}{\pi^2}\frac{\omega^2}{\hbar c^3}d\omega}
\end{equation}
Note that $g(\omega)$ and $g(E)$ are not the same functions. One is the number of states per unit energy while the other is the number of states per unit frequency. 

Suppose we have a gas of photons in thermal equilibrium inside a blackbody cavity of volume $V$ at temperature $T$. Working in the canonical ensemble, the canonical partition for a gas of photons at frequency $\omega$ is:
\begin{equation}
    Z_\omega = \sum_N e^{-\beta \hbar \omega N} = \frac{1}{1-e^{-\beta \hbar \omega}} 
\end{equation}
Instead of summing over all frequencies, we can use the density of states to perform an integral:
\begin{equation}
    \ln Z = \int_0^\infty \ln (Z_\omega) g(\omega)  d\omega  = -\frac{V}{c^3 \pi^2} \int_0^\infty {\omega^2} \ln(1-e^{-\beta \hbar \omega})
\end{equation}
Therefore the Free energy of a photon gas reads
\begin{equation}\label{free energy light}
    F=-\frac{1}{\beta}\ln Z = \frac{V}{\beta c^3 \pi^2} \int_0^\infty \omega^2 \ln(1-e^{-\beta \hbar \omega}) d\omega
\end{equation}
while the internal energy is:
\begin{align}\label{energy light}
    U = -\frac{\partial}{\partial \beta} \ln Z &= \frac{V}{\pi^2 c^3}  \int_0^\infty \frac{\hbar \omega^3 e^{-\beta \hbar \omega}}{1-e^{-\beta \hbar \omega}}\\
    &=\frac{V \hbar}{\pi^2 c^3}\int_0^\infty  \frac{\omega^3}{e^{\beta \hbar \omega}-1}
\end{align}
The energy density carried by a photon with frequency in the interval $[\omega,\omega+d\omega]$ is known as the spectral energy density and follows the distribution:
\begin{equation}\label{planck law}
    \boxed{u(\omega) d\omega = \frac{\hbar}{\pi^2c^3}  \frac{\omega^3}{e^{\beta \hbar \omega}-1}d\omega}
\end{equation}
This result is known as Planck's law. The maxima of the energy distribution for a given frequency $\omega$ occurs when $\frac{dU}{d\omega} = 0$:
\begin{equation}
   3\omega^2(e^{\beta \hbar \omega}-1) - \omega^3 \beta \hbar e^{\beta \hbar \omega} = 0 \implies  \boxed{\omega_{max} \approx \frac{2.822}{\hbar \beta}}
\end{equation}
which is known as Wien's displacement law. 

Let's now evaluate the integral in \eqref{energy light} by considering the slightly different integral (which shall be used later):
\begin{align}
    g_n(z) &= \frac{1}{\Gamma(n)} \int_0^\infty \frac{x^{n-1}e^{-x}}{z^{-1}-e^{-x}}\\
    &=\frac{z}{\Gamma(n)} \int_0^\infty x^{n-1} e^{-x} \sum_{m=0}^\infty z^m e^{-mx} dx\\
    &=\frac{z}{\Gamma(n)}\sum_{m=0}^\infty z^m\int_0^\infty x^{n-1}  e^{-(m+1)x} dx\\
    &= \frac{1}{\Gamma(n)}\sum_{m=1}^\infty z^m\int_0^\infty  x^{n-1}  e^{-mx}dx\\
    &= \frac{1}{\Gamma(n)} \sum_{m=1}^\infty \frac{z^m}{m^n}\int_0^\infty u^{n-1} e^{-u} du 
\end{align}
Identifying the Gamma function $\Gamma(n) = \int_0^\infty u^{n-1} e^{-u} du $ we see that $g_n(z)$, known as Bose-Einstein functions, take the form:
\begin{equation}
    \boxed{g_n(z) =  \sum_{m=1}^\infty \frac{z^m}{m^n} \implies g_n(1) = \zeta(n)}
\end{equation}
Consequently:
\begin{equation}
    \int_0^\infty \frac{\omega^3}{e^{\beta \hbar \omega}-1}d\omega = \frac{1}{\beta^3 \hbar^3} \int_0^\infty \frac{x^3}{e^x-1} dx = \frac{1}{\beta^4 \hbar^4} \zeta(4) \Gamma(4)=\frac{1}{\beta^4 \hbar^4} \frac{\pi^4}{15}
\end{equation}
giving an energy density $u=\frac{U}{V}$ of:
\begin{equation}
   \boxed{u = \frac{\pi^2k_B^4}{15\hbar^3 c^3}T^4}
\end{equation}
The energy per unit area per unit time (radiant emittance) $j$ emitted by an object at temperature $T$ is then given by:
\begin{equation}
     \boxed{j = \frac{uc}{4}=\sigma T^4, \ \sigma = \frac{\pi^2k_B^4}{60\hbar^3 c^2}}
\end{equation}
To see where $\frac{c}{4}$ comes from, note that the radiant emittance is the integral of $\frac{U}{A t} = uc \cos \theta$ where $\theta$ is the angle of the emitted photon to the normal. Therefore:
\begin{equation}
    j = \int_0^2\pi \int_0^\pi uc \cos \theta d\theta d\phi = \frac{uc}{4}
\end{equation}
as found earlier. Also, the integral in \eqref{free energy light} can be calculated exactly to give an expression for the free energy:
\begin{align}
    F &= \frac{V}{\beta c^3 \pi^2} \int_0^\infty \omega^2 \ln(1-e^{-\beta \hbar \omega})=-\frac{V}{3\beta c^3 \pi^2} \int_0^\infty \frac{\beta \hbar \omega^3e^{-\beta \hbar \omega}}{1-e^{-\beta \hbar \omega}}\\
    &=-\frac{V\hbar}{3c^3 \pi^2}\frac{1}{(\beta \hbar)^4} \int_0^\infty\frac{x^3}{e^x-1} dx
    = -\frac{V \pi^2 k_B^4}{45 \hbar^3 c^3}T^4
\end{align}
or more succintly:
\begin{equation}
    \boxed{F=-\frac{4\sigma V}{3c}T^4}
\end{equation}
Therefore, the radiation pressure will satisfy the equation of state for a photon gas:
\begin{equation}
    p=-\frac{\partial F}{\partial V}\bigg|_T = \frac{4\sigma}{3c}T^4 \implies \boxed{3pc = 4\sigma T^4}
\end{equation}
\section{Photon gas - a kinetic perspective}
Some of the important results in the previous section could have been motivated using kinetic gas theory as well. We consider a gas of photons in thermal equilibrium within a blackbody cavity of volume $V$ and temperature $T$. For a normal gas with $N$ particles moving randomly, the number of particles per unit volume moving out within a solid angle $d\Omega$ with speed in the interval $[v,v+dv]$ would be given by:
\begin{equation}
     \frac{d\Omega}{4\pi}n(v) dv = \frac{1}{4\pi}n(v)\sin \theta dv d\theta d\phi
\end{equation}
For photons however $n(v) = \delta(v-c)$ so integrating over all possible speeds and over $\phi \in [0,2\pi]$ we find that:
\begin{equation}
    \frac{1}{2}n\sin \theta d\theta, \ n=\frac{N}{V}
\end{equation}
particles move in the direction $[\theta,\theta+d\theta]$ per unit volume. For a photon in this cavity to hit a wall (oriented so that its normal is along $z$ for simplicity) in a time $\Delta t$ it must be within a distance $c \cos \theta dt$. Consequently the volume the gas particles must be inside of to collide with the wall is $Ac \cos \theta dt$. The number of gas particles within this volume is:
\begin{equation}
    \frac{1}{2}nAc \cos \theta \sin \theta d\theta = \frac{1}{4}nAc \sin(2\theta) d\theta dt
\end{equation}
the number of particles hitting the wall per unit area per unit time, or the incident particle flux per unit time is then:
\begin{equation}
    \Phi = \int_0^{\frac{\pi}{2}}\frac{1}{4}nc\sin(2\theta) d\theta = \frac{1}{4}nc
\end{equation}
where we integrated over $\theta \in [0,\pi/2]$ since particles with $\theta \in [\pi/2,\pi]$ are moving away from the wall.

Each particle has momentum $\frac{E}{c}$ so the momentum density is given by $\frac{u}{c}$. The momentum perpendicular to the wall carried by the gas particles incident on the wall per unit area per unit time is:
\begin{equation}
   \int_0^{\frac{\pi}{2}}\frac{1}{4}\frac{u}{c}\cos \theta c\sin(2\theta) d\theta = \frac{1}{6}u
\end{equation}
The pressure on the wall is twice this since the particles are also reflected back:
\begin{equation}
    p = \frac{1}{3}u
\end{equation}
as found earlier. Also, considering that each photon has energy $\hbar \omega$, the radiant emittance (energy per unit time per unit area incident) on the wall is:
\begin{equation}
    j = \hbar \omega \Phi = \frac{1}{4}n \hbar \omega c
\end{equation}
We may define $u=\braket{n \hbar \omega}$ as the energy density to write:
\begin{equation}
    j = \frac{1}{4}uc
\end{equation}
Finally, using the first law:
\begin{align}
    u = \frac{\partial U}{\partial V}\bigg|_T = T \frac{\partial S}{\partial V}\bigg|_T-p=T\frac{\partial p}{\partial T}\bigg|_V-p=\frac{1}{3}T\frac{\partial u}{\partial T}\bigg|_V-\frac{u}{3}
\end{align}
which gives after some simplification:
\begin{equation}
    4u=T\frac{\partial u}{\partial T}\bigg|_V
\end{equation}
This PDE has a solution $u=A T^4$ for some constant $A$, yielding:
\begin{equation}
    j = \frac{1}{4}Ac T^4
\end{equation}
which is the Stefan-Boltzmann law. It is important to note that these calculations do not allow us to calculate the pressure and radiant emittance since we are missing $u$, the energy density. Only a quantum statistical calculation would have allowed us to calculate this quantity.
\section{Blackbodies and Kirchoff's law}
In this section we try to understand how to quantify the aptitude of a surface in absorbing and emitting electromagnetic radiation at particular frequencies and temperatures. With this in mind, we define the spectral absorptivity $\alpha(\omega)$ as the fraction of the incident radiation absorbed at frequency $\omega$ and the spectral emissive power $j^*(\omega)$ for a surface so that $j^*(\omega) d\omega$ gives the power emitted per unit area with frequency in $[\omega,\omega+d\omega]$. In thermal equilibrium the radiation emitted by a surface must be equal to the radiation absorbed. Using our previous definitions we find that:
\begin{align}
    \text{emitted radiation} &= j^*(\omega) d\omega \\
    \text{absorbed radiation} &= \alpha(\omega) \frac{1}{4}cu(\omega)d\omega
\end{align}
so:
\begin{equation}
    j^*(\omega) d\omega = \alpha(\omega) \frac{1}{4}cu(\omega)d\omega \implies \boxed{\frac{j^*(\omega)}{\alpha(\omega)} = \frac{1}{4}cu(\omega) = j(\omega)>0}
\end{equation}
This result is known as Kirchoff's law. If we fix $\omega$ then the RHS will be a constant, and thus $\alpha \propto j^*$, if a body is a good absorber then it will also be a good emitter. A perfect absorber of radiation, known as a blackbody, will have $\alpha(\omega)=1$ for all frequences $\omega$. Thus Kirchoff's law implies that the spectral emissive power $j^*(\omega)$ will be equal to its maximum possible value $j$, the radiant emittance (all of the absorbed radiation is emitted) which we calculated earlier. If we thus place a gas of photons inside a cavity whose walls are blackbodies, then through perfect absorption and emission of photons the radiation contained within such a black-body is known as black-body radiation. This radiation follows the Planck distribution in \eqref{planck law} no matter what the absorbed radiation looked like. All that determines the distribution the radiation emitted by a blackbody is the temperature at which it is in equilibrium. It follows that the process of blackbody radiation is not reversible in general.
\section{Phonons}
\section{High temperature Bose gases}
\section{Bose-Einstein condensation}
After having looked at two important examples of Bose-Einstein statistics, let's now give a complete statistical description of the ideal Bose gas at low temperatures. We assume the thermal fluctuations are larger than the energy gaps in the gas, so that we may write the average number of particles as (we omit the $\braket{}$ unless otherwise stated):
\begin{equation}
    N =\int  \frac{g(E)}{e^{\beta(E-\mu)}-1} dE = \frac{V}{4\pi^2}\bigg(\frac{2m}{\hbar^2}\bigg)^{3/2}\int \frac{E^{1/2}}{e^{\beta(E-\mu)}-1} dE 
\end{equation}
Traditionally, we let $z=e^{\beta \mu}$ be the fugacity. Substituting $x=\beta E$ we find that:
\begin{align}
    N &= \frac{V}{4\pi^2}\bigg(\frac{2m}{\hbar^2}\bigg)^{3/2}\frac{1}{\beta^{3/2}}\int_0^\infty \frac{x^{1/2}}{z^{-1}e^x-1} dE\\
    &=\frac{V}{4\pi^2}\bigg(\frac{2mk_B T}{\hbar^2}\bigg)^{3/2} \Gamma(3/2)\sum_{m=1}^\infty \frac{z^m}{m^{3/2}}\\
    \implies & \boxed{n = \frac{1}{\lambda_{th}^3} \sum_{m=1}^\infty \frac{z^m}{m^{3/2}} \equiv \frac{g_{3/2}(z)}{\lambda_{th}^3}} \label{particle density}
\end{align}
where $\Gamma(3/2) = \frac{\sqrt{\pi}}{2}$ and $n=\frac{N}{V}$ is the particle density. Suppose we slowly reduce the temperature in \eqref{particle density} keeping $n$ constant. Since $\mu<0$ the fugacity $z=e^{\beta \mu}$ will also increase \footnote{indeed note that the $z\ll 1$ limit is a high temperature limit. To see why, note that as we decrease $T \rightarrow 0$ then we do have that $\beta \rightarrow \infty$, but the chemical potential $\mu(T,N)$ will also change if we keep $N$ fixed. It turns out that $\mu \rightarrow-\infty$ more rapidly than $\beta \rightarrow 0$, since to leading order we need $\frac{N}{V}\sim \frac{z}{\lambda^3}$ to be constant, and thus $z \sim T^{-3/2}$.}, but it cannot be larger than 1 since $z\in[0,1]$ for negative chemical potentials. The critical temperature $T_c$ at which $z=1$ can be found from \eqref{particle density}:
\begin{equation}
    \frac{\zeta(3/2)}{\lambda_{th}^3} = n \implies \boxed{T_c = \frac{2\pi \hbar^2}{mk_B} \bigg(\frac{n}{\zeta(3/2)}\bigg)^{2/3}}
\end{equation}
What would happen if we decreased the temperature further? Looking at the plot of $g_{3/2}(z)$:
\begin{figure}[h!]
    \centering
    \includegraphics[width=9cm]{polylogarithm.png}
    \label{fig:my_label}
\end{figure}

it seems like $g_{3/2}(z)$ and thus $\frac{N}{V}$ is bounded above by $\zeta(3/2)\approx 2.613$ and $\frac{\zeta(3/2)}{\lambda_{th}^3}$ respectively. So as we continue to decrease the temperature, while $z$ remains fixed at $1$ and so $g_{3/2}(z)=\zeta(3/2)$, $\lambda_{th}$ will instead decrease. This is not allowed, since the particle number is fixed, so it seems like it is impossible to cool our system below $T_c$ \footnote{even if $\mu$, and thus $z$ were to change below $T_c$, there is no way $g_{3/2}(z)$ can increase to compensate the increase in $\lambda_{th}^3$ since it is bounded above by $\zeta(3/2)$}  

Another way to look at this paradox is by keeping the temperature fixed and increasing the particle density. Again we see that increasing $n$ leads to an compensating increase in $z$ and thus $\mu$. There comes a point however where $\mu=0 \iff z=1$, which yields the following critical density
\begin{equation}
    n_c = \bigg(\frac{mk_BT}{2\pi \hbar^2}\bigg)^{3/2} \zeta(3/2)
\end{equation}
We cannot further increase $n$ since this would make $\mu>0$, but this makes no sense!

It turns out that at such low temperatures (or high densities) we have a violation of our initial assumption that a sum over microstates may be approximated by an integral. Indeed the integral in \eqref{dens} does not receive a contribution by the ground state $E_{GS}=0$. Using the Bose-Einstein distribution we see that the number of particles in the ground state is:
\begin{equation}
    N_0 = \frac{1}{z^{-1}-1} \implies \boxed{N = \frac{Vg_{3/2}(z)}{\lambda_{th}^3} + \frac{z}{1-z}}
\end{equation}
This resolves our paradox, since as $z$ approaches 1 we see that more and more particles occupy the ground state until $N_0$ becomes macroscopic when $z=1$. If we add more particles they will just go to the ground state so the number of excited particles $N_e=\frac{Vg_{3/2}(z)}{\lambda_{th}^3}$ will remain constant, making $z$ level off at $z=1$:
\begin{equation}
    z = \begin{cases}
     1, \ &\frac{V}{\lambda_{th}^3}\zeta(3/2)\leq N\\
     g_{3/2}^{-1}\big(\frac{N\lambda_{th}^3}{V}\big), \ &\frac{V}{\lambda_{th}^3}\zeta(3/2)> N
    \end{cases}
\end{equation}

Now as we decrease the temperature below the critical temperature, we have that the ratio of particles occupying the ground state will be:
\begin{equation}
   \frac{N_0}{N}=
    \begin{cases}
     1-\big(\frac{T}{T_c}\big)^{3/2}, \ &T<T_c\\
     0, \ &T>T_c
    \end{cases}
\end{equation}
This new state of matter where a macroscopic number of particles occupy the ground state is known as a Bose-Einstein condensate.
\begin{figure}[h!]
\centering
\begin{subfigure}{0.4\textwidth}
  \centering
  \includegraphics[width=\textwidth]{fugacity.png}
  \label{fig:sub1}
\end{subfigure}%
\begin{subfigure}{0.5\textwidth}
  \centering
  \includegraphics[width=\textwidth]{condensate n.png}
  \label{fig:sub1}
\end{subfigure}%
\label{fig:test}
\end{figure}

Let's also look at the pressure:
\begin{equation}
    pV = -\Phi_G = -\frac{1}{\beta} \ln \mathcal{Z}
\end{equation}
This time, we take into account the contribution due to the ground state by explicitly adding $\ln(1-z)$ (note that we can do this since $\mathcal{Z}$ is multiplicative and therefore $\ln(\mathcal{Z}) = \ln (\mathcal{Z}{GS}) + \ln (\mathcal{Z}_{e})$):
\begin{align}
    \ln \mathcal{Z} &= -\frac{V}{4\pi^2}\bigg(\frac{2m}{\hbar^2}\bigg)^{3/2}\int_0^\infty E^{1/2} \ln(1-ze^{-\beta E})dE-\ln(1-z)\\
    &=\frac{V}{4\pi^2}\bigg(\frac{2m}{\hbar^2}\bigg)^{3/2}\frac{2}{3}\beta\int_0^\infty \frac{E^{3/2}}{z^{-1}e^{\beta E}-1} dE-\ln(1-z)\\
     &=\frac{V}{4\pi^2}\bigg(\frac{2m}{\hbar^2}\bigg)^{3/2}\frac{2}{3}\frac{1}{\beta^{3/2}}\int_0^\infty \frac{x^{3/2}}{z^{-1}e^{x}-1} dx-\ln(1-z)\\
    &= \frac{V}{\lambda_{th}^3} g_{5/2}(z)=\frac{V}{\lambda_{th}^3} \sum_{m=1}^\infty \frac{z^m}{m^{5/2}}-\ln(1-z)
\end{align}
Thus
\begin{equation}
    p = \frac{k_B T}{\lambda_{th}^3}g_{5/2}(z) - \frac{\ln(1-z)}{V}
\end{equation}
Now note that if $T>T_c$ then $\ln(1-z)$ is finite and thus the second term is negligible. If instead $T<T_c$ then $z = \frac{1}{N_{gs}+1}$ so we get $\ln(N_{gs})/V$. The numerator diverges more slowly than the denominator so the second term will again vanish. Consequently we find that
\begin{equation}
    p= \frac{k_B T}{\lambda_{th}^3}g_{5/2}(z) 
\end{equation}
regardless of whether we are below or above the critical temperature. The fact that the ground state gives no pressure contribution makes sense since ground state particles have no net momentum ($E_{gs}=0$) and thus cannot possibly contribute to the pressure. Furthermore, in the condensate regime $T<T_c$ we have that $z=1$ and thus:
\begin{equation}
    p=\frac{k_B T}{\lambda_{th}^3}\zeta(5/2)
\end{equation}
Note that this value is independent of the number of particles. This makes sense, since in a BEC any particles that are added will be in the ground state, and such particles will not contribute to the total pressure.

Similarly, the internal energy reads (again the ground state gives a vanishing contribution to the internal energy in the thermodynamic limit):
\begin{align}
   U &= -\frac{\partial}{\partial \beta} \ln Z\bigg|_z= \frac{V}{4\pi^2}\bigg(\frac{2m}{\hbar^2}\bigg)^{3/2}\int_0^\infty E^{1/2}\frac{\partial}{\partial \beta} \ln(1-ze^{-\beta E})dE\\
    &=\frac{V}{4\pi^2}\bigg(\frac{2m}{\hbar^2}\bigg)^{3/2}\int_0^\infty \frac{E^{3/2}}{z^{-1}e^{\beta E}-1} dE\\
     &=\frac{V}{4\pi^2}\bigg(\frac{2m}{\hbar^2}\bigg)^{3/2}\frac{1}{\beta^{5/2}}\Gamma(5/2)\int_0^\infty \frac{x^{3/2}}{z^{-1}e^{x}-1} dx= \frac{3}{2}\frac{Vk_B T}{\lambda_{th}^3} g_{5/2}(z) = \frac{3}{2}pV
\end{align}
The heat capacity is then given by:
\begin{align}\label{heat capacity}
    C_V &= \frac{\partial U}{\partial T}\bigg|_V = \frac{15}{4}\frac{k_B V}{\lambda_{th}^3}g_{5/2}(z)+\frac{3}{2}\frac{Vk_BT}{\lambda_{th}^3}\frac{\partial g_{5/2}(z)}{\partial T}
\end{align}
For $T<T_c$ we have that $z=1$ is constant, and thus the second term will vanish:
\begin{equation}
    \boxed{c_V = \frac{15}{4}\frac{k_B}{\lambda_{th}^3}\zeta(5/2), \ T<T_c}
\end{equation}
Importantly, at the critical temperature:
\begin{equation}\label{specific heat cond}
    C_V = \frac{15}{4}\frac{\zeta(5/2)}{\zeta(3/2)}Nk_B, \ T=T_c
\end{equation}
For $T>T_c$ the second term in \eqref{heat capacity} becomes relevant. Since $\frac{V}{\lambda_{th}^3}=\frac{N}{g_{3/2}(z)}$ we have that:
\begin{align}
    U &= \frac{3}{2}Nk_B T\frac{g_{5/2}(z)}{g_{3/2}(z)}\\
    \implies C_V &= \frac{3}{2}Nk_B\frac{g_{5/2}(z)}{g_{3/2}(z)}+\frac{3}{2}Nk_B T \frac{\partial}{\partial T}\bigg(\frac{g_{5/2}(z)}{g_{3/2}(z)}\bigg)\\
    &=\frac{3}{2}Nk_B\frac{g_{5/2}(z)}{g_{3/2}(z)}+\frac{3}{2}Nk_B T \frac{1}{z}\bigg(1-\frac{g_{5/2}(z)g_{1/2}(z)}{g^2_{3/2}(z)}\bigg)\frac{\partial z}{\partial T}
\end{align}
where we used the fact that $\frac{\partial}{\partial z}(g_n(z)) = \frac{1}{z}g_{n-1}(z)$. We can write $\frac{\partial z}{\partial T}$ in a useful form by differentiating \eqref{particle density} which applies for $T>T_c$ only:
\begin{equation}
    \frac{1}{z}g_{1/2}(z)\frac{\partial z}{\partial T} = -\frac{3}{2}\frac{N}{V}\frac{\lambda_{th}^3}{T}
    \implies \frac{\partial z}{\partial T}  = - \frac{3z}{2T}\frac{g_{3/2}(z)}{g_{1/2}(z)}
\end{equation}
This allows us to write:
\begin{equation}\label{specific heat high}
    \boxed{C_V = \frac{3Nk_B}{4}\bigg(5\frac{g_{5/2}(z)}{g_{3/2}(z)}-3\frac{g_{3/2}(z)}{g_{1/2}(z)}\bigg), \ T>T_c}
\end{equation}
Firstly, note that as $T \rightarrow \infty$ then $z \rightarrow 0$ and since $\lim_{x \rightarrow 0}\frac{g_{m}(x)}{g_n(x)} = 1$ for all positive integers $m,n$ we have that $C_V \rightarrow \frac{3}{2}Nk_B$, which is the correct value for a classic ideal gas. A high temperature bose gas behaves as a high temperature classical gas. Secondly, note that as $T \rightarrow T_c$ from above, $z \rightarrow 1$ ans since $\zeta(1/2)$ diverges the second term vanishes, giving $C_V \rightarrow \frac{15Nk_B}{4}\frac{\zeta(5/2)}{\zeta(3/2)}$ which is the same value as in the condensate phase. We provide a plot for $C_V$ beside (taken from Greiner).
\begin{wrapfigure}{r}{0.4\textwidth}
  \begin{center}
    \includegraphics[width=0.4\textwidth]{specific heat capacity.png}
  \end{center}
\end{wrapfigure}
We see that in the Bose condensation regime $T<T_c$ the specific heat capacity grows as $T^{3/2}$. Instead for $T>T_c$ it decays into the classical ideal gas heat capacity. At $T=T_c$ we see a kink, and while a $C_V$ is continuous its first derivative is not. Since the second derivative of the internal energy is discontinuous, we call this a second order phase transition. In general second order phase transitions occur when the second derivative of a thermodynamic potential suffers from a discontinuity.

It may appear like the only reason we obtained a discontinuity is that we ignored the $\frac{\partial z}{\partial T}$ term in the $T<T_c$ regime:
\begin{equation}
    z=\bigg(1+\frac{1}{N_0}\bigg)^{-1} = \bigg[1+\frac{1}{N}\bigg(1-\frac{T}{T_c}\bigg)^{-3/2}\bigg]^{-1}\implies \frac{\partial z}{\partial T} \propto N^{-1}
\end{equation}
For finite $N$ this is a nonzero contribution, and indeed the kink would be smooth at some scale. There would be a smooth transition between the two temperature regimes. However, note that as $N \rightarrow \infty$ this contribution vanishes, and we do effectively get a discontinuity. This is true for any phase transition, as we shall soon see.
\chapter{Fermi-Dirac statistics}
\section{The Ideal Fermi gas}
Consider an ideal gas of free, non-interacting fermions with spin $s$. The number of states will have a degeneracy factor of $g_s = 2s+1$ (e.g. for spin half each state can be occupied by a spin up and a spin down fermion, giving two-fold degeneracy). The density of states then reads:
\begin{equation}
    g(E) = \frac{g_sV}{4\pi^2} \bigg(\frac{2m}{\hbar^2}\bigg)^{3/2} E^{1/2}
\end{equation}
Recall that according to Fermi-Dirac statistics the average number of fermions is given by:
\begin{equation}
    N = \sum_\textbf{k} \frac{1}{e^{\beta(\epsilon_\textbf{k}-\mu)}+1}
\end{equation}
In the high temperature limit the thermal fluctuations are much larger than the energy gap in our gas, so we may approximate this sum by an integral:
\begin{equation}
    N = \int \frac{g(E) dE}{z^{-1}e^{\beta \epsilon_{\textbf{k}}}+1}
\end{equation}
Similarly for the internal energy:
\begin{equation}
    U =\sum_\textbf{k} \frac{\epsilon_{\textbf{k}}}{e{\beta(\epsilon_\textbf{k}-\mu)}+1} =\int \frac{g(E) E dE}{z^{-1}e^{\beta \epsilon_{\textbf{k}}}+1}
\end{equation}
and the grand canonical potential:
\begin{equation}
    pV = -\Phi_G = \frac{1}{\beta} \sum_\textbf{k} \ln(1+e^{\beta(\epsilon_{\textbf{k}}-\mu)}) = \int g(E) \ln(1+ze^{-\beta E}) dE
\end{equation}
We again encounter a family of integrals $\int_0^\infty \frac{x^{n-1}}{z^{-1}e^x+1} dx$ which are slightly different to $g_n(z)$. They can however be calculated in exactly the same way:
\begin{align}
    f_n(z) &= \frac{1}{\Gamma(n)} \int_0^\infty \frac{x^{n-1}e^{-x}}{z^{-1}+e^{-x}}=\frac{1}{\Gamma(n)}\int_0^\infty ze^{-x}x^{n-1} \sum_{m=0}^\infty (-ze^{-x})^m\\
    &=-\frac{1}{\Gamma(n)}\sum_{m=0}^\infty (-z)^{m+1}\int_0^\infty x^{n-1}e^{-(m+1)x}dx\\
    &=-\frac{1}{\Gamma(n)}\sum_{m=1}^\infty (-z)^{m}\int_0^\infty x^{n-1}e^{-mx}dx\\
    &=-\frac{1}{\Gamma(n)}\sum_{m=1}^\infty \frac{(-z)^{m}}{m^n}\int_0^\infty s^{n-1}e^{-s}ds =- \sum_{m=1}^\infty \frac{(-z)^{m}}{m^n} = -g_n(-z)
\end{align}
Consequently the average number of bosons can be expressed as:
\begin{align}
   N &=\frac{g_s V}{4\pi^2} \bigg(\frac{2m}{\hbar^2}\bigg)^{3/2}\int_0^\infty \frac{E^{1/2}}{z^{-1}e^{\beta E}+1} dE\\
    &=\frac{g_s V}{4\pi^2} \bigg(\frac{2m}{\hbar^2 \beta}\bigg)^{3/2}\int_0^\infty \frac{x^{1/2}}{z^{-1}e^{x}+1} dx\\
    &=\frac{g_s V}{4\pi^2} \bigg(\frac{2mk_B T}{\hbar^2}\bigg)^{3/2} \Gamma(3/2) f_{3/2}(z)\\
    &= \frac{g_sV}{\lambda_{th}^3} f_{3/2}(z)
\end{align}
In the limit where $z\ll1$ this can be expanded to second order as:
\begin{align}\label{N approx}
    N\approx \frac{g_s V}{\lambda_{th}^3}z\bigg(1-\frac{z}{2\sqrt{2}}\bigg)
\end{align}
We see that $z\ll1$ is only satisfied if $\lambda_{th}^3\ll\frac{V}{N}$, so if the thermal wavelength is much smaller than the particle separation. Recalling that the thermal wavelength represents the average de Broglie wavelength of the fermions, this means that we are working in the quasi-classical, high temperature limit. To understand why $z\ll 1$ corresponds to this high temperature limit, note that at fixed particle density $\frac{N}{V}$ we need $\frac{z}{\lambda_{th}^3}$ to be roughly constant, implying that the fugacity scales as $T^{-3/2}$. Indeed one can solve \eqref{N approx} to get:
\begin{align}\label{invert}
    z &\approx \sqrt{2}\bigg(1\pm \sqrt{1-\sqrt{2}\frac{\lambda_{th}^3N}{g_sV}}\bigg)\\
    &\approx \sqrt{2}\bigg[1\pm \bigg(1-\sqrt{2}\frac{\lambda_{th}^3N}{g_sV}-\frac{1}{8}\bigg(\frac{\lambda_{th}^3N}{g_sV}\bigg)^2\bigg)\bigg]\\
    &\approx \frac{\lambda_{th}^3 N}{g_sV}\bigg(1+\frac{1}{2\sqrt{2}}\frac{\lambda_{th}^3 N}{g_sV}\bigg)
\end{align}
where the $+$ solution is discarded since $z\ll1$.
One can similarly find that for the internal energy:
\begin{equation}
    U = \frac{3g_sV}{2\beta \lambda_{th}^3} f_{5/2}(z) \approx \frac{3g_sV}{2\beta \lambda_{th}^3}z\bigg(1-\frac{z}{4\sqrt{2}}\bigg)
\end{equation}
Inserting \eqref{invert} into the above and simplifying, keeping only terms up to second order in $\frac{\lambda_{th}^3N}{V}$:
\begin{align}
    U &\approx \frac{3g_sV}{2\beta \lambda_{th}^3}\frac{\lambda_{th}^3 N}{g_sV}\bigg(1+\frac{1}{2\sqrt{2}}\frac{\lambda_{th}^3 N}{g_sV}\bigg)\bigg(1-\frac{1}{4\sqrt{2}}\frac{\lambda_{th}^3 N}{g_sV}\bigg)\\
    &\approx \frac{3N}{2\beta}\bigg(1+\frac{1}{4\sqrt{2}}\frac{\lambda_{th}^3 N}{g_sV}\bigg)
\end{align}
For $pV$, it is convenient to integrate by parts. Letting $G(E) = \int g(E)dE = \frac{2}{3}g(E)E$:
\begin{align}
    pV &= \frac{1}{\beta}\bigg[G(E) \ln(1+ze^{-\beta E})\bigg]_0^\infty +\int_0^\infty \frac{G(E)}{z^{-1}e^{\beta E}+1} dE\\
    &=  \int_0^\infty \frac{2}{3}\frac{g(E) E}{z^{-1}e^{\beta E}+1} dE = \frac{2}{3}U
\end{align}
Using our first order approximation for $U$ we get the equation of state:
\begin{equation}
    \boxed{pV \approx Nk_B T\bigg(1+\frac{1}{4\sqrt{2}}\frac{\lambda_{th}^3 N}{g_sV}\bigg)}
\end{equation}
\section{Zero temperature behaviour - Fermi energy}
Let's now look at the extremely low temperature limit $T \rightarrow 0$. It is easy to see that:
\begin{equation}
   N_\textbf{k} = \begin{cases}
     1, \ \text{ if } \epsilon_{\textbf{k}}<\mu\\
     0, \ \text{ if } \epsilon_{\textbf{k}}>\mu
    \end{cases}
\end{equation}
This intuitively makes sense. At absolute zero the lowest energy states are favoured, so particles will occupy the energy levels from the ground state up. 
\begin{figure}[h!]
    \centering
    \includegraphics[width=12cm]{fermi dirac.png}
    \label{fig:my_label}
\end{figure}

The highest occupied energy level $E_F$ is known as the \textbf{Fermi energy}, and must be equal to the zero temperature chemical potential:
\begin{equation}
    E_F = \mu(T=0)
\end{equation}
Note that in the GCE the chemical potential and temperature are independent natural variables. However, since we are keeping $N$ constant, if we vary $T$ we must also vary $\mu$ giving this temperature dependence of the chemical potential.

For a ideal fermion gas, we can look at the momentum space $(k_x,k_y,k_z)$. Due to the isotropicity of $\epsilon_\textbf{k} = \frac{\hbar^2 \textbf{k}^2}{2m}$, when we add particles to the gas they will fill up forming a sphere, a \textbf{Fermi sphere} or \textbf{Fermi sea}. The radius of that sphere gives the \textbf{Fermi momentum} $k_F$ which is related to the Fermi energy by $k_F^2 = \frac{2mE_F}{\hbar^2}$. The states with momentum $k_F$ form the \textbf{Fermi surface}.

Let's calculate $E_F$. Suppose we have $\braket{N}=N$ bosons. Then we need:
\begin{align}
    N &= \int_0^{E_F} g(E) dE = \frac{g_sV}{4\pi^2}\bigg(\frac{2m}{\hbar^2}\bigg)^{3/2}\int_0^{E_F} E^{1/2} dE\\
    &=\frac{g_sV}{6\pi^2}\bigg(\frac{2m}{\hbar^2}\bigg)^{3/2}E_F^{3/2}\\
    \implies \Aboxed{E_F &= \frac{\hbar^2}{2m}\bigg(\frac{6\pi^2 }{g_s}\frac{N}{V}\bigg)^{2/3}} \label{fermi energy}
\end{align}
giving the Fermi momentum:
\begin{equation}
    k_F = \bigg(\frac{6\pi^2 }{g_s}\frac{N}{V}\bigg)^{1/3}
\end{equation}
Also:
\begin{equation}
    U = \int_0^{E_F} g(E) E dE = \frac{g_sV}{10\pi^2}\bigg(\frac{2m}{\hbar^2}\bigg)^{3/2}E_F^{5/2} = \frac{3}{5}N E_F
\end{equation}
\begin{equation}
    pV = \frac{2}{3}U = \frac{2}{5} N E_F
\end{equation}
so at zero temperature the pressure does not vanish. This is a direct result of the Pauli exclusion principle \enquote{repelling} electrons and thus producing an outwards degeneracy pressure. 
\section{The Sommerfield approximation}
We have looked at the Fermi-Dirac distribution in the high temperature and absolute zero temperature limits. We are now ready to study the low-temperature limit, and we do so in the Sommerfield approximation. 

The first step is to split Fermi-Dirac functions $f_n(z)$ into two integrals:
\begin{align}
    \Gamma(n) f_n(z) &= \int_0^{\beta \mu} x^{n-1}\bigg(1-\frac{1}{1+ze^{-x}}\bigg)dx + \int_{\beta \mu}^\infty \frac{x^{n-1}}{z^{-1}e^x+1}dx \\
    &= \frac{(\beta \mu)^n}{n}-\int_0^{\beta \mu} \frac{x^{n-1}}{1+ze^{-x}}dx + \int_{\beta \mu}^\infty \frac{x^{n-1}}{z^{-1}e^x+1}dx
\end{align}
In the two integrals we have two exponentials, $e^{-x+\beta \mu}$ and $e^{x-\beta \mu}$ so it makes sense to use the variables $\eta_1 = -x+\beta\mu$ and $\eta_2 = x-\beta \mu$:
\begin{equation}
    \Gamma(n) f_n(z)=\frac{(\beta \mu)^n}{n}-\int_0^{\beta \mu} \frac{(\beta \mu -\eta_1)^{n-1}}{1+e^{\eta_1}}d\eta_1 + \int_{0}^\infty \frac{(\beta \mu +\eta_2)^{n-1}}{e^{\eta_2}+1}d\eta_2
\end{equation}
Let's look at the first integral, and expand it as:
\begin{equation}
   \int_0^{\beta \mu} \frac{(\beta \mu -\eta_1)^{n-1}}{1+e^{\eta_1}}d\eta_1 = \int_0^{\infty} \frac{(\beta \mu -\eta_1)^{n-1}}{1+e^{\eta_1}}d\eta_1 - \int_{\beta \mu}^\infty \frac{(\beta \mu -\eta_1)^{n-1}}{1+e^{\eta_1}}d\eta_1
\end{equation}
In the low temperature limit the integral from $0$ to $\infty$ is small (of order $z^{-1}$) so it can be safely neglected. Therefore, we get that:
\begin{equation}
     \Gamma(n) f_n(z) = \frac{(\beta \mu)^n}{n}+ \int_{0}^\infty \frac{(\beta \mu +\eta)^{n-1}-(\beta \mu - \eta)^{n-1}}{e^{\eta}+1}d\eta
\end{equation}
The numerator can be Taylor expanded in $\frac{\eta}{\beta \mu}$:
\begin{align}
    (\beta \mu +\eta_2)^{n-1}-(\beta \mu - \eta_1)^{n-1} &= (\beta \mu)^{n-1}\bigg[\bigg(1+\frac{\eta}{\beta \mu}\bigg)^{n-1} - \bigg(1-\frac{\eta}{\beta \mu}\bigg)^{n-1}\bigg]\\
    &\approx (\beta \mu)^{n-1}\bigg[\bigg(1+(n-1)\frac{\eta}{\beta \mu}\bigg) - \bigg(1-(n-1)\frac{\eta}{\beta \mu}\bigg)\bigg]\\
    &= 2(n-1)(\beta \mu)^{n-2} \eta 
\end{align}
so that:
\begin{equation}
     \Gamma(n) f_n(z) = \frac{(\beta \mu)^n}{n}+ 2(n-1)(\beta \mu)^{n-2}\int_{0}^\infty \frac{\eta}{e^{\eta}+1}d\eta
\end{equation}
We recognize the Fermi-Dirac function:
\begin{equation}
    \int_{0}^\infty \frac{\eta}{e^{\eta}+1}d\eta =f_2(1) = \frac{\pi^2}{12}
\end{equation}
so we finally find that ($\ln z = \beta \mu$):
\begin{equation}
      \Gamma(n) f_n(z) = \frac{(\ln z)^n}{n}+ \frac{\pi^2}{6}(n-1)(\ln z)^{n-2}
\end{equation}
or in another words:
\begin{equation}
    \boxed{f_n(z) \approx \frac{(\ln z)^n}{\Gamma(n+1)}\bigg(1+\frac{\pi^2}{6}\frac{n(n-1)}{(\ln z)^2}\bigg), \ z\ll 1}
\end{equation}
This allows us to give a low temperature approximation to $N$
\begin{align}
    N &= \frac{g_sV}{\lambda_{th}^3}f_{3/2}(z) \approx \frac{g_sV}{\lambda_{th}^3}\frac{(\ln z)^{3/2}}{\Gamma(5/2)}\bigg(1+\frac{\pi^2}{6}\frac{3}{4(\ln z)^2}\bigg)\\
    &=\frac{g_sV}{\lambda_{th}^3}\frac{4(\beta\mu)^{3/2}}{3\sqrt{\pi}}\bigg(1+\frac{\pi^2}{8}\frac{1}{(\beta \mu)^2}\bigg)\\
    &=\frac{g_sV}{6\pi^2 \hbar^3}(2m\mu)^{3/2}\bigg(1+\frac{\pi^2}{8}\frac{1}{(\beta \mu)^2}\bigg)
\end{align}
and the Fermi energy:
\begin{align}
    E_F &= \frac{\hbar^2}{2m}\bigg[\frac{6\pi^2}{g_s}\frac{g_sV}{\lambda_{th}^3}\frac{4(\beta\mu)^{3/2}}{3\sqrt{\pi}}\bigg(1+\frac{\pi^2}{8}\frac{1}{(\beta \mu)^2}\bigg)\bigg]^{2/3}\\
    &=\mu \bigg(1+\frac{\pi^2}{8}\frac{1}{(\beta \mu)^2}\bigg)^{2/3}\approx \mu \bigg(1+\frac{\pi^2}{12}\frac{1}{(\beta \mu)^2}\bigg)
\end{align}
By the same trick as before this can be inverted to give an expression for the chemical potential:
\begin{equation}\label{mu approx}
    \mu \approx E_F\bigg(1-\frac{\pi^2}{12}\frac{1}{(\beta E_F)^2}\bigg)
\end{equation}
Finally, the energy density:
\begin{align}
    \frac{U}{V} &= \frac{3g_s}{2\beta \lambda_{th}^3} \frac{8(\beta \mu)^{5/2}}{15\sqrt{\pi}}\bigg(1+\frac{\pi^2}{6}\frac{15}{4(\beta \mu)^2}\bigg)\\
    &=\frac{4g_s}{5\beta \lambda_{th}^3} \frac{(\beta \mu)^{5/2}}{\sqrt{\pi}}\bigg(1+\frac{5\pi^2}{8}\frac{1}{(\beta \mu)^2}\bigg)
\end{align}
To compute $C_V$ we need to take the derivative of the above with respect to temperature. This is problematic since we have $\mu$ in our expression, which has gained a temperature dependence at fixed particle number. We fix this by calculating $\frac{U}{N}$:
\begin{align}
    \frac{U}{N} &= \frac{4g_s}{5\beta \lambda_{th}^3} \frac{(\beta \mu)^{5/2}}{\sqrt{\pi}}\bigg(1+\frac{5\pi^2}{8}\frac{1}{(\beta \mu)^2}\bigg)\frac{6\pi^2 \hbar^3}{g_sV}\frac{1}{(2m\mu)^{3/2}}\bigg(1-\frac{\pi^2}{8}\frac{1}{(\beta \mu)^2}\bigg)\\
    &=\frac{3}{5}\mu \bigg(1+\frac{\pi^2}{2}\frac{1}{(\beta \mu)^2}\bigg)
\end{align}
and inserting \eqref{mu approx}:
\begin{align}
    \frac{U}{N} &\approx \frac{3}{5}E_F\bigg(1-\frac{\pi^2}{12}\frac{1}{(\beta E_F)^2}\bigg)\bigg[1+\frac{\pi^2}{2}\frac{1}{(\beta E_F)^2}\bigg(1-\frac{\pi^2}{6}\frac{1}{(\beta E_F)^2}\bigg)\bigg]\\
    &=\frac{3}{5}E_F\bigg(1-\frac{\pi^2}{12}\frac{1}{(\beta E_F)^2}\bigg)\bigg(1+\frac{\pi^2}{2}\frac{1}{(\beta E_F)^2}\bigg)\\
    &= \frac{3}{5}E_F\bigg(1+\frac{5\pi^2}{12}\frac{1}{(\beta E_F)^2}\bigg)
\end{align}
Taking the partial derivative is much simpler now:
\begin{equation}
    C_V = \frac{3}{5}E_F N \cdot \frac{5\pi^2}{12}\frac{1}{E_F^2}k_B^2 \cdot 2T = Nk_B T\frac{\pi^2}{2T_F}
\end{equation}
where $T_F = \frac{E_F}{k_B}$ is the \textbf{Fermi temperature}.
\section{Magnetism}
When discussing the motion of electrons in a magnetic field, we must consider the Lorentz force and the coupling of the electron spin to the magnetic field. The first interaction gives rise to \enquote{Landau diamagnetism} while the second gives rise to \enquote{Pauli paramagnetism}. 
\subsection{Landau diamagnetism}
The Hamiltonian of an electron of charge $-e$ in a magnetic field $\textbf{B} = \nabla \times \textbf{A}$ is:
\begin{equation}
    H = \frac{(\textbf{p}+e\textbf{A})^2}{2m}
\end{equation}
Interestingly, when calculating the classical partition function, the effect of adding a magnetic field is not noticeable. Indeed:

This result is the \textbf{Bohr-van Leeuwen} theorem. Magnetism can therefore appear only in the quantum regime. The Schrodinger equation for the electron in a constant magnetic field $\textbf{B} = B \hat{\textbf{z}}$ was solved in the Quantum mechanics volume. The energy levels were found to be:
\begin{equation}
    E = \frac{\hbar^2 k_z^2}{2m} + \hbar \omega_c\bigg(n+\frac{1}{2}\bigg)
\end{equation}
where the degeneracy of each oscillator is $\frac{eBL^2}{2\pi \hbar} = \frac{\Phi}{\Phi_0}$ ignoring spin degeneracy. The partition function is then given by:
\begin{equation}
    \ln \mathcal{Z} = \frac{L}{2\pi}\int \frac{2\Phi}{\Phi_0} \ln \bigg[1+z\exp\bigg(-\frac{\beta \hbar^2 k_z^2}{2m}-\beta \hbar \omega_c(n+1/2)\bigg)\bigg]dk_z
\end{equation}
The Euler sum formula:
\begin{equation}
    \sum_{n=0}^\infty f(n+\frac{1}{2}) \approx \int_0^\infty f(x)dx + \frac{1}{24}f'(0)
\end{equation}
can be applied to:
\begin{equation}
    f(x) = \int  \ln \bigg[1+\exp\bigg(-\frac{\beta \hbar^2 k_z^2}{2m}+\beta\mu - \beta\hbar \omega x\bigg)\bigg]dk_z
\end{equation}
We find that:
\begin{align}
    \ln \mathcal{Z} &= \frac{VB}{\pi \Phi_0}\sum_{n=0}^\infty f(n+1/2)\\
    &\approx \frac{VB}{\pi \Phi_0} \int_0^\infty f(x)dx - \frac{VB}{\pi \Phi_0} \frac{\hbar \omega_c}{24}\int \frac{1}{z^{-1}\exp(\beta \hbar^2 k_z^2/2m)+1} dk_z
\end{align}
The first term is just the contribution of the partition function of a classical ideal gas, and does not depend on the magnetic field as shown earlier. The second does depend on the magnetic field, and at $T=0$ the integrand is 1 if $|k|<k_F$ and vanishes otherwise, giving:
\begin{equation}
      \ln \mathcal{Z} =\ln \mathcal{Z}_{cl} - \frac{mV}{2\pi^2\hbar^2} \frac{(\hbar \omega_c)^2}{12}\beta k_F
\end{equation}
We now note that:
\begin{equation}
    g(E_F) = \frac{mV}{\pi^2 \hbar^2} k_F
\end{equation}
so
\begin{equation}
     \ln \mathcal{Z} =\ln \mathcal{Z}_{cl} - g(E_F) \frac{(\hbar \omega_c)^2}{24}\beta
\end{equation}
The magnetization is then given by:
\begin{equation}
    M = \frac{1}{\beta} \frac{\partial}{\partial B}(\ln \mathcal{Z}) = - g(E_F) \frac{(\hbar e)^2}{12m^4}B
\end{equation}
Using the Bohr magneton $\mu_B = \frac{|e|\hbar}{2mc}$ we find that:
\begin{equation}
    M =- \frac{1}{3}g(E_F)\mu_B^2 B
\end{equation}
Note that this magnetization has a negative sign and thus it points away from the direction of the applied magnetic field, it is \textbf{diamagnetic}.
\subsection{Pauli paramagnetism}

\section{White Dwarf stars}
Consider a star at temperature $T \rightarrow 0$ that has exhausted all its nuclear fuel, a \textbf{white dwarf}. To support themselves and avoid collapsing completely onto itself, these stars need to use the degeneracy pressure calculated earlier. 

Note that the fermion energies have both a kinetic contribution and a gravitational contribution, which we have not taken into account in $g(E)$. It can be easily shown that for a dwarf star of uniform density, mass $M$ and radius $R$:
\begin{equation}
    E_{G} = -\frac{3}{5}\frac{GM^2}{R}
\end{equation}
giving a total energy of:
\begin{equation}
    E = -\alpha \frac{M^2}{R}+\gamma \int_0^{E_F}E^{3/2} dE = -\alpha\frac{M^2}{R} + \gamma E_F^{5/2}
\end{equation}
Now $V \propto R^3$ and $m=\frac{M}{N}$ so $E_F \propto \frac{M^{2/3}}{mR^2}$. Consequently, for the total energy to be minimized, the equilibrium condition reads that:
\begin{equation}
    -\alpha \frac{M^2}{R} + \gamma \frac{M^{5/3}}{R^2} = 0 \implies R \propto M^{-1/3}
\end{equation}
This is a perplexing result, the more massive the star gets the smaller its size becomes!

We have not taken into account the fact that as the star shrinks, the Fermi energy grows, until it becomes on the order of magnitude of the electron mass. Here our non-relativistic treatment does not hold anymore. Consequently, we should use the relativistic density of states, and expand it in the ultra-relativistic limit $\frac{m}{E}\ll1$:
\begin{equation}
    g(E) = \frac{V}{\pi^2 \hbar^3 c^3} E^2 \sqrt{1-\frac{m^2 c^4}{E^2}} \approx \frac{V}{\pi^2 \hbar^3 c^3}\bigg(E^2 - \frac{m^2c^4}{2})
\end{equation}
giving a total kinetic energy upon integration of:
\begin{equation}\label{kinetic energy}
    E_{kin} \approx \frac{V}{\pi^2 \hbar^3 c^3}\bigg(\frac{1}{4}E_F^4 - \frac{m^2c^4}{4}E_F^2\bigg)
\end{equation}
To find the Fermi energy, we can express the particle number as:
\begin{equation}
    N \approx \frac{V}{\pi^2 \hbar^3 c^3}\bigg(\frac{1}{3}E_F^3 - \frac{m^2c^4}{2}E_F\bigg) \implies E_F \approx \bigg(\frac{3N}{V}\pi^2 \hbar^3 c^3\bigg)^{1/3}
\end{equation}
Inserting this into \eqref{kinetic energy} we find that:
\begin{equation}
    E = \bigg[\frac{3\hbar c}{4}\bigg(\frac{9\pi M^4}{4m_p^4}\bigg)^{1/3} - \frac{3}{5}GM\bigg] \frac{1}{R} -O(R)
\end{equation}
where $m_p$ is the proton mass, and $M=Nm_p$ since we may assume that most of the star's mass comes from the nucleus. As long as the term in square brackets is positive, we can always find a minimum energy. However if this is not the case then the star will be unstable, shrinking further and further. The mass at which this occurs is known as the \textbf{Chandrasekhar limit}:
\begin{equation}
    M_C = \frac{3\sqrt{\pi}}{2}\bigg(\frac{5\hbar c}{4G}\bigg)^{3/2}\frac{1}{m^2} \propto \frac{M_P^3}{m_p^2}
\end{equation}
where $M_P = \sqrt{\frac{\hbar c}{G}}$ is known as the \textbf{Planck mass}. If a star has a mass larger than $M_C$, its collapse will either be stopped by the degeneracy pressure of the neutrons, or will form a black hole.
\chapter{Real gases and Interacting systems}
\section{The virial expansion}
Thus far we have only considered non-interacting systems where the partition function could be factorized into the partition function for each particle. When we add interactions to the system this no longer holds and the partition function becomes much more arduous to solve. 

In this chapter we will concern ourselves with weakly interacting gases, where the density of particles is low and the temperature is high. We can write a general equation of state as a power expansion of $N/V$, which in the low density limit reduces to the ideal gas law:
\begin{equation}\label{virial}
    \frac{pV}{Nk_B T} = 1+B_2(T) \frac{N}{V} +B_3(T) \bigg(\frac{N}{V}\bigg)^2+...
\end{equation}
This is known as the virial expansion. The $B_i(T)$ functions are known as virial coefficients, understanding them will be the goal of this chapter. We begin by calculating $B_2(T)$, the second virial coefficient.
\section{The Second Virial coefficient}
The Hamiltonian for a gas of particles interacting through a central two-body potential $U(r)$ is given by
\begin{equation}
    H = \sum_{i=1}^N \frac{p_i^2}{2m} + \sum_{i>j} U(r_{ij}), \ r_{ij} = |\textbf{r}_i-\textbf{r}_j|
\end{equation}
where $\textbf{r}_i$ is the position of the $i$th particle. The partition function now becomes:
\begin{align}
    Z &= \frac{1}{N! h^{3N}} \int  d^{3N}\textbf{p} \ d^{3N}\textbf{r} \ \exp\bigg(-\frac{\beta p_i^2}{2m} - \beta \sum_{j>k} U(r_{jk})\bigg)\\
    &=\frac{1}{N! \lambda^{3N}} \int d^{3N}\textbf{r} \ e^{-\beta \sum_{j>k} U(r_{jk})}\\
    &=\frac{1}{N! \lambda^{3N}} \int d^{3N}\textbf{r} \ \prod_{j>k} e^{-\beta U(r_{jk})}
\end{align}
Unfortunately, the integral over position looks very intimidating. Let us look more closely at the properties of $U$.

We want to model neutral atoms which can only interact through Van der Waals interactions. These arise due to temporary dipole-dipole interactions induced by quantum fluctuations in the atom's electronic wave-function, and are of the form $U \sim \frac{1}{r^6}$. Indeed suppose that one atom, due to fluctuations, has a temporary dipole moment $p_1$ producing an electric field of the form $E\sim \frac{p_1}{r^3}$. As a result the second atom will have an induced dipole moment of $p_2 \sim E \sim \frac{p_1}{r^3}$ meaning that their potential energy scales as $\frac{p_1p_2}{r^3} \sim \frac{1}{r^6}$. Consequently the potential will be strongly repulsive at small distances, but we should also expect it to be attractive for large distances, vanishing at $r \rightarrow \infty$. One common potential that satisfies these requirements is the Lennard-Jones potential:
\begin{equation}
    U(r) \sim \bigg(\frac{r_0}{r}\bigg)^12-\bigg(\frac{r_0}{r}\bigg)^6
\end{equation}
If the gas density is low, or the temperature is high, then we should expect to recover the classical ideal gas. It therefore seems logical to expand the partition function in the ideal gas limit where $e^{-\beta U} \approx 1$. This yields the following expansion parameter
\begin{equation}
    f_{jk} = e^{-\beta U(r_{jk})}-1
\end{equation}
which vanishes in the low density limit $r_{ik}\rightarrow \infty$, and is equal to $-1$ when $r_{ik}\rightarrow 0$.
\begin{equation}
    e^{-\beta \sum_{j>k} U(r_{jk})}  = 1-\beta \sum_{j>k} U(r_{jk})
\end{equation}
We then find that
\begin{align}
    Z &= \frac{1}{N!\lambda^{3N}} \int d^{3N}\textbf{r} \  \prod_{j>k}(1+f_{jk})\\
    &=\frac{1}{N!\lambda^{3N}} \int d^{3N}\textbf{r} \  \bigg(1+\sum_{j>k}f_{jk}+\sum_{\substack{j>k\\l>m}} f_{jk}f_{lm}+...\bigg)\label{cluster}
\end{align}
Note note that
\begin{equation}
    \int d^{3N}\textbf{r}\ f_{jk} = V^{N-2} \int d^3\textbf{r}_j \  d^3\textbf{r}_k \ f_{jk}
\end{equation}
We now perform a change of variables from $(\textbf{r}_j,\textbf{r}_k)$ to $(\textbf{R}_{jk},\textbf{r}_{jk})$ where
\begin{equation}
    \textbf{R}_{jk} = \frac{1}{2}(\textbf{r}_j+\textbf{r}_k), \ \textbf{r}_{jk} =\textbf{r}_j-\textbf{r}_k
\end{equation}
whose Jacobian is 
\begin{equation}
    \frac{\partial(\textbf{r}_j,\textbf{r}_k)}{\partial(\textbf{R}_{jk},\textbf{r}_{jk})} = \begin{vmatrix}1 & \frac{1}{2}\\
    1 & -\frac{1}{2}
    \end{vmatrix}=-1
\end{equation}
thus yielding
\begin{equation}
     \int d^{3N}\textbf{r} \ f_{jk} = V^{N-1} \int d^3\textbf{r} \  f(r)
\end{equation}
where the limits of integration still span $\mathbb{R}^3$ (we don't have to worry about what happens at the edges since they provide a negligible contribution).

The sum over $\sum_{j>k} =\frac{1}{2}\sum_j \sum_{k\neq j}= \frac{1}{2}N(N-1)$ implying that the partition function takes the form
\begin{equation}
    Z = Z_{\text{ideal}} \bigg(1+\frac{N^2}{2V} \int d^3\textbf{r} \  f(r)+...\bigg)
\end{equation}
It will be helpful (to calculate logarithms more easily) to write the expression in brackets as a power of $N$. We can do so if the term with the integral is small (that is when we're near the ideal limt), in which case
\begin{equation}
     Z = Z_{\text{ideal}} \bigg(1+\frac{N}{2V} \int d^3\textbf{r}\  f(r)+...\bigg)^N
\end{equation}
The free energy then reads
\begin{equation}
    F=F_{\text{ideal}} - Nk_BT \ln\bigg(1+\frac{N}{2V}\int d^3\textbf{r} \ f(r)\bigg)
\end{equation}

For the Lennard-Jones potential the integral is approximately $\sim r_0^3$, where $r_0$ is the separation minimising the potential. Consequently our expansion holds when
\begin{equation}
    \frac{N}{2V}r_0^3 \ll 1 \iff \frac{N}{V} \ll \frac{1}{r_0^3}
\end{equation}
so when the gas density is much lower than the atomic density.
\section{Van der Waals equation}
Let us compute the partition function for the Sutherland potential
\begin{equation}
    U(r) = \begin{cases}
     \infty, \ r<r_0\\
     -U_0\bigg(\frac{r_0}{r}\bigg)^6, \ r\geq r_0
    \end{cases}
\end{equation}
which models the gas atoms as solid spheres of radii $\frac{r_0}{2}$. Then we find that
\begin{align}
    \int d^3\textbf{r} \ f(r) &= 4\pi \bigg[\int_{r_0}^\infty dr \ r^2  (e^{\beta U_0 r_0^6/r^6}-1)-\int_0^{r_0} dr \ r^2\bigg]\\
    &\approx  4\pi \bigg(-\frac{r_0^3}{3}+\int_{r_0}^\infty dr \  \frac{\beta U_0r_0^6}{r^4}\bigg)\\
    &=\frac{4\pi}{3}(\beta U_0 - 1)r_0^3
\end{align}
This gives the following approximation to the free energy
\begin{equation}
    F \approx F_{\text{ideal}} - Nk_B T \ln\bigg(1+\frac{2\pi N}{3V}(\beta U_0-1)r_0^3\bigg)
\end{equation}
Differentiating with respect to volume
\begin{align}
   p&= -\frac{\partial F}{\partial V}\bigg|_T = p_{\text{ideal}} + Nk_BT \frac{-\frac{2\pi N}{3V^2}(\beta U_0-1)r_0^3}{1+\frac{2\pi N}{3V}(\beta U_0-1)r_0^3}\\
   &\approx\frac{Nk_BT}{V}\bigg(1-\frac{2\pi N}{3V}(\beta U_0-1)r_0^3\bigg)\\
   &=\frac{k_BT}{v}\bigg(1\frac{2\pi}{3v}\Big(1-\frac{U_0}{k_BT}\Big)r_0^3\bigg)
\end{align}
where we defined $v=\frac{V}{N}$. We can rearrange this into
\begin{equation}
    p+\frac{2\pi r_0^3 U_0}{3v^2} = \frac{k_BT}{v}\bigg(1+\frac{2\pi r_0^3}{3v}\bigg)
\end{equation}
Now assuming that $v \gg r_0^3$, that is, the volume per particle is much larger than the atomic volume $4\pi r_0^3/3$ (i.e. low density limit) then we can expand the LHS to give
\begin{equation}
    p + \frac{2\pi r_0^3 U_0}{3}\frac{1}{v^2} = \frac{k_B T}{v}\bigg(1-\frac{2\pi r_0^3}{3}\frac{1}{v}\bigg)^{-1}
\end{equation}
Letting $a=\frac{2\pi}{3} r_0^3 U_0$ and $b=\frac{2\pi}{3}r_0^3 = \frac{a}{U_0}$ then we find
\begin{equation}
    \bigg(p+\frac{a}{v^2}\bigg)(v-b) = k_B T
\end{equation}
which is the Van der Waals equation of state.

We have calculated the second virial coefficient in its generality and applied it to the Sutherland potential to derive the Van der Waals equation. However, higher order coefficients are harder to solve, and we need a systematic method to evaluate them. The cluster expansion will allow us to do this.
\section{The cluster expansion}
We introduce a pictorial notation that will enable us to calculate the cluster expansion terms to any order. Given a term of the type $f_{ij} f_{kl} ...$ we firstly draw $N$ atoms, and then draw a line between all the pairs appearing as indices, so between $i,j$, then between $k,l$ etc... For example, we see that the graph associated to $f_{21}f_{23}f_{31}$ with $N=3$ is \resizebox{1cm}{!}{\begin{tikzpicture}[baseline={([yshift=-.5ex]current bounding box.center)}]
    \node[main node] (1)  {\tiny 1};
    \node[main node] (2) [right=9pt of 1] {\tiny 2};
    \node[main node] (3) [above right=7.8pt and 3.5pt of 1] {\tiny 3};

    \draw[-] (1) -- (2);
    \draw[-] (2) -- (3);
    \draw[-] (3) -- (1);
\end{tikzpicture}}. It is clear that in the cluster expansion \eqref{cluster} we will have all possible graphs $G$ with $N$ nodes
\begin{equation}
    Z = \frac{1}{N! \lambda^{3N}} \sum_{G\in \mathcal{G}_N} W[G]
\end{equation}
where 
\begin{equation}
    W[G] = \int d^{3N}\textbf{r} \ G
\end{equation}
For example we have that if $N=6$ then
\begin{equation}
W\Bigg[\ \resizebox{2cm}{!}{\begin{tikzpicture}[baseline={([yshift=-.5ex]current bounding box.center)}]
    \node[main node] (1)  {\tiny 1};
    \node[main node] (2) [right=9pt of 1] {\tiny 2};
    \node[main node] (3) [above right=7.8pt and 3.5pt of 1] {\tiny 3};

    \draw[-] (1) -- (2);
    \draw[-] (3) -- (1);
\end{tikzpicture} \ 
\begin{tikzpicture}[baseline={([yshift=-.5ex]current bounding box.center)}]
    \node[main node] (4)  {\tiny 4};
    \node[main node] (5) [below=7.8pt of 4] {\tiny 5};
    \draw[-] (4) -- (5);
\end{tikzpicture}\ \begin{tikzpicture}[baseline={([yshift=-.5ex]current bounding box.center)}]
    \node[main node] (6)  {\tiny 6};
\end{tikzpicture}}\  \Bigg] = \bigg(\int d^{3}\textbf{r}_1 \  d^3\textbf{r}_2 \ d^3\textbf{r}_3 \ f_{13} f_{12}\bigg)\bigg(\int d^3\textbf{r}_4 \ d^3\textbf{r}_5 \ f_{45}\bigg)\bigg(\int d^3\textbf{r}_6\bigg)
\end{equation}
so we see that the integrals factorize into connected graphs. These connected graphs \resizebox{3.5cm}{!}{\begin{tikzpicture}[baseline={([yshift=-.5ex]current bounding box.center)}]
    \node[main node] (1)  {\tiny 1};
    \node[main node] (2) [right=9pt of 1] {\tiny 2};
    \node[main node] (3) [above right=7.8pt and 3.5pt of 1] {\tiny 3};

    \draw[-] (1) -- (2);
    \draw[-] (3) -- (1);
\end{tikzpicture}\ , \ 
\begin{tikzpicture}[baseline={([yshift=-.5ex]current bounding box.center)}]
    \node[main node] (4)  {\tiny 4};
    \node[main node] (5) [below=7.8pt of 4] {\tiny 5};
    \draw[-] (4) -- (5);
\end{tikzpicture}\ \tiny{, and} \ \begin{tikzpicture}[baseline={([yshift=-.5ex]current bounding box.center)}]
    \node[main node] (6)  {\tiny 6};
\end{tikzpicture}} are examples of what we will call a 3-cluster, 2-cluster and 1-cluster respectively, since they each have 3, 2 and 1 nodes respectively. Thus a $l$-cluster will be a connected graph with $l$-nodes. In general we should find that if we have $m_l$ $l$-clusters in the cluster expansion then
\begin{equation}\label{cond1}
    \sum_{l=1}^N m_l l = N
\end{equation}
Let us define the following integrals
\begin{equation}
    U_l \equiv \int d^{3m_l}\textbf{r} \sum_{G\in \mathcal{C}_l} G
\end{equation}
where $\mathcal{C}_l$ is the set of $l$-clusters. For example
\begin{equation}
    U_3 = \int d^3\textbf{r}_1 \ d^3\textbf{r}_2 \ d^3\textbf{r}_3 \ \Bigg[\ \resizebox{1cm}{!}{\begin{tikzpicture}[baseline={([yshift=-.5ex]current bounding box.center)}]
    \node[main node] (1)  {\tiny 1};
    \node[main node] (2) [right=9pt of 1] {\tiny 2};
    \node[main node] (3) [above right=7.8pt and 3.5pt of 1] {\tiny 3};

    \draw[-] (1) -- (2);
    \draw[-] (2) -- (3);
\end{tikzpicture}} + \resizebox{1cm}{!}{\begin{tikzpicture}[baseline={([yshift=-.5ex]current bounding box.center)}]
    \node[main node] (1)  {\tiny 1};
    \node[main node] (2) [right=9pt of 1] {\tiny 2};
    \node[main node] (3) [above right=7.8pt and 3.5pt of 1] {\tiny 3};

    \draw[-] (1) -- (3);
    \draw[-] (2) -- (3);
\end{tikzpicture}} + \resizebox{1cm}{!}{\begin{tikzpicture}[baseline={([yshift=-.5ex]current bounding box.center)}]
    \node[main node] (1)  {\tiny 1};
    \node[main node] (2) [right=9pt of 1] {\tiny 2};
    \node[main node] (3) [above right=7.8pt and 3.5pt of 1] {\tiny 3};

    \draw[-] (1) -- (2);
    \draw[-] (1) -- (3);
\end{tikzpicture}}+ \resizebox{1cm}{!}{\begin{tikzpicture}[baseline={([yshift=-.5ex]current bounding box.center)}]
    \node[main node] (1)  {\tiny 1};
    \node[main node] (2) [right=9pt of 1] {\tiny 2};
    \node[main node] (3) [above right=7.8pt and 3.5pt of 1] {\tiny 3};

    \draw[-] (1) -- (2);
    \draw[-] (2) -- (3);
    \draw[-] (1) -- (3);
\end{tikzpicture}}\ \Bigg]
\end{equation}
which contains both $f^2$ and $f^3$ terms. Then we can write
\begin{equation}
    \sum_G W[G] =  \sum_{\{m_l\}_N}g \prod_l U_l^{m_l}
\end{equation}
where $g$ is a proportionality factor due to any multiple-counting, and $\{m_l\}_N$ is the set of all $\{m_1,...,m_N\}$ which satisfy \eqref{cond1}. Our job is to find $g(N)$.

For a given $\{m_l\}_N = \{m_1, m_2,...,m_N\}$, we can define the sum of all graphs with $m_1$ 1-clusters etc... as $S\{m_l\}_N$
implying that
\begin{equation}
    Q_N \equiv \sum_{G \in \mathcal{G}_N} W[G] = \sum_{\{m_l\}_N} S\{m_l\}
\end{equation}
Each term in $S\{m_1,...,m_N\}$ is of the form
\begin{equation}
    W[\text{($m_1$ $1$-clusters)} \ \text{($m_2$ $2$-clusters)} \ ...  ]
\end{equation}
For example if $N=9$ then a possible partition is $\{m_l\} = \{1,1,2\}$, and thus a possible term in $S\{1,1,2\}$ is
\begin{equation}\label{example}
    W\Bigg[\resizebox{3.5cm}{!}{\begin{tikzpicture}[baseline={([yshift=-.5ex]current bounding box.center)}]
    \node[main node] (1)  {\tiny 1};
    \node[main node] (2) [right=9pt of 1] {\tiny 2};
    \node[main node] (3) [above right=7.8pt and 3.5pt of 1] {\tiny 3};

    \draw[-] (1) -- (2);
    \draw[-] (2) -- (3);
    \draw[-] (1) -- (3);
\end{tikzpicture} \ \begin{tikzpicture}[baseline={([yshift=-.5ex]current bounding box.center)}]
    \node[main node] (4)  {\tiny 4};
\end{tikzpicture} \
\begin{tikzpicture}[baseline={([yshift=-.5ex]current bounding box.center)}]
    \node[main node] (5)  {\tiny 5};
    \node[main node] (6) [above=7.9pt of 5] {\tiny 6};

    \draw[-] (5) -- (6);
\end{tikzpicture} \ 
\begin{tikzpicture}[baseline={([yshift=-.5ex]current bounding box.center)}]
    \node[main node] (7)  {\tiny 7};
    \node[main node] (8) [right=9pt of 1] {\tiny 8};
    \node[main node] (9) [above right=7.8pt and 3.5pt of 1] {\tiny 9};

    \draw[-] (7) -- (8);
    \draw[-] (9) -- (7);
\end{tikzpicture}}\Bigg]
\end{equation}
If every $l$-cluster had the same value upon integration our job would be pretty much done. However, this is clearly not the case e.g. it is easy to see that the two $3$-clusters in the above example will yield different integrals.

If we categorize $l$-clusters into different types corresponding to different values when integrated, then we see that there is only one type of $1$-clusters and $2$-clusters, while there are four types of $3$-clusters
\begin{equation*}
\resizebox{6.4cm}{!}{\begin{tikzpicture}[baseline={([yshift=-.5ex]current bounding box.center)}]
    \node[main node] (1)  {\tiny};
    \node[main node] (2) [right=9pt of 1] {\tiny};
    \node[main node] (3) [above right=7.8pt and 3.5pt of 1] {\tiny};
    
    \draw[-] (1) -- (3);
    \draw[-] (2) -- (3);
\end{tikzpicture} \
 \begin{tikzpicture}[baseline={([yshift=-.5ex]current bounding box.center)}]
    \node[main node] (1)  {\tiny};
    \node[main node] (2) [right=9pt of 1] {\tiny};
    \node[main node] (3) [above right=7.8pt and 3.5pt of 1] {\tiny};

    \draw[-] (1) -- (2);
    \draw[-] (1) -- (3);
\end{tikzpicture} \
 \begin{tikzpicture}[baseline={([yshift=-.5ex]current bounding box.center)}]
    \node[main node] (1)  {\tiny};
    \node[main node] (2) [right=9pt of 1] {\tiny};
    \node[main node] (3) [above right=7.8pt and 3.5pt of 1] {\tiny};

    \draw[-] (1) -- (2);
    \draw[-] (2) -- (3);
\end{tikzpicture} \
    \begin{tikzpicture}[baseline={([yshift=-.5ex]current bounding box.center)}]
    \node[main node] (1)  {\tiny};
    \node[main node] (2) [right=9pt of 1] {\tiny};
    \node[main node] (3) [above right=7.8pt and 3.5pt of 1] {};

    \draw[-] (1) -- (2);
    \draw[-] (2) -- (3);
    \draw[-] (1) -- (3);
\end{tikzpicture}}
\end{equation*}
Note that $3$-clusters of a different type don't necessarily have different values, indeed the first three are equal to each other. Nevertheless they are not equivalent as they are distinct terms in the cluster expansion (different indices in $f_{ij}f_{kl}...$).

Suppose there are $K_l$ different types of $l$-clusters. Then the $m_l$ $l$-clusters can be decomposed into $n_1$ $l$-clusters of type $1$, $n_2$ $l$-clusters of type $2$, etc...
\begin{align}
    W[\text{($m_l$ $l$-clusters)}] &= W[\text{($n_1$ $l$-clusters type $1$)} \ \text{($n_2$ $l$-clusters type $2$)} ...]\\
    &=(W[\text{$l$-cluster of type $1$}])^{n_1}(W[\text{$l$-cluster of type $2$}])^{n_2}...
\end{align}
where we require
\begin{equation}
    \sum_{i=1}^{K_l} n_i = m_l
\end{equation}
In \eqref{example} for example we had two different $3$-clusters, each of a different type.

Let us now consider all terms in $S\{m_l\}$ where all but the $m_l$ $l$-cluster are fixed, the latter ranging over all possible $l$-clusters. The numbers at the nodes of the $m_l$ $l$-clusters is therefore fixed, but we further fix the $l$ numbers at the nodes of each of the $l$-clusters (we'll deal with the permutations later). Therefore two graphs in the set we are considering can only differ in the number of $l$-cluster types $\{n_1,...,n_{K_l}\}$.


The $m_l$ $l$-cluster contribution is given by summing over all sets of number of $l$-cluster types $\{n_i\} \equiv \{n_1,...,n_{K_l}\}$:
\begin{equation}
      W[\text{($m_l$ $l$-clusters)}]  = \sum_{\{n_i\} \in A} \prod_{i=1}^{K_l} (W[\text{$l$-cluster of type $i$}])^{n_i}
\end{equation}
where
\begin{equation}
    A = \Big\{\{n_1,...,n_{K_l}\}:\sum_i n_i = m_l, \ 0 \leq n_i \leq m_l\Big\}
\end{equation}
Repeating this argument for all $l$ we then find that
\begin{equation}\label{fixed n}
    \prod_{l=1}^N W[\text{($m_l$ $l$-clusters)}]=\prod_{l=1}^N\bigg(\sum_{\{n_i\} \in A} \prod_{i=1}^{K_l} (W[\text{$l$-cluster of type $i$}])^{n_i}\bigg)
\end{equation}
is the sum of the integrals of the $N$-particle graphs contributing to $S\{m_i\}$ with fixed numbers on the nodes on each cluster.

All other graphs can be generated from the ones we have identified in \eqref{fixed n} by permutations of the particle numbers. In other words if we view the calculation we have just done as looking only at the shape of the graphs, now we are tasked with assigning numbers to each node. There are $N!$ such permutations, but not all of these will produce a new graph. Indeed there are two types of permutations that will not produce new graphs
\begin{enumerate}
    \item[(i)] permutations within each of the clusters: suppose we permute the particle numbers within a given $l$-cluster. This will either produce the same cluster, or another $l$-cluster of a different type. Both are accounted for, so we divide by $\prod_{l=1}^N (l!)^{m_l}$:
\begin{equation}\label{number wrong}
    \frac{N!}{\prod_{l=1}^N (l!)^{m_l}}
\end{equation}
    \item[(ii)] permutations of clusters ofsame type: suppose we permute the particle numbers so that two clusters of the same type effectively get interchanged
\begin{equation*}\resizebox{7cm}{!}{
\begin{tikzpicture}[baseline={([yshift=-.5ex]current bounding box.center)}]
    \node[main node] (1)  {\tiny 1};
    \node[main node] (2) [right=9pt of 1] {\tiny 2};
    \node[main node] (3) [above right=7.8pt and 3.5pt of 1] {\tiny 3};

    \draw[-] (1) -- (2);
    \draw[-] (2) -- (3);
    \draw[-] (1) -- (3);
\end{tikzpicture}\
\begin{tikzpicture}[baseline={([yshift=-.5ex]current bounding box.center)}]
    \node[main node] (4)  {\tiny 4};
    \node[main node] (5) [right=9pt of 1] {\tiny 5};
    \node[main node] (6) [above right=7.8pt and 3.5pt of 1] {\tiny 6};

    \draw[-] (4) -- (5);
    \draw[-] (5) -- (6);
    \draw[-] (6) -- (4);
\end{tikzpicture} $\longleftrightarrow$ \begin{tikzpicture}[baseline={([yshift=-.5ex]current bounding box.center)}]
    \node[main node] (4)  {\tiny 4};
    \node[main node] (5) [right=9pt of 1] {\tiny 5};
    \node[main node] (6) [above right=7.8pt and 3.5pt of 1] {\tiny 6};

    \draw[-] (4) -- (5);
    \draw[-] (5) -- (6);
    \draw[-] (6) -- (4);
\end{tikzpicture} \ \begin{tikzpicture}[baseline={([yshift=-.5ex]current bounding box.center)}]
    \node[main node] (1)  {\tiny 1};
    \node[main node] (2) [right=9pt of 1] {\tiny 2};
    \node[main node] (3) [above right=7.8pt and 3.5pt of 1] {\tiny 3};

    \draw[-] (1) -- (2);
    \draw[-] (2) -- (3);
    \draw[-] (1) -- (3);
\end{tikzpicture}}
\end{equation*}
Since there are $n_j!$ permutations of all $l$-clusters of type $j$, we should divide \eqref{number wrong} by $n_1!n_2!...n_{K_l}!$ so that the correct factor becomes
\begin{equation}
    \frac{N!}{\prod_{l=1}^N (l!)^{m_l}\prod_{j=1}^{K_l}n_j!} 
\end{equation}
\end{enumerate}
We find that
\begin{equation}
    S\{m_i\} = N! \prod_{l=1}^N \frac{1}{(l!)^{m_l}}\prod_{i=1}^{K_l} \frac{(W[\text{$l$-cluster of type $i$}])^{n_i}}{n_i!}
\end{equation}
The multinomial theorem is a generalization of the binomial theorem and tells us that
\begin{equation}
    \bigg(\sum_{i=1}^m x\bigg)^N =N! \sum_{\{n_i\}\in A} \prod_{i=1}^m \frac{x_i^{n_i}}{n_i!}
\end{equation}
Consequently
\begin{equation}
    S\{m_i\} = N! \prod_{l=1}^N \frac{U_l^{m_l}}{(l!)^{m_l}m_l!}
\end{equation}
finally implying that
\begin{equation}
    Z = \frac{1}{\lambda^{3N}}\sum_{\{m_l\}_N} \prod_{l=1}^N \frac{U_l^{m_l}}{(l!)^{m_l}m_l!}
\end{equation}
Just like when computing quantum statistics, we see that performing the restricted sum over $\{m_l\}_N$ is a messy business. We can simplify matters by moving to the grand canonical ensemble, where the partition function reads
\begin{align}
    \mathcal{Z} &= \sum_{N=0}^\infty z^N\bigg(\frac{1}{\lambda^{3N}}\sum_{\{m_l\}_N} \prod_{l=1}^N \frac{U_l^{m_l}}{(l!)^{m_l}m_l!}\bigg)\\
    &=\sum_{m_1,...,m_N=0}^\infty\bigg[\prod_{l=1}^\infty \frac{U_l^{m_l}}{(l!)^{m_l}m_l!}\bigg(\frac{z}{\lambda^3}\bigg)^{lm_l}\bigg]\\
    &=\prod_{l=1}^\infty \sum_{m_l=0}^\infty \frac{1}{m_l!}\bigg(\frac{U_lz^l}{l! \lambda^{3l}}\bigg)^{m_l}=\prod_{l=1}^\infty \exp\bigg(\frac{U_lz^l}{\lambda^{3l}l!}\bigg)
\end{align}
Let us define $b_l = \frac{\lambda^3}{V}\frac{U_l}{l!\lambda^{3l}}$
then
\begin{equation}
    \mathcal{Z} = \exp\bigg(\frac{V}{\lambda^3}\sum_{l=1}^\infty b_l z^l\bigg)
\end{equation}
This allows us to calculate the pressure
\begin{equation}
    p = \frac{k_B T}{V} \ln \mathcal{Z} = \frac{k_B T}{\lambda^3} \sum_{l=1}^\infty b_l z^l
\end{equation}
and the particle density
\begin{equation}
    \frac{N}{V} = \frac{z}{V}\frac{\partial}{\partial z}(\ln \mathcal{Z}) = \frac{1}{\lambda^3} \sum_{l=1}^\infty lb_lz^l
\end{equation}
The equation of state for the gas can be expressed as
\begin{equation}
    \frac{pV}{Nk_B T} = \frac{\sum_l b_l z^l}{\sum_l l b_lz^l}
\end{equation}
Equating this to the virial expansion in \eqref{virial} we find that
\begin{alignat}{2}
    \sum_{l=1}^\infty b_l z^l &= \sum_{l=1}^\infty B_l\bigg(\frac{N}{V}\bigg)^{l-1} \sum_{m=1}^\infty m b_m z^m\\
    &=\sum_{l=1}^\infty B_l\frac{1}{\lambda^{3(l-1)}} \bigg(\sum_{n=1}^\infty nb_nz^n\bigg)^{l-1} \sum_{m=1}^\infty m b_m z^m\\
    &=\bigg[1+\frac{B_2}{\lambda^3}(z+2b_2z^2+3b_3z^3+...)+\frac{B_3}{\lambda^6}(z+2b_2z^2+3b_3z^3+...)^2\bigg]\\ \nonumber
    &\quad \hspace{4cm} \times (z+2b_2z^2+3b_3z^3+...)
\end{alignat}
where $b_1=B_1=1$ are the first virial coefficients of course. Keeping only terms up to fourth order
\begin{equation}
    z+b_2z^2+b_3z^3 = z+\bigg(2b_2+\frac{B_2}{\lambda^3}\bigg)z^2+\bigg(3b_3+\frac{4b_2B_2}{\lambda^3}+\frac{B_3}{\lambda^6}\bigg)z^3+o(z^4)
\end{equation}
Equating coefficients we get
\begin{align}
    B_2 = -\lambda^3 b_2^2 = -\frac{1}{2V} \int d\textbf{r}_1d\textbf{r}_2f(r_{12}) = -\frac{1}{2}\int d^3r f(r)
\end{align}
just as we found earlier. We can also calculate the third virial coefficient
\begin{equation}
    B_3 = -2\lambda^6 b^3 - 4\lambda^3b_2B_2 = \lambda^6(4b_2^2-2b^3)
\end{equation}
Repeating this process ad infinitum we can systematically find all virial coefficients.
\chapter{Phase transitions}
\section{The Van der Waals gas-liquid phase transition}
\subsection*{The critical temperature}
We have observed that when a gas of non-interacting bosons is cooled below a certain temperature its properties start to change abruptly. We start with the Van der Waals (VdW) equation we derived earlier:
\begin{equation}
    p = \frac{k_B T}{v-b} - \frac{a}{v^2}
\end{equation}
where $v=\frac{V}{N}$ is the average volume taken by one particle. At constant temperature we get the following isotherms:
\begin{figure}[h!]
    \centering
    \includegraphics[width=7cm]{isotherms.png}
    \label{fig:my_label}
\end{figure}
where we ignored the $\frac{a}{v^2}$ term since $v>b$. As we can see for large temperatures the isotherm looks like a smooth, strictly decreasing function. For sufficiently low temperatures however the isotherm seems to have a . To transition from one regime to the other we must have a temperature, a critical temperature, at which the isotherm has an inflection point. A quick calculation yields:
\begin{equation}
    \frac{dp}{dv} = \frac{2a}{v^3}-\frac{k_BT_c}{(v-b)^2}=0, \ \frac{d^2p}{dv^2} =\frac{2k_BT}{(v-b)^3} -\frac{6a}{v^4} \implies T_c = \frac{8a}{27b}\frac{1}{k_B}
\end{equation}
where the inflection point occurs at $v=3b$.

Looking more closely at the $T<T_c$ isotherm we see that for some pressures three different particle densities are available. The highest particle density point (low $v$) corresponds to a liquid phase, since $|\frac{dp}{dv}|\gg1$ implying that this phase is very hard to compress. The lowest particle density point (high $v$) corresponds to a gas phase, since $|\frac{dp}{dv}|\ll 1$ implying that this phase is easy to compress. The middle solution is particularly different, if we decrease its pressure it will respond by decreasing its volume, while if we increase its pressure then its volume will also increase. In other words the compressibility is negative $\kappa = -\frac{\partial v}{\partial p}\bigg|_T<0$, thus violating the principle of thermodynamic stability. Indeed either way any perturbation would lead this point to either extreme, it is not a stable phase at all.

\subsection*{Maxwell's construction}
To fix the problem of the stable phase, Maxwell proposed a where we replace the unphysical, unstable part of the isotherms with a flat line. This flat line leads to a coexistence between the liquid and gasseous phase.
\begin{figure}[h!]
\centering
\begin{subfigure}{0.45\textwidth}
  \centering
  \includegraphics[width=\textwidth]{maxwell construction.jpeg}
  \label{fig:sub1}
\end{subfigure}%
\begin{subfigure}{0.4\textwidth}
  \centering
  \includegraphics[width=\textwidth]{phase coexistence.png}
  \label{fig:sub2}
\end{subfigure}
\caption{Carnot cycle and engine}
\label{fig:test}
\end{figure}
We know that phase equilibrium at constant pressure and temperature requires the the chemical potential of the two phases m be equal, so in our case
\begin{equation}
    \mu_{\text{g}} = \mu_{\text{l}}
\end{equation}
where $\mu_g$ and $\mu_l$ are the chemical potentials of the gas and liquid phases respectively. Therefore the chemical potential will change along the isotherms by
\begin{equation}
    d\mu = \frac{\partial \mu}{\partial p}\bigg|_T dp
\end{equation}
but we know that
\begin{equation}
    \frac{\partial G}{\partial p}\bigg|_{N,T} = \frac{\partial \mu}{\partial p}\bigg|_T N = V \implies   d\mu = \frac{V(p,T)}{N} dp
\end{equation}
so that
\begin{equation}
    \mu(p,T) = \mu_l + \int_{p_l}^p \frac{V(p',T)}{N} dp'
\end{equation}
We thus see that the liquid and gas phase coexist at the pressure $p$ where the integral vanishes. Therefore
\begin{equation*}
    \int_{p_l}^p v(p',T) dp' = \int_{A}^B d(pv) - \int_{A}^B pdv = p(v_g-v_l) -  \int_{A}^B pdv 
\end{equation*}
However, $p(v_l-v_g)$ is the area of the rectangle under $p$ while the integral is the area under the isotherm. The above vanishes only if the area between the isotherm and $p$ vanishes. If we calculate this pressure at all temperatures then we eventually get a coexistence line, as shown beside. Note that within this coexistence region the isotherms must be flat, reflecting the fact that the gas and liquid phases together, in equilibrium, can have different average densities despite each having a fixed density.

\subsection*{The Clausius-Clapeyron equation}
Alternatively we can look at the $p-T$ diagram where the coexistence line now becomes a line, a phase boundary. If we are not on this boundary then the two phases cannot coexist, all particles are either liquid or gaseous, but if we are right on top of it then chemical equilibrium is achieved. At the critical point there is no coexistence as there is the isotherms are monotonic.

\begin{figure}[h!]
    \centering
    \includegraphics[width=5cm]{phase diagram.png}
    \label{fig:my_label}
\end{figure}

Since the phase equilibrium condition is $\frac{G_{l}}{N_l}=\frac{G_{g}}{N_g}$ it follows that the Gibbs free energy must be continuous on either side of the phase transition line. Defining $g=\frac{G}{N}$, $v=\frac{V}{N}$ and $s=\frac{S}{N}$ it then follows that
\begin{equation}
    -s_ldT + v_l dp = -s_g dT + v_gdp \implies \frac{dp}{T} = \frac{s_g-s_l}{v_g-v_l}
\end{equation}
We further define the specific latent heat $L$ as the energy per particle released as we go through the phase transition
\begin{equation}
    L = T(s_g-s_l)
\end{equation}
Then we have that
\begin{equation}
    \boxed{\frac{dp}{dt} = \frac{L}{T(v_g-v_l)}}
\end{equation}
which is known as the Clausius-Clapeyron equation. In the gas-liquid diagram we have a discontinuity in $s$ giving rise to a non-zero latent heat, this is a first order phase transition (we explain the phase transition order in more detail later). As we move along the phase line towards the critical temperature the difference between $s_l$ and $s_g$ decreases, until at $T_c$ it vanishes giving rise to a second order phase transition. Above $T_c$ we no longer have a distinction between, so there is no longer a phase transition. 

Let us make some further assumptions, namely that $L$ is a constant, $v_g \gg v_l$ and that the gas is now ideal. Then
\begin{equation}
    \frac{dp}{dT} = \frac{Lp}{k_BT^2} \implies p = p_0 e^{-L/k_BT}
\end{equation}

There is another elegant way to calculate the critical point. let's rearrange the VdW equation into
\begin{equation}
    pv^3 - (pb+k_BT)v^2+av-ab=0
\end{equation}
If we are below the critical temperature then this has three real roots, but if we are above it then there is only one real root. So there must be some temperature where we go from one case to the other, where the three roots must coincide:
\begin{equation}
    p_c(v-v_c)=0
\end{equation}
It is easy to see that this occurs when $v_c=3b$ and $p_c = \frac{a}{27b^2}$ which gives the desired critical temperature of $k_BT_c = \frac{8a}{27b}$.
\subsection*{Critical exponents}
We can rewrite the VdW equation by defining the following dimensionless variables
\begin{equation}
    T' = \frac{T}{T_c}, \ v' = \frac{v}{v_c}, \ p' = \frac{p}{p_c}
\end{equation}
to find that
\begin{equation}
    p'=\frac{8T'}{3v'-1}-\frac{3}{v'^2}
\end{equation}
Note also that the following ratio is independent of $a$ and $b$ (and should therefore be the same for all gases)
\begin{equation}
    \frac{p_cv_c}{k_BT_c} = \frac{3}{8}
\end{equation}
We can solve the VdW equation in this revamped form to yield the two stable solutions
\begin{equation}\label{VdW 2}
    T' = \frac{(3v'_l-1)(3v'_g-1)(v'_l+v'_g)}{8v_g^2v_l^2}
\end{equation}
There is some sort of universality in the phase transition.

Next let's look at what happens near the critical point as we move along the coexistence curve. We define $\epsilon=v_g'-v_l'$, and noting that $v'_l$ and $v'_g$ are symmetric in \eqref{VdW 2} we can set $v'_g = 1+\epsilon/2$ and $v'_l = 1-\epsilon/2$. Then
\begin{align}
    T' &= \frac{(2-3\epsilon/2)(2+3\epsilon/2)}{4(1-\epsilon^2/4)^2}\\
    &\approx \frac{1}{4}\bigg(4-\frac{9}{4}\epsilon^2\bigg)\bigg(1+\frac{\epsilon^2}{2}\bigg)\\
    &=\frac{1}{4}\bigg(4-\frac{1}{4}\epsilon+o(\epsilon^2)\bigg)
\end{align}
We therefore find that 
\begin{equation}
    T' = 1-\frac{1}{16}(v'_g-v'_l)^2 \implies v_g-v_l \sim (T_c-T)^{1/2}
\end{equation}
Next let's look at the pressure as we approach the critical temperature along an isotherm. Notice that at the critical point $\frac{\partial p}{\partial v}\bigg|_{T_c} = \frac{\partial^2p}{\partial v^2}\bigg|_{T_c}=0$ so we find that
\begin{equation}
    p-p_c \sim (v-v_c)^3
\end{equation}
Finally let's look at the compressibility $\kappa = -\frac{\partial v}{\partial p}\bigg|_T$. We expect this to divergence at the critical point so to leading order
\begin{equation}
    \kappa \sim (T-T_c)^{-1}
\end{equation}
Experiments have shown that the correct scaling laws are
\begin{align}
    &v_g-v_l \sim (T_c-T)^\beta, \ \beta \approx 0.32 \\
    &p-p_c\sim (v-v_c)^\delta, \ \delta \approx 4.8\\
    & \kappa \sim (T-T_c)^{-\gamma}, \ \gamma \approx 1.2
\end{align}
The numbers $\beta, \gamma, \delta$ are known as critical exponents.
\section{Phase transitions in the Ising model}
\subsection*{The Ising model}
Consider a $d$-dimensional spin lattice with $N$ sites, so that the spin on every site either points up (s=1) or down (s=-1). We insert a magnetic field $B$ along the $z$-axis and add a (anti)ferromagnetic interaction $J$ between neighbouring spins
\begin{equation}
    H = -J \sum_{\braket{ij}} s_i s_j -B\sum_i s_i
\end{equation}
If $J>0$ then we have a ferromagnet favouring parallel spins, while if $J<0$ then we have an antiferromagnet favouring antiparallel spins.

Note that this problem is closely related to that of a lattice gas. Indeed suppose we have $N$ hard-core particles on a $d$-dimensional lattice. There is an attractive interaction $4J$ between neighbouring sites and a chemical potential $\mu$ which yields the following Hamiltonian
\begin{equation}
    H = -4J \sum_{\braket{ij}} n_i n_j - \mu \sum_i n_i
\end{equation}
This is equivalent to the Ising model if we define $s_i=2n_i-1$ and remember that $n_i=0,1$.
\subsection*{Mean field theory}
We consider a $d$-dimensional spin lattice and write $s_i = \braket{s_i}+\delta s_i$ where $\delta s_i$ is the spin fluctuation at site $i$. Then the spin interaction term becomes
\begin{equation}
    s_is_j = (\braket{s_i}+\delta s_i)(\braket{s_j}+\delta s_j)  = \braket{s_i}\braket{s_j} + \delta s_i \braket{s_j} + \delta s_j \braket{s_i} + \delta s_i \delta s_j
\end{equation}
Next we use a mean field (MF) approximation and assume that $(s_i-\braket{s_i})(s_j-\braket{s_j})$ is small, in other words the fluctuations in the spin of neighbouring sites is small. We can therefore ignore the $\delta s_i \delta s_j$ term and
\begin{align}
    s_i s_j &= \braket{s_i}\braket{s_j} + \delta s_i \braket{s_j} + \delta s_j \braket{s_i}\\
    &=s_i\braket{s_j} + s_j \braket{s_i} - \braket{s_i}\braket{s_j}
\end{align}
Let us now impose translational invariance (and thus periodic boundary conditions) along our chain, meaning that the spin expectation value is the same along all sites: $\braket{s_i} = m$ for all $i$. Then the Ising hamiltonian becomes
\begin{equation}
    H = -J\sum_{\braket{ij}} (2ms_i-m^2) -B \sum_i s_i
\end{equation}
The first sum is over pairs of nearest neighbours, so $\sum_{\braket{ij}} s_i = \frac{q}{2}\sum_i s_i$ and $\sum_{\braket{ij}} m^2 = \frac{1}{2}Nqm^2$ where $q$ is the number of nearest neighbours for a given site. Consequently
\begin{equation}
    H = \frac{1}{2}Nqm^2J-(Jqm+B)\sum_i s_i
\end{equation}
Defining an effective magnetic fied $B_{\text{eff}} = B+Jqm$ then we get
\begin{equation}
    H = \frac{1}{2}Nqm^2J - B_{\text{eff}} \sum_i s_i
\end{equation}
We have managed to decouple the spin-interaction and have a non-interacting model which we can easily solve using the tools we have developed thus far. Note that the effective magnetic field is slightly larger (or smaller) since we have absorbed the spin-interaction $J$ into the magnetic field, and the mean field approximation ensured that this would be a constant contribution.

The partition function now reads \footnote{note
\begin{equation*}
    \sum_{\{s_i\}} \prod_i e^{s_i} = \bigg(\sum_{s_1=\pm1} ... \sum_{s_N=\pm1}\bigg)(e^{s_1}... e^{s_N}) = \bigg(\sum_{s_1 = \pm 1} e^{s_1}\bigg)...\bigg(\sum_{s_N = \pm 1} e^{s_N}\bigg)... = \prod_i \sum_{s_i=\pm1} e^{s_i} 
\end{equation*}}
\begin{align}\label{partition ising}
    Z  &= e^{-\frac{1}{2}\beta N qm^2J} \sum_{\{s_i\}} \prod_i e^{\beta B_{\text{eff}} s_i}= e^{-\frac{1}{2}\beta N qm^2J}  \prod_i (e^{\beta B_{\text{eff}}}+e^{-\beta B_\text{eff}})\\
    &=e^{-\frac{1}{2}\beta N qm^2J}(e^{\beta B_{\text{eff}}}+e^{-\beta B_\text{eff}})^N = 2^N e^{-\frac{1}{2}\beta N qm^2J} \cosh(\beta B_{\text{eff}})
\end{align}
The magnetisation is given by
\begin{align}
    m &= \frac{1}{N} \sum_{\{s_i\}} p_i \bigg(\sum_i s_i\bigg) = \frac{1}{Z_{c}N} \sum_{\{s_i\}} e^{-\beta E_{\{s_i\}}}\bigg(\sum_i s_i\bigg)\\
    &=\frac{1}{N \beta}\frac{\partial}{\partial B_{\text{eff}}}\ln Z_c 
\end{align}
so using \eqref{partition ising} we finally find that
\begin{equation}\label{consistency}
    m =\tanh(\beta B_{\text{eff}}) = \tanh(\beta B + \beta Jqm)
\end{equation}
This is a self-consistency equation for $m$, it ensures that the magnetisation we defined when imposing the mean field approximation yields its own correct value. Unfortunately it has no analytic solution, but we can still view it graphically. For a given $T$ and $B$ we will obtain the magnetisation value predicted by our MFT.
\begin{figure}[h!]
\centering
\begin{subfigure}{0.4\textwidth}
  \centering
  \includegraphics[width=\textwidth]{noBsmall.png}
  \label{fig:sub1}
  \caption{}
\end{subfigure}%
\begin{subfigure}{0.4\textwidth}
  \centering
  \includegraphics[width=\textwidth]{noBbigg.png}
  \label{fig:sub2}
  \caption{}
\end{subfigure}
\caption{Plots of $m(T,B)=m$ and $m(T,B)=\tanh(\beta Jqm)$ for (a) $Jq\beta>1$ and (b) $Jq\beta<1$.}
\label{fig:test}
\end{figure}
Let us first consider the case where $B=0$. Then we find that
\begin{equation}
    m = \tanh(\beta J qm) \approx \beta J qm +\frac{1}{3}(\beta J qm)^3
\end{equation}
for $Jq\ll k_B T$ (high temperatures), so the slope near the origin is $\beta J q$. It is clear that if $\beta J q>1$ then there is only one solution, $m=0$. This implies a paramagnetic phase, the thermal fluctuations overpower the magnetic order preference of aligned spins. If instead $\beta Jq<1$ then there are three solutions, $m=\pm m_0,0$ for some $m_0$. We shall soon see that in this case the $m=0$ case is unphysical, just like in the VdW equation, so we only have $m=\pm m_0$. This implies a ferromagnetic phase, the magnetic order preferred by the system overcomes the entropic contribution due to thermal fluctuations. As $\beta \rightarrow \infty$ we have $m_0 \rightarrow \pm m$ so we do indeed obtain perfect ferromagnetism. 


There is a critical temperature where we transition between these two phases
\begin{equation}
    k_BT_c = Jq
\end{equation}
\begin{figure}[h!]
\centering
\begin{subfigure}{0.35\textwidth}
  \centering
  \includegraphics[width=\textwidth]{mnoB.png}
  \label{Beq0 phase}
  \caption{}
\end{subfigure}%
\begin{subfigure}{0.35\textwidth}
  \centering
  \includegraphics[width=\textwidth]{myesB.png}
  \label{Bnot0 phase}
  \caption{}
\end{subfigure}
\caption{Plots of the magnetisation $m$ given by the self-consistency equation for (a) $B =0$ and (b) $B \neq 0$.}
\label{fig:test}
\end{figure}
Now consider the case where $B \neq 0$. In the  there is only one solution $m=m_0$, while in the low temperature limit we again obtain three different roots, but one will be unstable and the other meta-stable leaving only one possible magnetisation. Consequently we will only ever have one solution to the self-consistency equation so there is no phase transition as we vary the temperature with fixed magnetic field. However, if we fix the temperature and vary the magnetic field from negative to positive then we will have a sudden jump from negative to positive magnetisation yielding a phase transition.

Let us try to calculate the critical exponents for the $B=0$ phase transition. Suppose we go along one of the black lines in \ref{Beq0 phase}. As $T$ approaches $T_c$ from below, te magnetisation $m$ will be small, allowing us to perform a Taylor expansion
\begin{equation}
    m \approx \beta J qm -\frac{1}{3}(\beta J qm)^3 = \frac{T_c}{T}m - \frac{1}{3}\bigg(\frac{T_c}{T}\bigg)^3m^3, \ T \rightarrow T_c^{-}
\end{equation}
This can be rearranged into
\begin{equation}
  \bigg(\frac{T_c}{T}-1\bigg)m  + \frac{1}{3}\bigg(\frac{T_c}{T}\bigg)^3 m^3 = 0
\end{equation}
which yields the solutions
\begin{equation}
    m = \pm \sqrt{3\bigg(\frac{T}{T_c}\bigg)^2\frac{T_c-T}{T_c}}, \ T \rightarrow T_c^{-}
\end{equation}
We introduce the dimensionless variable $t=\frac{T-T_c}{T_c}=T'-1$ which allows us to write
\begin{equation}
   m = \pm \sqrt{-3(1+t)^2t} \approx \pm \sqrt{-3(1+2t)t} \sim |t|^{1/2}
\end{equation}
so in other words
\begin{equation}
    m \sim \pm (T-T_c)^{1/2}
\end{equation}
which is the same critical exponent as in the VdW gas.

We can also sit at the critical temperature so that $\beta Jq=1$ and look at the magnetisation as $B \rightarrow 0$. Instead of expanding \eqref{consistency} in $m$ we now expand in $B$
\begin{equation}
    m \approx \frac{B}{Jq} + m - \frac{1}{3}m^3
\end{equation}
so that
\begin{equation}
    m \sim B^{1/3}
\end{equation}
We can also look at the susceptibility as we approach $T_c$ isothermally:
\begin{equation}
    \chi = \frac{\partial m}{\partial B}\bigg|_{T}
\end{equation}
near the critical point. If we differentiate both sides of the self-consistency equation we get
\begin{equation}
    \chi = \beta\sech^2(\beta B + \beta Jqm)(1+Jq\chi)
\end{equation}
implying that
\begin{equation}
    \chi = \frac{\beta \sech^2(\beta B + \beta Jqm)}{1-\beta Jq \sech^2(\beta B + \beta Jqm)} = \frac{\beta}{\cosh^2(\beta B + \beta Jqm)-\beta Jq}
\end{equation}
If $B=0$ this reduces to
\begin{equation}
    \chi = \frac{\beta}{\cosh^2(\beta Jqm)-\beta Jq}
\end{equation}
and using the fact that $m \sim |t|^{1/2}$ as $T \rightarrow T_c^-$ we can expand $\cosh x \approx 1+x^2/2$ to find that
\begin{equation}
    \chi \approx \frac{\beta}{1+\beta^2 J^2 q^2 |t|-\beta J q} \sim |t|^{-1}, \ T \rightarrow T_c^-
\end{equation}
since $\beta J q \rightarrow 1$ as $T \rightarrow T_c^-$.

So, is our mean field theory approximation right? Not even remotely in $d=1$, we predicted a phase transition at $k_BT_c = 2Jq$ whereas it turns out that there is no phase transition in one dimension! In 1D the thermal fluctuations are strong enough to forbid ordered phases from forming \footnote{this is not true for quantum phases, where the \enquote{quantumness} adds an extra dimension to our system. Bose-Einstein condensation is an example of this.}. In two dimensions we do have a phase transition at $k_B T_c \approx 2.27 Jq$ (as opposed to $k_B T_c \approx 4 Jq$ according to MFT) and the critical exponents are 
\begin{align}
    &m \sim (T_c-T)^\beta , \ \beta= \frac{1}{8}\\ 
    &m \sim B^{1/\delta} , \ \delta= 15\\
    &\chi \sim (T-T_c)^{-\gamma}, \ \gamma = \frac{7}{4}
\end{align}
We see that the mean field approximation is still bad, but at least it correctly predicts the phase transition (but not the critical temperature). 
However the mean field approximation is justified for large dimensions, where it makes sense for each spin to effectively feel an averaged interaction from its neighbours. In low dimensions this is simply wrong to assume, but in $d\rightarrow \infty$ dimensions (whatever that means) the MFT would provide the exact solutions.

One may wonder how a discontinuity in thermodynamic quantities can occur if they are all derived from the partition function, a sum of smooth, analytic functions. In a finite system this is a finite sum so we cannot possibly get a discontinuity in the derivatives of thermodynamic potentials. In the limit where $N \rightarrow \infty$ however the sum becomes infinite and we can get a discontinuity. Thus phase transitions can only occur in the thermodynamic limit.

Finally, note that in 3D the critical exponents are 
\begin{equation}
    \beta \approx 0.32, \ \delta \approx 4.8, \ \gamma \approx 1.2
\end{equation}
which are the same as those in the VdW gas! This is not merely a cosmic coincidence, it turns out that when systems undergo phase transitions they lose information about their microscopic details. Systems which have the same critical exponents behave mathematically identically near the critical temperature and form universality classes. A single, universal theory can be used to describe systems which superficially look different.
\section{Landau-Ginzburg theory: a taster}
\subsection*{Landau's theory}
At the center of Landau's theory of phase transitions is the Free energy, let us compute it for the Ising model
\begin{equation}
    F=-\frac{1}{\beta} \ln Z=\frac{1}{2} N qm^2J-\frac{N}{\beta}\ln (2\cosh(\beta B_{\text{eff}}))
\end{equation}

Suppose we treat $F$ as a function of $m$, so we magically find a way to go out of equilibrium and keep $m$ fixed and independent of $T,V$. We can then expand $F$ in $m$ which is small near the critical temperature:
\begin{align}
    F &\approx \frac{1}{2}Nqm^2J - \frac{N}{\beta}\ln 2-\frac{N}{\beta} \ln(1+\frac{(\beta B_{\text{eff}})^2}{2}+\frac{(\beta B_{\text{eff}})^4}{24})\\
    &\approx \frac{1}{2}Nqm^2J - \frac{N}{\beta}\ln 2-\frac{N}{\beta}\bigg[\frac{(\beta B_{\text{eff}})^2}{2}+\frac{(\beta B_{\text{eff}})^4}{24}-\frac{1}{2}\bigg(\frac{(\beta B_{\text{eff}}}{2}\bigg)^2\bigg]
\end{align}
If $B=0$ then $B_{\text{eff}} = Jqm$ so that
\begin{align}
    F &\approx \frac{1}{2}Nqm^2J - \frac{N}{\beta}\ln 2-\frac{N}{\beta}\bigg[\frac{(\beta Jqm)^2}{2}-\frac{(\beta Jqm)^4}{12}\bigg]\\
    &=-Nk_B T \ln 2 +\frac{NJq}{2}\bigg(1-\frac{Jq}{k_BT}\bigg) m^2 + \frac{NJ^4q^4}{12k_B^3T^3}m^4\\
    &=-Nk_B T \ln 2 +\frac{Nk_B}{2}(T_c-T) m^2 + \frac{Nk_BT_c^4}{12T^3}m^4
\end{align}
We managed to expand the free energy near the critical temperature $T_c$ as a polynomial in $m$ of the form
\begin{equation}
    F =F_0 + a(T-T_c)m^2+bm^4
\end{equation}
with $F_0 = -Nk_B T \ln 2$, $a =-\frac{Nk_B}{2}$ and $b = \frac{Nk_BT_c^4}{12T^3}$. Here $m$ is what's known as an order parameter, it measures the degree of order across the boundary of a phase transition, with $m=0$ indicating paramagnetic order while if $m\neq0$ then we have ferromagnetic order.

Equilibrium occurs at stationary points of $F$ which in our case are given by
\begin{equation}
    \frac{\partial F}{\partial m} = 2a(T-T_c)m + 4bm^3 = 0 \implies m = 0 \ \text{ or } \  \sqrt{\frac{a(T_c-T)}{2b}}
\end{equation}
The number of solutions clearly depends on the signs of $a,b$ and the temperature $T$. We can assume that $b>0$ at all temperatures then the free energy looks like 
\begin{figure}[h!]
\centering
\begin{subfigure}{0.3\textwidth}
  \centering
  \includegraphics[width=\textwidth]{Free1.png}
  \caption{}
  \label{fig:sub1}
\end{subfigure}%
\begin{subfigure}{0.3\textwidth}
  \centering
  \includegraphics[width=\textwidth]{Free2.png}
  \caption{}
  \label{fig:sub2}
\end{subfigure}
\begin{subfigure}{0.3\textwidth}
  \centering
  \includegraphics[width=\textwidth]{Free3.png}
  \caption{}
  \label{fig:sub2}
\end{subfigure}
\caption{Free energy at $B=0$ and (a) $T<T_c$ (b) $T=T_c$ (c) $T>T_c$.}
\label{fig:test}
\end{figure}

We see that if $T\geq T_c$ only one solution ($m=0$) is real and physical, representing the paramagnetic phase. If however $T\leq T_c$ then we get three solutions, the one at the origin is meta-stable and can fall back to the other two under perturbative effects. The other two solutions $m = \pm \sqrt{\frac{a(T_c-T)}{2b}}$ are stable and represents the ferromagnetic phase. This result is in agreement with the critical exponents we calculated earlier.

Now suppose that $B \neq 0$. Then the free energy reads
\begin{equation}
    F = -Nk_B T \ln 2 + \frac{JNq}{2}m^2- \frac{N}{2k_BT}(B+Jqm)^2 + \frac{Nk_BT_c^4}{12T^3}(B+Jqm)^4
\end{equation}
which is not even anymore. 

Again for high temperatures we only have one minimum shifted from theorigin yielding a non-zero magnetisation. Suppose we instead start at sufficiently low temperatures with $B<0$, where we have two minima and a local maximum. The plot will not be symmetric so the minimum on the left will be lower than the other. This will be the preferred, physical magnetisation, while the other two will be either unstable (the middle magnetisation) or meta-stable (the other local minima). 

As we continue to increase the magnetic field the two local minima get more and more symmetric until at $B=0$ we get the graph discussed before. We then enter the $B>0$ regime where the situation is flipped, the minimum on the right will be the physical magnetisation. Here we encounter a phase transition where we suddenly transition from a negative magnetisation to a positive one, exactly as we had previously predicted.

Note that unlike before the position of the minimum transitions smoothly as we vary the temperature.
\subsection*{Ginzburg's addition}
Landau's theory ignores the fluctuations in the system which we can take into account using the revamped Landau theory: the Landau-Ginzburg theory. 

\chapter{Diffusion and Brownian motion}
\section{Langevin equation}
\part{Statistical field theory}
\chapter{Path integrals}
\chapter{Landau-Ginzburg theory}
\chapter{Renormalisation group}
\acknowledgments

This is the most common positions for acknowledgments. A macro is
available to maintain the same layout and spelling of the heading.

\paragraph{Note added.} This is also a good position for notes added
after the paper has been written.





% The bibliography will probably be heavily edited during typesetting.
% We'll parse it and, using the arxiv number or the journal data, will
% query inspire, trying to verify the data (this will probalby spot
% eventual typos) and retrive the document DOI and eventual errata.
% We however suggest to always provide author, title and journal data:
% in short all the informations that clearly identify a document.

\begin{thebibliography}{99}

\bibitem{a}
Author, \emph{Title}, \emph{J. Abbrev.} {\bf vol} (year) pg.

\bibitem{b}
Author, \emph{Title},
arxiv:1234.5678.

\bibitem{c}
Author, \emph{Title},
Publisher (year).


% Please avoid comments such as "For a review'', "For some examples",
% "and references therein" or move them in the text. In general,
% please leave only references in the bibliography and move all
% accessory text in footnotes.

% Also, please have only one work for each \bibitem.


\end{thebibliography}
\end{document}
